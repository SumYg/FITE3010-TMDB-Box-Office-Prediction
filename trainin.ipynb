{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# with open('features.npy', 'rb') as f:\n",
    "with open('features_short_new.npy', 'rb') as f:\n",
    "    features = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load only last column of a csv file\n",
    "labels = pd.read_csv('data/train.csv', usecols=[22]).values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use features for revenue prediction with xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m xg_reg \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(objective \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreg:squarederror\u001b[39m\u001b[39m'\u001b[39m, colsample_bytree \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m, learning_rate \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m,\n\u001b[1;32m      2\u001b[0m                 max_depth \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, alpha \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, n_estimators \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m xg_reg\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[1;32m      6\u001b[0m preds \u001b[39m=\u001b[39m xg_reg\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      8\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mean_squared_error(y_test, preds))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1026\u001b[0m     params,\n\u001b[1;32m   1027\u001b[0m     train_dmatrix,\n\u001b[1;32m   1028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1029\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1030\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1031\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1032\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1033\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1034\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1035\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1036\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 1000)\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random forest for revenue prediction\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7300667550937856"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = RandomForestRegressor(max_depth=9, random_state=0, n_estimators=200, n_jobs=-1)\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7311662124075626"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr1 = RandomForestRegressor(max_depth=9, random_state=0, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "regr1.fit(X_train, y_train)\n",
    "\n",
    "preds = regr1.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2118/2989140855.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  regr.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0647369106560136"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr =  RandomForestRegressor(n_estimators=100, max_depth=10, max_features=200, n_jobs=-1)\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mlp for revenue prediction\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9852636169898046.00000000\n",
      "Iteration 2, loss = 4782843223819727.00000000\n",
      "Iteration 3, loss = 4464188199836452.00000000\n",
      "Iteration 4, loss = 4733131575039396.00000000\n",
      "Iteration 5, loss = 4472290547449394.50000000\n",
      "Iteration 6, loss = 4269837520448216.50000000\n",
      "Iteration 7, loss = 4252351230490286.00000000\n",
      "Iteration 8, loss = 4281510894202930.50000000\n",
      "Iteration 9, loss = 4214104031035255.50000000\n",
      "Iteration 10, loss = 4258927805230352.00000000\n",
      "Iteration 11, loss = 4248635872957139.00000000\n",
      "Iteration 12, loss = 4312872056249307.50000000\n",
      "Iteration 13, loss = 4283809214964793.00000000\n",
      "Iteration 14, loss = 4302902660721231.50000000\n",
      "Iteration 15, loss = 4290204102481290.00000000\n",
      "Training loss did not improve more than tol=0.000010 for 5 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.402850552228329"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(800, 200, 200, 500), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=5)  # best\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9854516901168444.00000000\n",
      "Iteration 2, loss = 4595335552527831.00000000\n",
      "Iteration 3, loss = 4469739844047478.50000000\n",
      "Iteration 4, loss = 4260818295737205.00000000\n",
      "Iteration 5, loss = 4339022870742403.50000000\n",
      "Iteration 6, loss = 4250233669606186.50000000\n",
      "Iteration 7, loss = 4242800917244683.00000000\n",
      "Iteration 8, loss = 4322731268131541.50000000\n",
      "Iteration 9, loss = 4260251642815720.00000000\n",
      "Iteration 10, loss = 4294661196346540.50000000\n",
      "Iteration 11, loss = 4227379049223653.50000000\n",
      "Iteration 12, loss = 4391905475063076.50000000\n",
      "Iteration 13, loss = 4374447492365332.50000000\n",
      "Iteration 14, loss = 4246667755614483.00000000\n",
      "Iteration 15, loss = 4341612288546292.00000000\n",
      "Iteration 16, loss = 4390742857691477.50000000\n",
      "Iteration 17, loss = 4422849052051387.00000000\n",
      "Training loss did not improve more than tol=0.000010 for 5 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3882717886877685"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1000, 200, 200, 500), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=5)  # best\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3727823519272846"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1070578721402.98205566\n",
      "Iteration 2, loss = 169154981609.45535278\n",
      "Iteration 3, loss = 40208648798.18979645\n",
      "Iteration 4, loss = 20221177680.94396591\n",
      "Iteration 5, loss = 4665536178.37637901\n",
      "Iteration 6, loss = 1700233403.78227735\n",
      "Iteration 7, loss = 592078433.94917178\n",
      "Iteration 8, loss = 106634006.82095124\n",
      "Iteration 9, loss = 35144780.76744615\n",
      "Iteration 10, loss = 12643713.28618718\n",
      "Iteration 11, loss = 17373984.17633588\n",
      "Iteration 12, loss = 169458137.15847519\n",
      "Iteration 13, loss = 105565254.15546705\n",
      "Iteration 14, loss = 67669582.42899403\n",
      "Training loss did not improve more than tol=0.000010 for 3 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5734036178410284"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1000, 200, ), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=3)\n",
    "\n",
    "mlp_regr.fit(X_train, y_train/1e5)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds*1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9180644562707392.00000000\n",
      "Iteration 2, loss = 4774383633193965.00000000\n",
      "Iteration 3, loss = 4541828974319462.00000000\n",
      "Iteration 4, loss = 4263188750547031.00000000\n",
      "Iteration 5, loss = 4239700343049356.00000000\n",
      "Iteration 6, loss = 4328264131995082.00000000\n",
      "Iteration 7, loss = 4300430317325314.50000000\n",
      "Iteration 8, loss = 4255010981248534.00000000\n",
      "Iteration 9, loss = 4270247582459696.50000000\n",
      "Training loss did not improve more than tol=0.000010 for 3 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4818402431593323"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1500, 200, 200, 500), max_iter=1000, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=36, early_stopping=False, tol=0.00001, n_iter_no_change=3)\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_regr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save the model to disk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfinalized_model_mlp.sav\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m pickle\u001b[39m.\u001b[39mdump(mlp_regr, \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp_regr' is not defined"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model_mlp.sav'\n",
    "pickle.dump(mlp_regr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "mlp_regr = pickle.load(open('finalized_model_mlp.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9613999578834082.00000000\n",
      "Iteration 2, loss = 5274506705902056.00000000\n",
      "Iteration 3, loss = 4375524773283108.50000000\n",
      "Iteration 4, loss = 4374893037198315.50000000\n",
      "Iteration 5, loss = 4297481899681947.50000000\n",
      "Iteration 6, loss = 4297198571356853.00000000\n",
      "Iteration 7, loss = 4350095746821185.00000000\n",
      "Iteration 8, loss = 4327564641121450.00000000\n",
      "Iteration 9, loss = 4283455694872205.00000000\n",
      "Iteration 10, loss = 4245628883181325.00000000\n",
      "Iteration 11, loss = 4262620671572127.50000000\n",
      "Iteration 12, loss = 4242173005254726.00000000\n",
      "Iteration 13, loss = 4244069790362055.50000000\n",
      "Iteration 14, loss = 4229229553737124.00000000\n",
      "Iteration 15, loss = 4231335361051601.00000000\n",
      "Iteration 16, loss = 4308477256085541.50000000\n",
      "Iteration 17, loss = 4284650450555558.50000000\n",
      "Iteration 18, loss = 4302296566850923.50000000\n",
      "Training loss did not improve more than tol=0.000010 for 3 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.466465940541211"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1000, 200, 200), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=3)\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 12413326378239124.00000000\n",
      "Validation score: -0.232501\n",
      "Iteration 2, loss = 12413325714564238.00000000\n",
      "Validation score: -0.232501\n",
      "Iteration 3, loss = 12413324159517366.00000000\n",
      "Validation score: -0.232500\n",
      "Iteration 4, loss = 12413321346996724.00000000\n",
      "Validation score: -0.232500\n",
      "Iteration 5, loss = 12413316111855992.00000000\n",
      "Validation score: -0.232499\n",
      "Iteration 6, loss = 12413307770656950.00000000\n",
      "Validation score: -0.232498\n",
      "Iteration 7, loss = 12413295316480594.00000000\n",
      "Validation score: -0.232497\n",
      "Validation score did not improve more than tol=0.000010 for 5 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.903050181562582"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(700, 700), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.00001, n_iter_no_change=5)\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9467108744254146.00000000\n",
      "Validation score: 0.349119\n",
      "Iteration 2, loss = 5108539062161769.00000000\n",
      "Validation score: 0.581433\n",
      "Iteration 3, loss = 4471654366310205.50000000\n",
      "Validation score: 0.635126\n",
      "Iteration 4, loss = 4166449026078603.50000000\n",
      "Validation score: 0.634182\n",
      "Iteration 5, loss = 4142180488720378.50000000\n",
      "Validation score: 0.633509\n",
      "Iteration 6, loss = 4167795321421011.00000000\n",
      "Validation score: 0.634597\n",
      "Validation score did not improve more than tol=0.000100 for 2 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.194647771455093"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(700, 700), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.0001, n_iter_no_change=2)\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(700, 700), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.0001, n_iter_no_change=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2299880113177872"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hidden... | hidden... |\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11375587325816614.00000000\n",
      "Validation score: -0.034541\n",
      "Iteration 2, loss = 8428575149673836.00000000\n",
      "Validation score: 0.349458\n",
      "Iteration 3, loss = 4987947446972370.00000000\n",
      "Validation score: 0.284422\n",
      "Iteration 4, loss = 4622814866719912.00000000\n",
      "Validation score: 0.337244\n",
      "Iteration 5, loss = 4305961583378769.50000000\n",
      "Validation score: 0.397142\n",
      "Iteration 6, loss = 4330318488294520.50000000\n",
      "Validation score: 0.399170\n",
      "Iteration 7, loss = 4362668666187466.00000000\n",
      "Validation score: 0.408670\n",
      "Iteration 8, loss = 4374220486846312.50000000\n",
      "Validation score: 0.369903\n",
      "Iteration 9, loss = 4344932282600223.50000000\n",
      "Validation score: 0.388153\n",
      "Iteration 10, loss = 4334746168919979.50000000\n",
      "Validation score: 0.398735\n",
      "Iteration 11, loss = 4316292373711605.50000000\n",
      "Validation score: 0.391287\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-3.087   \u001b[0m | \u001b[0m342.0    \u001b[0m | \u001b[0m395.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11136908684676256.00000000\n",
      "Validation score: -0.001707\n",
      "Iteration 2, loss = 7864117027034809.00000000\n",
      "Validation score: 0.444219\n",
      "Iteration 3, loss = 4788909047674117.00000000\n",
      "Validation score: 0.637308\n",
      "Iteration 4, loss = 4648298797922020.00000000\n",
      "Validation score: 0.638748\n",
      "Iteration 5, loss = 4456929699388795.50000000\n",
      "Validation score: 0.608838\n",
      "Iteration 6, loss = 4452262832403810.50000000\n",
      "Validation score: 0.637822\n",
      "Iteration 7, loss = 4390110221490623.50000000\n",
      "Validation score: 0.631820\n",
      "Iteration 8, loss = 4403220870927273.00000000\n",
      "Validation score: 0.632284\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-3.211   \u001b[0m | \u001b[0m275.1    \u001b[0m | \u001b[0m557.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8693071227611712.00000000\n",
      "Validation score: 0.421237\n",
      "Iteration 2, loss = 4654546067858742.00000000\n",
      "Validation score: 0.516407\n",
      "Iteration 3, loss = 4120482875237545.00000000\n",
      "Validation score: 0.559091\n",
      "Iteration 4, loss = 4117058269346890.00000000\n",
      "Validation score: 0.580572\n",
      "Iteration 5, loss = 4100515385158389.50000000\n",
      "Validation score: 0.580793\n",
      "Iteration 6, loss = 4148582334270706.00000000\n",
      "Validation score: 0.578222\n",
      "Iteration 7, loss = 4015564436968071.50000000\n",
      "Validation score: 0.579254\n",
      "Iteration 8, loss = 4027123699335781.50000000\n",
      "Validation score: 0.579672\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-2.886   \u001b[0m | \u001b[95m994.8    \u001b[0m | \u001b[95m752.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9247440551976690.00000000\n",
      "Validation score: 0.162022\n",
      "Iteration 2, loss = 5482543490688441.00000000\n",
      "Validation score: 0.539323\n",
      "Iteration 3, loss = 4150719401376204.00000000\n",
      "Validation score: 0.565134\n",
      "Iteration 4, loss = 4024333395375183.00000000\n",
      "Validation score: 0.522289\n",
      "Iteration 5, loss = 3972513447051934.50000000\n",
      "Validation score: 0.551838\n",
      "Iteration 6, loss = 3986869655771956.00000000\n",
      "Validation score: 0.547437\n",
      "Iteration 7, loss = 4025876117393433.00000000\n",
      "Validation score: 0.523987\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-3.248   \u001b[0m | \u001b[0m781.5    \u001b[0m | \u001b[0m208.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11311973190775058.00000000\n",
      "Validation score: 0.133429\n",
      "Iteration 2, loss = 7646858506136882.00000000\n",
      "Validation score: 0.686175\n",
      "Iteration 3, loss = 4789964921571001.00000000\n",
      "Validation score: 0.534328\n",
      "Iteration 4, loss = 4621377652554061.00000000\n",
      "Validation score: 0.709341\n",
      "Iteration 5, loss = 4511785158984577.00000000\n",
      "Validation score: 0.718222\n",
      "Iteration 6, loss = 4571266070836014.00000000\n",
      "Validation score: 0.648982\n",
      "Iteration 7, loss = 4502132786099089.00000000\n",
      "Validation score: 0.712301\n",
      "Iteration 8, loss = 4477069476825215.00000000\n",
      "Validation score: 0.695501\n",
      "Iteration 9, loss = 4459296201690094.00000000\n",
      "Validation score: 0.688235\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-3.152   \u001b[0m | \u001b[0m266.9    \u001b[0m | \u001b[0m955.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11835810351496078.00000000\n",
      "Validation score: -0.103667\n",
      "Iteration 2, loss = 9038982762552266.00000000\n",
      "Validation score: 0.333247\n",
      "Iteration 3, loss = 5175900751564645.00000000\n",
      "Validation score: 0.297662\n",
      "Iteration 4, loss = 4957631243115706.00000000\n",
      "Validation score: 0.209818\n",
      "Iteration 5, loss = 4455529128173025.50000000\n",
      "Validation score: 0.423213\n",
      "Iteration 6, loss = 4542692053312009.00000000\n",
      "Validation score: 0.379150\n",
      "Iteration 7, loss = 4496105520335060.50000000\n",
      "Validation score: 0.347333\n",
      "Iteration 8, loss = 4485395506347235.50000000\n",
      "Validation score: 0.357777\n",
      "Iteration 9, loss = 4520350692311571.00000000\n",
      "Validation score: 0.402280\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-3.158   \u001b[0m | \u001b[0m270.8    \u001b[0m | \u001b[0m587.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9136602400278264.00000000\n",
      "Validation score: 0.260112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m7        \u001b[0m | \u001b[0m-3.59    \u001b[0m | \u001b[0m786.9    \u001b[0m | \u001b[0m821.3    \u001b[0m |\n",
      "Iteration 1, loss = 10518599238916330.00000000\n",
      "Validation score: 0.322232\n",
      "Iteration 2, loss = 5007060162897217.00000000\n",
      "Validation score: 0.472295\n",
      "Iteration 3, loss = 5091064493678721.00000000\n",
      "Validation score: 0.598908\n",
      "Iteration 4, loss = 4662544059594209.00000000\n",
      "Validation score: 0.598116\n",
      "Iteration 5, loss = 4623396090526691.00000000\n",
      "Validation score: 0.576150\n",
      "Iteration 6, loss = 4462688179875994.50000000\n",
      "Validation score: 0.598359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m8        \u001b[0m | \u001b[0m-3.207   \u001b[0m | \u001b[0m659.9    \u001b[0m | \u001b[0m641.2    \u001b[0m |\n",
      "Iteration 1, loss = 10251500311837104.00000000\n",
      "Validation score: -0.000713\n",
      "Iteration 2, loss = 6963454146581432.00000000\n",
      "Validation score: 0.405383\n",
      "Iteration 3, loss = 4078656998781455.50000000\n",
      "Validation score: 0.537691\n",
      "Iteration 4, loss = 4204342291245517.50000000\n",
      "Validation score: 0.539390\n",
      "Iteration 5, loss = 4008674322957336.50000000\n",
      "Validation score: 0.522439\n",
      "Iteration 6, loss = 3957388536890302.50000000\n",
      "Validation score: 0.539712\n",
      "Iteration 7, loss = 3970471676513728.50000000\n",
      "Validation score: 0.539092\n"
     ]
    }
   ],
   "source": [
    "# use Basyian Optimization for hyperparameter tuning mlp\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def mlp_cv(hidden_layer_sizes, hidden_layer_sizes1):\n",
    "    regr = MLPRegressor(hidden_layer_sizes=(int(hidden_layer_sizes), int(hidden_layer_sizes1)), max_iter=1000, alpha=0.01,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.0006, n_iter_no_change=3)\n",
    "    regr.fit(X_train_scaled, y_train)\n",
    "    preds = regr.predict(X_train_scaled)\n",
    "    return -rmsle(y_test, preds)\n",
    "\n",
    "mlp_bo = BayesianOptimization(\n",
    "        mlp_cv,\n",
    "        {\n",
    "            'hidden_layer_sizes': (100, 1000),\n",
    "            'hidden_layer_sizes1': (100, 1000),\n",
    "        }\n",
    "    )\n",
    "\n",
    "mlp_bo.maximize(n_iter=10, init_points=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11867311510283992.00000000\n",
      "Iteration 2, loss = 11867007958852102.00000000\n",
      "Iteration 3, loss = 11865243499438610.00000000\n",
      "Iteration 4, loss = 11859052896370234.00000000\n",
      "Iteration 5, loss = 11843172897697096.00000000\n",
      "Iteration 6, loss = 11807633379559452.00000000\n",
      "Iteration 7, loss = 11741986562520302.00000000\n",
      "Iteration 8, loss = 11613994102165054.00000000\n",
      "Iteration 9, loss = 11415648492407526.00000000\n",
      "Iteration 10, loss = 11114399338167910.00000000\n",
      "Iteration 11, loss = 10699740333967104.00000000\n",
      "Iteration 12, loss = 10129325436427210.00000000\n",
      "Iteration 13, loss = 9487662338565064.00000000\n",
      "Iteration 14, loss = 8779340193346599.00000000\n",
      "Iteration 15, loss = 8161701825609054.00000000\n",
      "Iteration 16, loss = 7632102388757609.00000000\n",
      "Iteration 17, loss = 7298021646151120.00000000\n",
      "Iteration 18, loss = 7037393193655040.00000000\n",
      "Iteration 19, loss = 6864654174271565.00000000\n",
      "Iteration 20, loss = 6697552651973083.00000000\n",
      "Iteration 21, loss = 6549667340940541.00000000\n",
      "Iteration 22, loss = 6404721557477468.00000000\n",
      "Iteration 23, loss = 6273090117248350.00000000\n",
      "Iteration 24, loss = 6136206762017560.00000000\n",
      "Iteration 25, loss = 6001447103474885.00000000\n",
      "Iteration 26, loss = 5873565159672845.00000000\n",
      "Iteration 27, loss = 5740725578298914.00000000\n",
      "Iteration 28, loss = 5606400776825841.00000000\n",
      "Iteration 29, loss = 5479539948093507.00000000\n",
      "Iteration 30, loss = 5341669142578653.00000000\n",
      "Iteration 31, loss = 5203229211381284.00000000\n",
      "Iteration 32, loss = 5062194164684983.00000000\n",
      "Iteration 33, loss = 4915307741069494.00000000\n",
      "Iteration 34, loss = 4774046487993329.00000000\n",
      "Iteration 35, loss = 4625280218032630.00000000\n",
      "Iteration 36, loss = 4482024216237778.00000000\n",
      "Iteration 37, loss = 4347578950352589.00000000\n",
      "Iteration 38, loss = 4186888954470734.50000000\n",
      "Iteration 39, loss = 4045111037860137.00000000\n",
      "Iteration 40, loss = 3899997417236941.50000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.213741745552174"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(800, 500, 100), max_iter=40, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11867311510283992.00000000\n",
      "Iteration 2, loss = 11867007958852102.00000000\n",
      "Iteration 3, loss = 11865243499438610.00000000\n",
      "Iteration 4, loss = 11859052896370234.00000000\n",
      "Iteration 5, loss = 11843172897697096.00000000\n",
      "Iteration 6, loss = 11807633379559452.00000000\n",
      "Iteration 7, loss = 11741986562520302.00000000\n",
      "Iteration 8, loss = 11613994102165054.00000000\n",
      "Iteration 9, loss = 11415648492407526.00000000\n",
      "Iteration 10, loss = 11114399338167910.00000000\n",
      "Iteration 11, loss = 10699740333967104.00000000\n",
      "Iteration 12, loss = 10129325436427210.00000000\n",
      "Iteration 13, loss = 9487662338565064.00000000\n",
      "Iteration 14, loss = 8779340193346599.00000000\n",
      "Iteration 15, loss = 8161701825609054.00000000\n",
      "Iteration 16, loss = 7632102388757609.00000000\n",
      "Iteration 17, loss = 7298021646151120.00000000\n",
      "Iteration 18, loss = 7037393193655040.00000000\n",
      "Iteration 19, loss = 6864654174271565.00000000\n",
      "Iteration 20, loss = 6697552651973083.00000000\n",
      "Iteration 21, loss = 6549667340940541.00000000\n",
      "Iteration 22, loss = 6404721557477468.00000000\n",
      "Iteration 23, loss = 6273090117248350.00000000\n",
      "Iteration 24, loss = 6136206762017560.00000000\n",
      "Iteration 25, loss = 6001447103474885.00000000\n",
      "Iteration 26, loss = 5873565159672845.00000000\n",
      "Iteration 27, loss = 5740725578298914.00000000\n",
      "Iteration 28, loss = 5606400776825841.00000000\n",
      "Iteration 29, loss = 5479539948093507.00000000\n",
      "Iteration 30, loss = 5341669142578653.00000000\n",
      "Iteration 31, loss = 5203229211381284.00000000\n",
      "Iteration 32, loss = 5062194164684983.00000000\n",
      "Iteration 33, loss = 4915307741069494.00000000\n",
      "Iteration 34, loss = 4774046487993329.00000000\n",
      "Iteration 35, loss = 4625280218032630.00000000\n",
      "Iteration 36, loss = 4482024216237778.00000000\n",
      "Iteration 37, loss = 4347578950352589.00000000\n",
      "Iteration 38, loss = 4186888954470734.50000000\n",
      "Iteration 39, loss = 4045111037860137.00000000\n",
      "Iteration 40, loss = 3899997417236941.50000000\n",
      "Iteration 41, loss = 3759271632654575.00000000\n",
      "Iteration 42, loss = 3621936016019620.00000000\n",
      "Iteration 43, loss = 3496162630462079.50000000\n",
      "Iteration 44, loss = 3354715657925859.00000000\n",
      "Iteration 45, loss = 3218761196044523.50000000\n",
      "Iteration 46, loss = 3096169089255044.50000000\n",
      "Iteration 47, loss = 2973296475338320.50000000\n",
      "Iteration 48, loss = 2854646427148609.50000000\n",
      "Iteration 49, loss = 2746863010162829.00000000\n",
      "Iteration 50, loss = 2631766895860788.00000000\n",
      "Iteration 51, loss = 2517099029006146.00000000\n",
      "Iteration 52, loss = 2416446852598399.50000000\n",
      "Iteration 53, loss = 2319282447499351.50000000\n",
      "Iteration 54, loss = 2216252235836323.00000000\n",
      "Iteration 55, loss = 2116931215346812.25000000\n",
      "Iteration 56, loss = 2030244406201221.50000000\n",
      "Iteration 57, loss = 1940490602826770.00000000\n",
      "Iteration 58, loss = 1857557258272393.75000000\n",
      "Iteration 59, loss = 1778103541310057.75000000\n",
      "Iteration 60, loss = 1695737704197584.50000000\n",
      "Iteration 61, loss = 1623475764375725.75000000\n",
      "Iteration 62, loss = 1556658714978481.00000000\n",
      "Iteration 63, loss = 1484267692405483.00000000\n",
      "Iteration 64, loss = 1417740979711618.50000000\n",
      "Iteration 65, loss = 1354227595477166.25000000\n",
      "Iteration 66, loss = 1294683114322430.00000000\n",
      "Iteration 67, loss = 1238065911770975.25000000\n",
      "Iteration 68, loss = 1184337016799379.75000000\n",
      "Iteration 69, loss = 1129612402736030.50000000\n",
      "Iteration 70, loss = 1081619363412499.87500000\n",
      "Iteration 71, loss = 1033077165816081.75000000\n",
      "Iteration 72, loss = 989864972578120.50000000\n",
      "Iteration 73, loss = 950163203027605.75000000\n",
      "Iteration 74, loss = 908268554893985.25000000\n",
      "Iteration 75, loss = 868215381273504.25000000\n",
      "Iteration 76, loss = 832278079425532.00000000\n",
      "Iteration 77, loss = 797428094170822.87500000\n",
      "Iteration 78, loss = 765135963944035.62500000\n",
      "Iteration 79, loss = 732005548382610.12500000\n",
      "Iteration 80, loss = 705095792935014.37500000\n",
      "Iteration 81, loss = 674556495044340.50000000\n",
      "Iteration 82, loss = 647036997927053.87500000\n",
      "Iteration 83, loss = 622242565133689.75000000\n",
      "Iteration 84, loss = 595422031232874.75000000\n",
      "Iteration 85, loss = 572920137810288.62500000\n",
      "Iteration 86, loss = 552974904000016.93750000\n",
      "Iteration 87, loss = 530083277700004.56250000\n",
      "Iteration 88, loss = 509304980883929.18750000\n",
      "Iteration 89, loss = 489214901379390.93750000\n",
      "Iteration 90, loss = 471451839090490.75000000\n",
      "Iteration 91, loss = 453025532563239.87500000\n",
      "Iteration 92, loss = 436067195157342.37500000\n",
      "Iteration 93, loss = 420412018204269.12500000\n",
      "Iteration 94, loss = 404547365490642.25000000\n",
      "Iteration 95, loss = 390095309443591.37500000\n",
      "Iteration 96, loss = 375118438833278.87500000\n",
      "Iteration 97, loss = 361417438706851.43750000\n",
      "Iteration 98, loss = 348643728004764.50000000\n",
      "Iteration 99, loss = 336913162742171.06250000\n",
      "Iteration 100, loss = 324227194170848.00000000\n",
      "Iteration 101, loss = 313817516124606.12500000\n",
      "Iteration 102, loss = 302278959383644.06250000\n",
      "Iteration 103, loss = 291840624841466.68750000\n",
      "Iteration 104, loss = 281684683464436.62500000\n",
      "Iteration 105, loss = 271615507172981.87500000\n",
      "Iteration 106, loss = 261844324334830.28125000\n",
      "Iteration 107, loss = 252898105770378.56250000\n",
      "Iteration 108, loss = 244346852907208.00000000\n",
      "Iteration 109, loss = 236649082898162.87500000\n",
      "Iteration 110, loss = 228172799519684.21875000\n",
      "Iteration 111, loss = 220361560852261.90625000\n",
      "Iteration 112, loss = 212945541189463.15625000\n",
      "Iteration 113, loss = 205491943343133.34375000\n",
      "Iteration 114, loss = 198614969343808.71875000\n",
      "Iteration 115, loss = 192083171332981.00000000\n",
      "Iteration 116, loss = 186680488863166.46875000\n",
      "Iteration 117, loss = 179560556525395.12500000\n",
      "Iteration 118, loss = 173935086499675.40625000\n",
      "Iteration 119, loss = 167938487813206.81250000\n",
      "Iteration 120, loss = 162164596432791.40625000\n",
      "Iteration 121, loss = 156768241871258.40625000\n",
      "Iteration 122, loss = 151509810230365.00000000\n",
      "Iteration 123, loss = 146313550183729.28125000\n",
      "Iteration 124, loss = 141918380114716.78125000\n",
      "Iteration 125, loss = 137054716344850.98437500\n",
      "Iteration 126, loss = 132614842243614.48437500\n",
      "Iteration 127, loss = 128353702548980.76562500\n",
      "Iteration 128, loss = 124161678865160.26562500\n",
      "Iteration 129, loss = 120133331780438.50000000\n",
      "Iteration 130, loss = 116096300086809.23437500\n",
      "Iteration 131, loss = 112162041472835.95312500\n",
      "Iteration 132, loss = 109193416065008.51562500\n",
      "Iteration 133, loss = 105067225202569.70312500\n",
      "Iteration 134, loss = 102032671661283.95312500\n",
      "Iteration 135, loss = 98607532354645.43750000\n",
      "Iteration 136, loss = 95441062333366.68750000\n",
      "Iteration 137, loss = 92282673270055.01562500\n",
      "Iteration 138, loss = 89471319326183.43750000\n",
      "Iteration 139, loss = 86605738894220.12500000\n",
      "Iteration 140, loss = 83623321631415.26562500\n",
      "Iteration 141, loss = 81080691732222.21875000\n",
      "Iteration 142, loss = 78626439406784.50000000\n",
      "Iteration 143, loss = 76138050871330.85937500\n",
      "Iteration 144, loss = 73838642388220.87500000\n",
      "Iteration 145, loss = 71378997509135.59375000\n",
      "Iteration 146, loss = 69028816252129.82812500\n",
      "Iteration 147, loss = 67000735082430.58593750\n",
      "Iteration 148, loss = 64890797314368.96093750\n",
      "Iteration 149, loss = 62886356640477.14843750\n",
      "Iteration 150, loss = 60799086469714.81250000\n",
      "Iteration 151, loss = 58967973224208.02343750\n",
      "Iteration 152, loss = 57027821972193.73437500\n",
      "Iteration 153, loss = 55339044423829.17968750\n",
      "Iteration 154, loss = 53608291363035.99218750\n",
      "Iteration 155, loss = 52107402044040.68750000\n",
      "Iteration 156, loss = 50327917800604.91406250\n",
      "Iteration 157, loss = 48715443949312.42968750\n",
      "Iteration 158, loss = 47265887686285.92187500\n",
      "Iteration 159, loss = 45911319661323.07812500\n",
      "Iteration 160, loss = 44561079590318.30468750\n",
      "Iteration 161, loss = 43155570681346.58593750\n",
      "Iteration 162, loss = 41815228732926.82031250\n",
      "Iteration 163, loss = 40607990884299.32812500\n",
      "Iteration 164, loss = 39340598584952.07812500\n",
      "Iteration 165, loss = 38165719688297.23437500\n",
      "Iteration 166, loss = 36961540710346.35937500\n",
      "Iteration 167, loss = 35978614183015.71093750\n",
      "Iteration 168, loss = 35028090564121.52734375\n",
      "Iteration 169, loss = 33866195627725.44140625\n",
      "Iteration 170, loss = 32797689469068.80859375\n",
      "Iteration 171, loss = 31871888565948.02734375\n",
      "Iteration 172, loss = 30914042572696.65234375\n",
      "Iteration 173, loss = 30191434089401.27343750\n",
      "Iteration 174, loss = 29141445140136.26171875\n",
      "Iteration 175, loss = 28282911422644.21093750\n",
      "Iteration 176, loss = 27464550209308.87500000\n",
      "Iteration 177, loss = 26658860452677.67968750\n",
      "Iteration 178, loss = 25906483806851.10156250\n",
      "Iteration 179, loss = 25100060861162.89843750\n",
      "Iteration 180, loss = 24379967808626.80078125\n",
      "Iteration 181, loss = 23693669382346.58593750\n",
      "Iteration 182, loss = 23061987559529.80859375\n",
      "Iteration 183, loss = 22397749021913.23828125\n",
      "Iteration 184, loss = 21727803590472.43359375\n",
      "Iteration 185, loss = 21180379926052.51171875\n",
      "Iteration 186, loss = 20581757929057.45312500\n",
      "Iteration 187, loss = 20014837850335.34375000\n",
      "Iteration 188, loss = 19472129741004.07812500\n",
      "Iteration 189, loss = 18909729439059.18750000\n",
      "Iteration 190, loss = 18407850274706.77343750\n",
      "Iteration 191, loss = 17905563444338.32421875\n",
      "Iteration 192, loss = 17486576250416.51953125\n",
      "Iteration 193, loss = 16919119775431.32031250\n",
      "Iteration 194, loss = 16507626927754.67578125\n",
      "Iteration 195, loss = 16083912460328.34375000\n",
      "Iteration 196, loss = 15617334210159.17968750\n",
      "Iteration 197, loss = 15208657877619.81640625\n",
      "Iteration 198, loss = 14836891108150.69921875\n",
      "Iteration 199, loss = 14436448487054.79687500\n",
      "Iteration 200, loss = 14051647118904.13671875\n",
      "Iteration 201, loss = 13708244101766.96093750\n",
      "Iteration 202, loss = 13377516678826.89257812\n",
      "Iteration 203, loss = 13024264750614.58398438\n",
      "Iteration 204, loss = 12686575920363.20117188\n",
      "Iteration 205, loss = 12400262506053.68554688\n",
      "Iteration 206, loss = 12081976076115.38671875\n",
      "Iteration 207, loss = 11759572027795.36914062\n",
      "Iteration 208, loss = 11529701540588.91406250\n",
      "Iteration 209, loss = 11207583580731.11523438\n",
      "Iteration 210, loss = 10928159515320.69335938\n",
      "Iteration 211, loss = 10657105265172.88281250\n",
      "Iteration 212, loss = 10419259126692.10742188\n",
      "Iteration 213, loss = 10154310776990.81250000\n",
      "Iteration 214, loss = 9916473785601.91015625\n",
      "Iteration 215, loss = 9680089831939.19726562\n",
      "Iteration 216, loss = 9479043435809.44921875\n",
      "Iteration 217, loss = 9278561622896.12890625\n",
      "Iteration 218, loss = 9068391943804.22070312\n",
      "Iteration 219, loss = 8839395301537.21093750\n",
      "Iteration 220, loss = 8641665707936.05664062\n",
      "Iteration 221, loss = 8432146768759.37011719\n",
      "Iteration 222, loss = 8260030136503.38378906\n",
      "Iteration 223, loss = 8061685367439.08691406\n",
      "Iteration 224, loss = 7899812282226.86035156\n",
      "Iteration 225, loss = 7729902478915.38867188\n",
      "Iteration 226, loss = 7566878465025.74804688\n",
      "Iteration 227, loss = 7399563200978.99707031\n",
      "Iteration 228, loss = 7258857144558.41992188\n",
      "Iteration 229, loss = 7101492208144.20214844\n",
      "Iteration 230, loss = 6939432182625.87500000\n",
      "Iteration 231, loss = 6797515863356.25292969\n",
      "Iteration 232, loss = 6676977178873.77246094\n",
      "Iteration 233, loss = 6531793100102.78515625\n",
      "Iteration 234, loss = 6393654511541.76269531\n",
      "Iteration 235, loss = 6276805880314.34863281\n",
      "Iteration 236, loss = 6146915459975.71289062\n",
      "Iteration 237, loss = 6027726778294.95703125\n",
      "Iteration 238, loss = 5908220180687.63281250\n",
      "Iteration 239, loss = 5810921088536.86132812\n",
      "Iteration 240, loss = 5698260897441.42285156\n",
      "Iteration 241, loss = 5574697094969.71875000\n",
      "Iteration 242, loss = 5481214061009.18750000\n",
      "Iteration 243, loss = 5380899811115.30957031\n",
      "Iteration 244, loss = 5269947415808.70800781\n",
      "Iteration 245, loss = 5185573163879.68945312\n",
      "Iteration 246, loss = 5083126118677.83300781\n",
      "Iteration 247, loss = 5004523760304.76269531\n",
      "Iteration 248, loss = 4906093288590.57324219\n",
      "Iteration 249, loss = 4816820495582.45996094\n",
      "Iteration 250, loss = 4740535380552.20605469\n",
      "Iteration 251, loss = 4653671659768.63183594\n",
      "Iteration 252, loss = 4579898426596.55566406\n",
      "Iteration 253, loss = 4504432446384.96679688\n",
      "Iteration 254, loss = 4420921494447.11328125\n",
      "Iteration 255, loss = 4350947413035.76757812\n",
      "Iteration 256, loss = 4278953086887.62060547\n",
      "Iteration 257, loss = 4209294731889.21337891\n",
      "Iteration 258, loss = 4134527508324.53662109\n",
      "Iteration 259, loss = 4072174857025.24755859\n",
      "Iteration 260, loss = 4006949882382.19091797\n",
      "Iteration 261, loss = 3944492500713.54931641\n",
      "Iteration 262, loss = 3879775204172.63916016\n",
      "Iteration 263, loss = 3816070112148.51660156\n",
      "Iteration 264, loss = 3757335220052.41406250\n",
      "Iteration 265, loss = 3703438239387.12353516\n",
      "Iteration 266, loss = 3644911055096.88378906\n",
      "Iteration 267, loss = 3589138521273.88134766\n",
      "Iteration 268, loss = 3537529645895.60839844\n",
      "Iteration 269, loss = 3490794204593.76855469\n",
      "Iteration 270, loss = 3433139619561.30273438\n",
      "Iteration 271, loss = 3384056979192.69775391\n",
      "Iteration 272, loss = 3337018491541.33300781\n",
      "Iteration 273, loss = 3286394008246.09960938\n",
      "Iteration 274, loss = 3238905662398.01220703\n",
      "Iteration 275, loss = 3192216048422.97802734\n",
      "Iteration 276, loss = 3152421025817.37500000\n",
      "Iteration 277, loss = 3105413077432.66601562\n",
      "Iteration 278, loss = 3062621710772.41796875\n",
      "Iteration 279, loss = 3022201122511.38037109\n",
      "Iteration 280, loss = 2977773647245.44482422\n",
      "Iteration 281, loss = 2938145453318.88818359\n",
      "Iteration 282, loss = 2897351583866.66601562\n",
      "Iteration 283, loss = 2860865514506.48095703\n",
      "Iteration 284, loss = 2822773353340.15527344\n",
      "Iteration 285, loss = 2787735342655.55273438\n",
      "Iteration 286, loss = 2752584380957.21337891\n",
      "Iteration 287, loss = 2714398309483.87255859\n",
      "Iteration 288, loss = 2678862964243.17773438\n",
      "Iteration 289, loss = 2645946304234.38574219\n",
      "Iteration 290, loss = 2613139136584.84619141\n",
      "Iteration 291, loss = 2580412302505.00830078\n",
      "Iteration 292, loss = 2546404195148.30175781\n",
      "Iteration 293, loss = 2515052753192.61523438\n",
      "Iteration 294, loss = 2486438265239.84423828\n",
      "Iteration 295, loss = 2455610357683.95898438\n",
      "Iteration 296, loss = 2424499065547.43603516\n",
      "Iteration 297, loss = 2395496640872.29833984\n",
      "Iteration 298, loss = 2366756061085.75976562\n",
      "Iteration 299, loss = 2339791939802.20605469\n",
      "Iteration 300, loss = 2312149349094.35351562\n",
      "Iteration 301, loss = 2284245048202.52929688\n",
      "Iteration 302, loss = 2257354335445.52880859\n",
      "Iteration 303, loss = 2232920168967.43652344\n",
      "Iteration 304, loss = 2207709047765.13134766\n",
      "Iteration 305, loss = 2181541290566.87207031\n",
      "Iteration 306, loss = 2157416128748.68701172\n",
      "Iteration 307, loss = 2130689895087.73803711\n",
      "Iteration 308, loss = 2108603714416.93115234\n",
      "Iteration 309, loss = 2082776787248.53588867\n",
      "Iteration 310, loss = 2059762116782.63696289\n",
      "Iteration 311, loss = 2036698241259.07177734\n",
      "Iteration 312, loss = 2013961405861.67968750\n",
      "Iteration 313, loss = 1991704514914.63378906\n",
      "Iteration 314, loss = 1969952953926.85156250\n",
      "Iteration 315, loss = 1949364136520.90332031\n",
      "Iteration 316, loss = 1928174237163.89208984\n",
      "Iteration 317, loss = 1908087436633.74365234\n",
      "Iteration 318, loss = 1887461678661.76879883\n",
      "Iteration 319, loss = 1866925551998.44580078\n",
      "Iteration 320, loss = 1848006173602.28540039\n",
      "Iteration 321, loss = 1829232517802.08300781\n",
      "Iteration 322, loss = 1809267762338.60229492\n",
      "Iteration 323, loss = 1791198586195.40722656\n",
      "Iteration 324, loss = 1772378023518.26171875\n",
      "Iteration 325, loss = 1755316012452.81616211\n",
      "Iteration 326, loss = 1737756410269.49633789\n",
      "Iteration 327, loss = 1720262402176.31616211\n",
      "Iteration 328, loss = 1703064805138.68945312\n",
      "Iteration 329, loss = 1688156784926.45581055\n",
      "Iteration 330, loss = 1671592910495.51660156\n",
      "Iteration 331, loss = 1654021600693.56420898\n",
      "Iteration 332, loss = 1637853631001.91406250\n",
      "Iteration 333, loss = 1621595530216.01831055\n",
      "Iteration 334, loss = 1606948651987.32202148\n",
      "Iteration 335, loss = 1592153660866.55590820\n",
      "Iteration 336, loss = 1577411413694.58325195\n",
      "Iteration 337, loss = 1562154168527.89233398\n",
      "Iteration 338, loss = 1548660148336.23144531\n",
      "Iteration 339, loss = 1533392365981.29101562\n",
      "Iteration 340, loss = 1520794410242.76440430\n",
      "Iteration 341, loss = 1507000596127.81079102\n",
      "Iteration 342, loss = 1492012353040.62158203\n",
      "Iteration 343, loss = 1480435360761.81030273\n",
      "Iteration 344, loss = 1466364908462.97607422\n",
      "Iteration 345, loss = 1453766757144.54296875\n",
      "Iteration 346, loss = 1441062827280.24316406\n",
      "Iteration 347, loss = 1430631512721.96899414\n",
      "Iteration 348, loss = 1417179258796.69433594\n",
      "Iteration 349, loss = 1404527657711.47167969\n",
      "Iteration 350, loss = 1393273051653.50000000\n",
      "Iteration 351, loss = 1381750609580.15966797\n",
      "Iteration 352, loss = 1370268691568.81860352\n",
      "Iteration 353, loss = 1359140261894.28857422\n",
      "Iteration 354, loss = 1347362933995.53930664\n",
      "Iteration 355, loss = 1336868380497.75415039\n",
      "Iteration 356, loss = 1324933925345.63085938\n",
      "Iteration 357, loss = 1315417227982.31005859\n",
      "Iteration 358, loss = 1304445805963.12670898\n",
      "Iteration 359, loss = 1294028198724.12695312\n",
      "Iteration 360, loss = 1284082068514.54443359\n",
      "Iteration 361, loss = 1273198076360.34008789\n",
      "Iteration 362, loss = 1263931930630.45825195\n",
      "Iteration 363, loss = 1253745154639.84252930\n",
      "Iteration 364, loss = 1243702956884.84423828\n",
      "Iteration 365, loss = 1234499194321.53491211\n",
      "Iteration 366, loss = 1225830094122.56665039\n",
      "Iteration 367, loss = 1215574169160.55322266\n",
      "Iteration 368, loss = 1206691225424.61938477\n",
      "Iteration 369, loss = 1197733004540.44287109\n",
      "Iteration 370, loss = 1188686905956.25927734\n",
      "Iteration 371, loss = 1179915993090.35253906\n",
      "Iteration 372, loss = 1171860302460.83251953\n",
      "Iteration 373, loss = 1162986589127.29077148\n",
      "Iteration 374, loss = 1154319903330.03710938\n",
      "Iteration 375, loss = 1146165758305.82788086\n",
      "Iteration 376, loss = 1138158021620.76147461\n",
      "Iteration 377, loss = 1129865449906.13427734\n",
      "Iteration 378, loss = 1121492693946.75585938\n",
      "Iteration 379, loss = 1113914517308.97851562\n",
      "Iteration 380, loss = 1105809850963.21752930\n",
      "Iteration 381, loss = 1096598405678.37109375\n",
      "Iteration 382, loss = 1088908671536.53100586\n",
      "Iteration 383, loss = 1081424914816.98229980\n",
      "Iteration 384, loss = 1073426385698.40209961\n",
      "Iteration 385, loss = 1065532833776.24975586\n",
      "Iteration 386, loss = 1057736709993.25500488\n",
      "Iteration 387, loss = 1049881607325.63562012\n",
      "Iteration 388, loss = 1042466458845.98376465\n",
      "Iteration 389, loss = 1034989909087.55627441\n",
      "Iteration 390, loss = 1027525033197.98229980\n",
      "Iteration 391, loss = 1020346970226.14294434\n",
      "Iteration 392, loss = 1012548400150.20275879\n",
      "Iteration 393, loss = 1006216896436.29663086\n",
      "Iteration 394, loss = 999158497926.51452637\n",
      "Iteration 395, loss = 991694790276.67431641\n",
      "Iteration 396, loss = 985248181609.69714355\n",
      "Iteration 397, loss = 978824832312.03662109\n",
      "Iteration 398, loss = 971319398994.60437012\n",
      "Iteration 399, loss = 964802257330.68835449\n",
      "Iteration 400, loss = 958538102863.52294922\n",
      "Iteration 401, loss = 951856347347.42687988\n",
      "Iteration 402, loss = 945121922847.78625488\n",
      "Iteration 403, loss = 939597773191.15954590\n",
      "Iteration 404, loss = 932728667852.17028809\n",
      "Iteration 405, loss = 927248281549.59472656\n",
      "Iteration 406, loss = 920922487325.51086426\n",
      "Iteration 407, loss = 915255155625.27832031\n",
      "Iteration 408, loss = 909420327087.32568359\n",
      "Iteration 409, loss = 902888085584.18151855\n",
      "Iteration 410, loss = 897509392543.17480469\n",
      "Iteration 411, loss = 891276999713.88500977\n",
      "Iteration 412, loss = 885697870371.88293457\n",
      "Iteration 413, loss = 880278021893.71252441\n",
      "Iteration 414, loss = 873758275593.81286621\n",
      "Iteration 415, loss = 868474329631.65551758\n",
      "Iteration 416, loss = 862830212977.75573730\n",
      "Iteration 417, loss = 856875318556.34667969\n",
      "Iteration 418, loss = 851372039115.44299316\n",
      "Iteration 419, loss = 845489806195.50708008\n",
      "Iteration 420, loss = 840178725959.38391113\n",
      "Iteration 421, loss = 834975881101.37231445\n",
      "Iteration 422, loss = 829300204354.01977539\n",
      "Iteration 423, loss = 823426026767.77722168\n",
      "Iteration 424, loss = 818484767643.47058105\n",
      "Iteration 425, loss = 813107831961.72265625\n",
      "Iteration 426, loss = 807796931311.51550293\n",
      "Iteration 427, loss = 802672522597.23681641\n",
      "Iteration 428, loss = 797312235603.55224609\n",
      "Iteration 429, loss = 792363432201.53955078\n",
      "Iteration 430, loss = 786366802299.75683594\n",
      "Iteration 431, loss = 780218085239.01782227\n",
      "Iteration 432, loss = 775051808927.40234375\n",
      "Iteration 433, loss = 768295915427.80639648\n",
      "Iteration 434, loss = 762696133947.48132324\n",
      "Iteration 435, loss = 755829770622.93933105\n",
      "Iteration 436, loss = 749960742203.05920410\n",
      "Iteration 437, loss = 743827194900.29150391\n",
      "Iteration 438, loss = 738885382476.52734375\n",
      "Iteration 439, loss = 732175189683.68713379\n",
      "Iteration 440, loss = 726420480457.31970215\n",
      "Iteration 441, loss = 720450373122.70690918\n",
      "Iteration 442, loss = 714580612897.61096191\n",
      "Iteration 443, loss = 708975339829.00671387\n",
      "Iteration 444, loss = 702138306585.95300293\n",
      "Iteration 445, loss = 697434158649.64135742\n",
      "Iteration 446, loss = 691724844080.22851562\n",
      "Iteration 447, loss = 685305268439.43823242\n",
      "Iteration 448, loss = 680092229012.28857422\n",
      "Iteration 449, loss = 674667659855.37182617\n",
      "Iteration 450, loss = 668992463056.13989258\n",
      "Iteration 451, loss = 663709134287.35217285\n",
      "Iteration 452, loss = 658089886839.68603516\n",
      "Iteration 453, loss = 652504675209.83410645\n",
      "Iteration 454, loss = 647245767578.83288574\n",
      "Iteration 455, loss = 642103292633.24621582\n",
      "Iteration 456, loss = 636693713282.96191406\n",
      "Iteration 457, loss = 631623841336.96667480\n",
      "Iteration 458, loss = 625933728237.99255371\n",
      "Iteration 459, loss = 620759084715.57141113\n",
      "Iteration 460, loss = 614977847897.66186523\n",
      "Iteration 461, loss = 609384047558.21154785\n",
      "Iteration 462, loss = 604767221308.11682129\n",
      "Iteration 463, loss = 598861271164.34008789\n",
      "Iteration 464, loss = 593151930046.29321289\n",
      "Iteration 465, loss = 588544945666.88256836\n",
      "Iteration 466, loss = 582908986751.91699219\n",
      "Iteration 467, loss = 578091817864.60778809\n",
      "Iteration 468, loss = 572710749964.58935547\n",
      "Iteration 469, loss = 567779248311.19311523\n",
      "Iteration 470, loss = 562998753976.32214355\n",
      "Iteration 471, loss = 557551616591.35168457\n",
      "Iteration 472, loss = 553517187485.46752930\n",
      "Iteration 473, loss = 548139354633.90698242\n",
      "Iteration 474, loss = 543127438554.06115723\n",
      "Iteration 475, loss = 538692250277.71832275\n",
      "Iteration 476, loss = 533541859420.66082764\n",
      "Iteration 477, loss = 529312584218.36383057\n",
      "Iteration 478, loss = 524843546413.07543945\n",
      "Iteration 479, loss = 519828008576.85449219\n",
      "Iteration 480, loss = 515846859024.50366211\n",
      "Iteration 481, loss = 511751639769.98480225\n",
      "Iteration 482, loss = 506759895315.31909180\n",
      "Iteration 483, loss = 502409361138.71575928\n",
      "Iteration 484, loss = 498117316812.32812500\n",
      "Iteration 485, loss = 493587053477.77258301\n",
      "Iteration 486, loss = 489858957651.21301270\n",
      "Iteration 487, loss = 485716939852.69976807\n",
      "Iteration 488, loss = 481156512224.17968750\n",
      "Iteration 489, loss = 477467712067.40502930\n",
      "Iteration 490, loss = 473073532996.33715820\n",
      "Iteration 491, loss = 469283544202.15856934\n",
      "Iteration 492, loss = 465635229484.27282715\n",
      "Iteration 493, loss = 461399543297.42004395\n",
      "Iteration 494, loss = 457475166450.28997803\n",
      "Iteration 495, loss = 453695835442.44519043\n",
      "Iteration 496, loss = 449766123946.40893555\n",
      "Iteration 497, loss = 446439732679.96392822\n",
      "Iteration 498, loss = 442280826938.37811279\n",
      "Iteration 499, loss = 438510175516.47998047\n",
      "Iteration 500, loss = 434790565953.73394775\n",
      "Iteration 501, loss = 431437356030.38000488\n",
      "Iteration 502, loss = 427754714772.78710938\n",
      "Iteration 503, loss = 424521308493.79064941\n",
      "Iteration 504, loss = 421913230568.78955078\n",
      "Iteration 505, loss = 417204594902.76580811\n",
      "Iteration 506, loss = 413370416140.32745361\n",
      "Iteration 507, loss = 409803056903.12854004\n",
      "Iteration 508, loss = 406661853140.37670898\n",
      "Iteration 509, loss = 403687971175.81744385\n",
      "Iteration 510, loss = 400472376070.67095947\n",
      "Iteration 511, loss = 396732715133.85150146\n",
      "Iteration 512, loss = 393553048669.59539795\n",
      "Iteration 513, loss = 390812775252.90783691\n",
      "Iteration 514, loss = 388241604324.21051025\n",
      "Iteration 515, loss = 386226186179.23168945\n",
      "Iteration 516, loss = 382263421175.57531738\n",
      "Iteration 517, loss = 380155594606.77770996\n",
      "Iteration 518, loss = 378936907887.47460938\n",
      "Iteration 519, loss = 380872942251.39355469\n",
      "Iteration 520, loss = 385428945911.30810547\n",
      "Iteration 521, loss = 392216920954.26062012\n",
      "Iteration 522, loss = 385047549158.41223145\n",
      "Iteration 523, loss = 384468700426.87231445\n",
      "Iteration 524, loss = 397747459494.08465576\n",
      "Iteration 525, loss = 431462777253.98480225\n",
      "Iteration 526, loss = 464051420018.07098389\n",
      "Iteration 527, loss = 483449929696.39648438\n",
      "Iteration 528, loss = 476401445623.41082764\n",
      "Iteration 529, loss = 491187627522.22021484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.9853199639609076"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(800, 500, 100), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9658812395500416.00000000\n",
      "Iteration 2, loss = 4880336480572720.00000000\n",
      "Iteration 3, loss = 4332360837154315.00000000\n",
      "Iteration 4, loss = 4194391133010503.00000000\n",
      "Iteration 5, loss = 4168126972030673.50000000\n",
      "Iteration 6, loss = 4134115957056925.00000000\n",
      "Iteration 7, loss = 4171014934373055.00000000\n",
      "Iteration 8, loss = 4133475728046373.00000000\n",
      "Iteration 9, loss = 4162364366452409.00000000\n",
      "Iteration 10, loss = 4101856638924171.50000000\n",
      "Iteration 11, loss = 4104163450234917.50000000\n",
      "Iteration 12, loss = 4117484279157487.00000000\n",
      "Iteration 13, loss = 4105489633723290.50000000\n",
      "Iteration 14, loss = 4116760156048518.00000000\n",
      "Iteration 15, loss = 4118411532866270.50000000\n",
      "Iteration 16, loss = 4136889469632305.00000000\n",
      "Iteration 17, loss = 4121678957485150.00000000\n",
      "Iteration 18, loss = 4110214517932114.50000000\n",
      "Iteration 19, loss = 4114349507400780.00000000\n",
      "Iteration 20, loss = 4104246014542306.50000000\n",
      "Iteration 21, loss = 4107820770373103.50000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(1000, 100), max_iter=1000, random_state=21,\n",
       "             verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(1000, 100), max_iter=1000, random_state=21,\n",
       "             verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(1000, 100), max_iter=1000, random_state=21,\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from evaluation_features.npy\n",
    "with open('evaluation_features.npy', 'rb') as f:\n",
    "    evaluation_features = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from evaluation_features.npy\n",
    "with open('evaluation_features_short.npy', 'rb') as f:\n",
    "    evaluation_features = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = pd.read_csv('data/test.csv', usecols=[0]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4398, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4398,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict revenue for test data\n",
    "preds = regr.predict(evaluation_features)\n",
    "# save predictions to a csv file\n",
    "np.savetxt('result_mlp3.csv', np.concatenate((test_ids, preds.reshape(-1, 1)), axis=1), delimiter=',', header='id,revenue', comments='', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = regr.predict(scaler.transform(evaluation_features))\n",
    "\n",
    "np.savetxt('result_mlp5.csv', np.concatenate((test_ids, preds.reshape(-1, 1)), axis=1), delimiter=',', header='id,revenue', comments='', fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict revenue for test data\n",
    "preds = mlp_regr.predict(evaluation_features)\n",
    "# save predictions to a csv file\n",
    "np.savetxt('result_mlp-f.csv', np.concatenate((test_ids, preds.reshape(-1, 1)), axis=1), delimiter=',', header='id,revenue', comments='', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10649169009674728.00000000\n",
      "Iteration 2, loss = 7423416665176381.00000000\n",
      "Iteration 3, loss = 4387045960520514.50000000\n",
      "Iteration 4, loss = 4478326826968907.00000000\n",
      "Iteration 5, loss = 4245883173492859.00000000\n",
      "Iteration 6, loss = 4256421994430762.00000000\n",
      "Iteration 7, loss = 4224290369340590.00000000\n",
      "Iteration 8, loss = 4220636750145438.00000000\n",
      "Iteration 9, loss = 4220286952294331.50000000\n",
      "Iteration 10, loss = 4219868623491017.50000000\n",
      "Iteration 11, loss = 4222761196760835.50000000\n",
      "Iteration 12, loss = 4226513886461231.00000000\n",
      "Iteration 13, loss = 4234880087621019.50000000\n",
      "Iteration 14, loss = 4209723827266092.50000000\n",
      "Iteration 15, loss = 4217978707976180.00000000\n",
      "Iteration 16, loss = 4215376911955309.00000000\n",
      "Iteration 17, loss = 4249808424897363.50000000\n",
      "Iteration 18, loss = 4251845922937924.50000000\n",
      "Iteration 19, loss = 4234977163333819.50000000\n",
      "Iteration 20, loss = 4332773937400825.00000000\n",
      "Iteration 21, loss = 4287544090011541.50000000\n",
      "Iteration 22, loss = 4236184431768347.50000000\n",
      "Iteration 23, loss = 4221311517582032.00000000\n",
      "Iteration 24, loss = 4246232925625554.00000000\n",
      "Iteration 25, loss = 4283001165480641.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.596870894442328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "regr = MLPRegressor(hidden_layer_sizes=(500, 200), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data and transform the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the test data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean and standard deviation for each column of the training data\n",
    "means = X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index where the mean is > 10000\n",
    "idx = np.where(means > 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([768]),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize only some columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data and transform the data\n",
    "X_train_scaled = scaler.fit_transform(X_train[:, 768:769])\n",
    "\n",
    "# transform the test data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test[:, 768:769])\n",
    "\n",
    "# replace\n",
    "X_train[:, 768:769] = X_train_scaled\n",
    "X_test[:, 768:769] = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mean Squared Logarithmic Error cannot be used when targets contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m regr\u001b[39m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[1;32m      8\u001b[0m preds \u001b[39m=\u001b[39m regr\u001b[39m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m---> 10\u001b[0m rmsle(y_test, preds)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mrmsle\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmsle\u001b[39m(y_true, y_pred):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msqrt(mean_squared_log_error(y_true, y_pred))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:525\u001b[0m, in \u001b[0;36mmean_squared_log_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    522\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    524\u001b[0m \u001b[39mif\u001b[39;00m (y_true \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m (y_pred \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many():\n\u001b[0;32m--> 525\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMean Squared Logarithmic Error cannot be used when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtargets contain negative values.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m mean_squared_error(\n\u001b[1;32m    531\u001b[0m     np\u001b[39m.\u001b[39mlog1p(y_true),\n\u001b[1;32m    532\u001b[0m     np\u001b[39m.\u001b[39mlog1p(y_pred),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m     squared\u001b[39m=\u001b[39msquared,\n\u001b[1;32m    536\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
     ]
    }
   ],
   "source": [
    "# use linear regression for revenue prediction\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(regr, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
