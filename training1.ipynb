{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# with open('features.npy', 'rb') as f:\n",
    "with open('features_short_new.npy', 'rb') as f:\n",
    "    features = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load only last column of a csv file\n",
    "labels = pd.read_csv('data/train.csv', usecols=[22]).values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mlp for revenue prediction\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10959152420755202.00000000\n",
      "Iteration 2, loss = 5714702516873238.00000000\n",
      "Iteration 3, loss = 4324062301870460.50000000\n",
      "Iteration 4, loss = 4277536412974506.50000000\n",
      "Iteration 5, loss = 4406643460095309.50000000\n",
      "Iteration 6, loss = 4253841160107158.00000000\n",
      "Iteration 7, loss = 4224070372485407.50000000\n",
      "Iteration 8, loss = 4248647569966187.50000000\n",
      "Iteration 9, loss = 4251279468394391.00000000\n",
      "Iteration 10, loss = 4247901426517860.50000000\n",
      "Iteration 11, loss = 4333332539829557.00000000\n",
      "Iteration 12, loss = 4397680134887775.50000000\n",
      "Iteration 13, loss = 4241420667515744.50000000\n",
      "Training loss did not improve more than tol=0.000010 for 5 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.595710952306413"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1000, 200, 200, 50, 50), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=5)  # best\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch to train the regressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RevenuePredictor(nn.Module):\n",
    "    def __init__(self, input_size, drop_prob=0.2):\n",
    "        super(RevenuePredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 800)\n",
    "        self.fc2 = nn.Linear(800, 200)\n",
    "        # self.fc3 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)\n",
    "        # self.fc4 = nn.Linear(200, 50)\n",
    "        # self.fc5 = nn.Linear(200, 50)\n",
    "        # self.fc6 = nn.Linear(50, 1)\n",
    "        self.fc6 = nn.Linear(100, 1)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.gelu(self.fc1(x)))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # x = F.gelu(self.fc4(x))\n",
    "        # x = F.relu(self.fc5(x))\n",
    "        return self.fc6(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            r = self.forward(x)\n",
    "            self.train()\n",
    "            return r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch_mode(model):\n",
    "    outputs = model.predict(torch.from_numpy(X_test).float())\n",
    "    try:\n",
    "        return rmsle(y_test, outputs.detach().numpy())\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch_cuda_mode(model):\n",
    "    outputs = model.predict(torch.from_numpy(X_test).cuda().float())\n",
    "    try:\n",
    "        return rmsle(y_test, outputs.cpu().detach().numpy())\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2318762.64 0 7.889089683500408\n",
      "Training loss: 2306291.7066666665 1 5.533100494291208\n",
      "Training loss: 2174264.6533333333 2 3.8263469484230472\n",
      "Training loss: 1605120.295 3 3.2643268913894246\n",
      "Training loss: 1439742.1183333334 4 3.2150177392585078\n",
      "Training loss: 1429873.835 5 3.160712108603875\n",
      "Training loss: 1446965.025 6 3.129914945356873\n",
      "Training loss: 1423235.7183333333 7 3.0924996165623155\n",
      "Training loss: 1418685.35 8 3.0632722457715826\n",
      "Training loss: 1417076.2666666666 9 3.0405920514041\n",
      "Training loss: 1430459.3133333332 10 3.0380082791980456\n",
      "Training loss: 1456446.89 11 3.0069040966553287\n",
      "Training loss: 1401322.1866666668 12 2.975067575133062\n",
      "Training loss: 1400642.6733333333 13 2.9458872570163157\n",
      "Training loss: 1411213.3 14 2.9467980277662313\n",
      "Training loss: 1429121.39 15 2.9315219684653857\n",
      "Training loss: 1440432.5066666666 16 2.9029898706918655\n",
      "Training loss: 1455684.44 17 2.8968152778574274\n",
      "Training loss: 1420012.5766666667 18 2.8738651548155874\n",
      "Training loss: 1411842.8 19 2.855478820965082\n",
      "Training loss: 1415626.0466666666 20 2.835753691153313\n",
      "Training loss: 1445977.1433333333 21 2.8258630582088635\n",
      "Training loss: 1406794.8016666668 22 2.796381058761407\n",
      "Training loss: 1425305.5833333333 23 2.7817609434146635\n",
      "Training loss: 1421427.0533333332 24 2.7763876277978623\n",
      "Training loss: 1423161.2633333334 25 2.756673080686472\n",
      "Training loss: 1436133.8916666666 26 2.743882466828684\n",
      "Training loss: 1426836.9066666667 27 2.7382550030194\n",
      "Training loss: 1395289.2333333334 28 2.7166982301935\n",
      "Training loss: 1449554.5383333333 29 2.7064678067883725\n",
      "Training loss: 1431251.9716666667 30 2.7114922298894717\n",
      "Training loss: 1421300.1983333332 31 2.682248925230991\n",
      "Training loss: 1453233.8633333333 32 2.66704675986404\n",
      "Training loss: 1405849.2583333333 33 2.670499465981369\n",
      "Training loss: 1425200.27 34 2.653901342462517\n",
      "Training loss: 1398443.1583333334 35 2.6329565934603085\n",
      "Training loss: 1416040.985 36 2.6287229059220936\n",
      "Training loss: 1420118.4366666668 37 2.6322804516820546\n",
      "Training loss: 1417655.545 38 2.611538322409511\n",
      "Training loss: 1443319.38 39 2.599150181260126\n",
      "Training loss: 1432257.7 40 2.5873775944244115\n",
      "Training loss: 1416966.0383333333 41 2.5879785363928716\n",
      "Training loss: 1437736.7383333333 42 2.5725048282616516\n",
      "Training loss: 1418023.14 43 2.571368918729499\n",
      "Training loss: 1420848.7333333334 44 2.564841735525585\n",
      "Training loss: 1432644.7466666666 45 2.560235578785541\n",
      "Training loss: 1428071.51 46 2.553769870465812\n",
      "Training loss: 1437739.34 47 2.5402451432082285\n",
      "Training loss: 1421861.9333333333 48 2.5371209378562685\n",
      "Training loss: 1398836.7416666667 49 2.537976027230197\n",
      "Training loss: 1392532.4833333334 50 2.527668083961573\n",
      "Training loss: 1408652.905 51 2.5279489563480935\n",
      "Training loss: 1428589.4533333334 52 2.5167590572068597\n",
      "Training loss: 1397668.135 53 2.5081695279932807\n",
      "Training loss: 1449021.215 54 2.4987742895165885\n",
      "Training loss: 1411705.45 55 2.4883465569318193\n",
      "Training loss: 1420223.38 56 2.5010144270927133\n",
      "Training loss: 1414397.495 57 2.496571086094616\n",
      "Training loss: 1419545.8916666666 58 2.4682016234577198\n",
      "Training loss: 1414243.1033333333 59 2.482470706489155\n",
      "Training loss: 1423107.4866666666 60 2.4883193641624577\n",
      "Training loss: 1396213.9033333333 61 2.4675912550908556\n",
      "Training loss: 1423179.9816666667 62 2.4671927834466856\n",
      "Training loss: 1418199.7016666667 63 2.4745304760949836\n",
      "Training loss: 1426400.5 64 2.4606935212431513\n",
      "Training loss: 1426350.0933333333 65 2.457440785112311\n",
      "Training loss: 1408174.6833333333 66 2.4753879117616013\n",
      "Training loss: 1433591.625 67 2.4476523199846913\n",
      "Training loss: 1413716.7816666667 68 2.4599994603144504\n",
      "Training loss: 1418896.4266666668 69 2.4434606199551334\n",
      "Training loss: 1442500.17 70 2.454833610993878\n",
      "Training loss: 1430886.2533333334 71 2.4476043772018867\n",
      "Training loss: 1381812.8716666666 72 2.435526404337136\n",
      "Training loss: 1431069.6683333332 73 2.434738665648915\n",
      "Training loss: 1450331.8316666668 74 2.4433588690358894\n",
      "Training loss: 1400809.5666666667 75 2.436015270836601\n",
      "Training loss: 1429800.2833333334 76 2.4227671026667075\n",
      "Training loss: 1409943.81 77 2.429162475575168\n",
      "Training loss: 1427989.0333333334 78 2.424034095265486\n",
      "Training loss: 1385736.915 79 2.423501064789552\n",
      "Training loss: 1410894.9183333332 80 2.4226416474356456\n",
      "Training loss: 1413208.5366666666 81 2.418808299835984\n",
      "Training loss: 1410559.4583333333 82 2.4222818395146124\n",
      "Training loss: 1426427.955 83 2.4137574020982226\n",
      "Training loss: 1434988.255 84 2.4173442245612695\n",
      "Training loss: 1406736.5016666667 85 2.410514775351323\n",
      "Training loss: 1397941.655 86 2.4088427687559113\n",
      "Training loss: 1412243.465 87 2.4070136432937357\n",
      "Training loss: 1404538.4433333334 88 2.406480169340626\n",
      "Training loss: 1418707.0283333333 89 2.401105431864221\n",
      "Training loss: 1415115.2616666667 90 2.4080778069861664\n",
      "Training loss: 1427547.3733333333 91 2.406442125528656\n",
      "Training loss: 1407521.2466666666 92 2.418647896011059\n",
      "Training loss: 1414229.3116666668 93 2.3887444895703207\n",
      "Training loss: 1432150.9566666668 94 2.3876954382492994\n",
      "Training loss: 1395250.6116666666 95 2.4052183296326484\n",
      "Training loss: 1414079.22 96 2.3945581016451603\n",
      "Training loss: 1422259.1416666666 97 2.3956010936707983\n",
      "Training loss: 1426304.3483333334 98 2.4011154835638\n",
      "Training loss: 1399922.8533333333 99 2.404618921679126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "model = RevenuePredictor(X_train.shape[-1])\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# train the network\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = torch.sqrt(criterion(outputs, label))\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))\n",
    "        # break\n",
    "\n",
    "# # test the network\n",
    "# test_loss = 0\n",
    "# for i in range(0, len(X_test), batch_size):\n",
    "#     # get the training data\n",
    "#     inputs = torch.from_numpy(X_test[i:i+batch_size]).float()\n",
    "#     labels = torch.from_numpy(y_test[i:i+batch_size]).float()\n",
    "\n",
    "#     # forward pass\n",
    "#     outputs = model.forward(inputs)\n",
    "#     loss = rmsle(labels, outputs.detach().numpy())\n",
    "#     test_loss += loss.item()\n",
    "# print(f\"Test loss: {test_loss/len(X_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 29668317852248.746 0 16.09964275330125\n",
      "Training loss: 29668317852248.746 1 15.893583973974103\n",
      "Training loss: 29668317852248.746 2 15.671121284027993\n",
      "Training loss: 29668317852248.746 3 15.442167837167407\n",
      "Training loss: 29668317852248.746 4 15.220846952464731\n",
      "Training loss: 29668317852248.746 5 15.012378512734724\n",
      "Training loss: 29668317852248.746 6 14.811386058829523\n",
      "Training loss: 29668317852248.746 7 14.6119695861421\n",
      "Training loss: 29668317852248.746 8 14.41223457310922\n",
      "Training loss: 29668316957463.895 9 14.211040849886864\n",
      "Training loss: 29668315167894.188 10 14.008978196120253\n",
      "Training loss: 29668315167894.188 11 13.806926575093085\n",
      "Training loss: 29668313378324.48 12 13.606269581302962\n",
      "Training loss: 29668313378324.48 13 13.406191633670215\n",
      "Training loss: 29668313378324.48 14 13.206753994119563\n",
      "Training loss: 29668312483539.625 15 13.008300456689593\n",
      "Training loss: 29668308904400.215 16 12.811181415789958\n",
      "Training loss: 29668308904400.215 17 12.615882632209077\n",
      "Training loss: 29668306220045.652 18 12.42275757453945\n",
      "Training loss: 29668303535691.094 19 12.23216726049133\n",
      "Training loss: 29668303535691.094 20 12.04398952278002\n",
      "Training loss: 29668299956551.68 21 11.858506814930541\n",
      "Training loss: 29668297272197.12 22 11.675832734987685\n",
      "Training loss: 29668295482627.414 23 11.495844888186499\n",
      "Training loss: 29668289219133.44 24 11.319843361050392\n",
      "Training loss: 29668281166069.76 25 11.146612756639476\n",
      "Training loss: 29668274007790.934 26 10.97616659506918\n",
      "Training loss: 29668268639081.812 27 10.808553097554796\n",
      "Training loss: 29668259691233.28 28 10.643760508197264\n",
      "Training loss: 29668247164245.332 29 10.481841952124176\n",
      "Training loss: 29668236426827.094 30 10.322630844816873\n",
      "Training loss: 29668223899839.15 31 10.166300330029609\n",
      "Training loss: 29668202425002.668 32 10.012712004133089\n",
      "Training loss: 29668185424090.453 33 9.861790762474977\n",
      "Training loss: 29668163949253.973 34 9.713595141336183\n",
      "Training loss: 29668137105708.375 35 9.568031518433997\n",
      "Training loss: 29668105788238.508 36 9.4250628816221\n",
      "Training loss: 29668074470768.64 37 9.284635985393622\n",
      "Training loss: 29668036889804.8 38 9.146718254861609\n",
      "Training loss: 29667991255777.28 39 9.011293071999793\n",
      "Training loss: 29667942042610.348 40 8.878270272927885\n",
      "Training loss: 29667885671164.586 41 8.74765464887161\n",
      "Training loss: 29667819457085.44 42 8.619381828654246\n",
      "Training loss: 29667748769082.027 43 8.493411266496377\n",
      "Training loss: 29667668238445.227 44 8.36968460472434\n",
      "Training loss: 29667576075605.332 45 8.248170389617393\n",
      "Training loss: 29667475859701.76 46 8.128828450094028\n",
      "Training loss: 29667360432455.68 47 8.011609280528695\n",
      "Training loss: 29667231583436.8 48 7.896473554168\n",
      "Training loss: 29667092891784.535 49 7.783382696454207\n",
      "Training loss: 29666934514865.492 50 7.672288727219691\n",
      "Training loss: 29666755557894.83 51 7.563172071545331\n",
      "Training loss: 29666559600011.945 52 7.45596483830265\n",
      "Training loss: 29666345746432.0 53 7.350641867909275\n",
      "Training loss: 29666103259736.746 54 7.2471651585603265\n",
      "Training loss: 29665839298205.01 55 7.145499170301556\n",
      "Training loss: 29665550282697.387 56 7.045608461658175\n",
      "Training loss: 29665230844504.746 57 6.947457534538261\n",
      "Training loss: 29664879194057.387 58 6.851012022604905\n",
      "Training loss: 29664492647000.746 59 6.756240144988122\n",
      "Training loss: 29664070308549.973 60 6.6631111829842204\n",
      "Training loss: 29663608599565.652 61 6.571594386928085\n",
      "Training loss: 29663102151338.668 62 6.481660705170513\n",
      "Training loss: 29662550963869.01 63 6.39328242144674\n",
      "Training loss: 29661946089308.16 64 6.306429359947133\n",
      "Training loss: 29661294685934.934 65 6.221078279255487\n",
      "Training loss: 29660582437191.68 66 6.137201538067981\n",
      "Training loss: 29659810237863.254 67 6.054774518829663\n",
      "Training loss: 29658971824455.68 68 5.973792198683269\n",
      "Training loss: 29658063617829.547 69 5.89417457586659\n",
      "Training loss: 29657080249275.734 70 5.815956298382299\n",
      "Training loss: 29656018139654.83 71 5.739097145503019\n",
      "Training loss: 29654871025472.85 72 5.663575630271968\n",
      "Training loss: 29653634432805.547 73 5.589372639259734\n",
      "Training loss: 29652302992943.785 74 5.516468697651144\n",
      "Training loss: 29650865073684.48 75 5.444845267701866\n",
      "Training loss: 29649319780242.773 76 5.374484746499677\n",
      "Training loss: 29647662638694.4 77 5.305369678088131\n",
      "Training loss: 29645880227266.56 78 5.237483659611217\n",
      "Training loss: 29643968966819.84 79 5.170811325379495\n",
      "Training loss: 29641923488645.12 80 5.105336888889354\n",
      "Training loss: 29639729476184.746 81 5.041046186054428\n",
      "Training loss: 29637385139869.01 82 4.977924882460198\n",
      "Training loss: 29634879742279.68 83 4.915969441879978\n",
      "Training loss: 29632198966859.094 84 4.855137878310139\n",
      "Training loss: 29629343708392.105 85 4.795446946882651\n",
      "Training loss: 29626297860751.36 86 4.73687482329809\n",
      "Training loss: 29623054265658.027 87 4.679410436686799\n",
      "Training loss: 29619597711769.6 88 4.623042613545264\n",
      "Training loss: 29615921040807.254 89 4.567760985627049\n",
      "Training loss: 29612017989277.01 90 4.513555879210381\n",
      "Training loss: 29607867977127.254 91 4.460417073748373\n",
      "Training loss: 29603461161724.586 92 4.40833560080813\n",
      "Training loss: 29598789490005.332 93 4.357302944028965\n",
      "Training loss: 29593835066272.426 94 4.307310124846918\n",
      "Training loss: 29588588047892.48 95 4.258349333888845\n",
      "Training loss: 29583035013092.69 96 4.210412621133824\n",
      "Training loss: 29577158066176.0 97 4.163492381572767\n",
      "Training loss: 29570944680154.453 98 4.117581747503257\n",
      "Training loss: 29564384117609.812 99 4.072673466709913\n",
      "Training loss: 29557454903705.6 100 4.028760894750214\n",
      "Training loss: 29550140932314.453 101 3.985837483019369\n",
      "Training loss: 29542432360802.99 102 3.943897133934599\n",
      "Training loss: 29534305924765.01 103 3.9029334837617653\n",
      "Training loss: 29525748202427.734 104 3.8629409448422947\n",
      "Training loss: 29516737718954.668 105 3.82391344968893\n",
      "Training loss: 29507261052573.01 106 3.7858454411284193\n",
      "Training loss: 29497296728446.293 107 3.74873150026119\n",
      "Training loss: 29486825061307.734 108 3.712565857929606\n",
      "Training loss: 29475824576320.85 109 3.6773433850944337\n",
      "Training loss: 29464287220421.973 110 3.643058561477746\n",
      "Training loss: 29452178097001.812 111 3.609706125541263\n",
      "Training loss: 29439480205148.16 112 3.5772425171968196\n",
      "Training loss: 29426149700403.2 113 3.545658062720084\n",
      "Training loss: 29412176740133.547 114 3.5149789492379884\n",
      "Training loss: 29397538954717.867 115 3.485207034678857\n",
      "Training loss: 29382222922383.36 116 3.4563406356097377\n",
      "Training loss: 29366194641305.6 117 3.428375710961387\n",
      "Training loss: 29349444716243.625 118 3.401307912345457\n",
      "Training loss: 29331940040157.867 119 3.375132065936413\n",
      "Training loss: 29313663164743.68 120 3.3498491242917936\n",
      "Training loss: 29294588141240.32 121 3.3254340762684054\n",
      "Training loss: 29274684099570.348 122 3.301900212402805\n",
      "Training loss: 29253932696644.266 123 3.279234858499207\n",
      "Training loss: 29232307983701.332 124 3.257431563164432\n",
      "Training loss: 29209787143727.785 125 3.2364833085078692\n",
      "Training loss: 29186339306646.188 126 3.2163834349073293\n",
      "Training loss: 29161941208050.348 127 3.1971243704287717\n",
      "Training loss: 29136568241356.8 128 3.1786987342741377\n",
      "Training loss: 29110195352589.652 129 3.1610985694830833\n",
      "Training loss: 29082799724735.15 130 3.14431583364812\n",
      "Training loss: 29054353172070.4 131 3.128342021151498\n",
      "Training loss: 29024827061480.105 132 3.1131684196867226\n",
      "Training loss: 28994201707697.492 133 3.0987860044870903\n",
      "Training loss: 28962453846316.375 134 3.0851853526418336\n",
      "Training loss: 28929556633791.15 135 3.072356863258012\n",
      "Training loss: 28895486805715.625 136 3.0602905557603672\n",
      "Training loss: 28860221545076.055 137 3.04897616330013\n",
      "Training loss: 28823740271820.8 138 3.038403168340318\n",
      "Training loss: 28786013458049.707 139 3.0285606523178044\n",
      "Training loss: 28747028576774.83 140 3.0194374435172566\n",
      "Training loss: 28706763258374.83 141 3.0110222034434817\n",
      "Training loss: 28665196475405.652 142 3.003303146269552\n",
      "Training loss: 28622308542600.535 143 2.9962683507002317\n",
      "Training loss: 28578086932971.52 144 2.989905545157923\n",
      "Training loss: 28532511513859.414 145 2.984202312019682\n",
      "Training loss: 28485571547845.973 146 2.9791459028853726\n",
      "Training loss: 28437248244449.28 147 2.97472347344651\n",
      "Training loss: 28387534892782.934 148 2.9709218606562864\n",
      "Training loss: 28336420308036.266 149 2.9677278333163155\n",
      "Training loss: 28283894200183.465 150 2.9651277849978745\n",
      "Training loss: 28229951200515.414 151 2.963108128077686\n",
      "Training loss: 28174592203816.96 152 2.96165511290183\n",
      "Training loss: 28117810499201.707 153 2.960754793831037\n",
      "Training loss: 28059603402315.094 154 2.9603930726873635\n",
      "Training loss: 27999979861005.652 155 2.960555807321529\n",
      "Training loss: 27938942559627.945 156 2.961228891235812\n",
      "Training loss: 27876497314283.52 157 2.9623978965169684\n",
      "Training loss: 27812664257631.574 158 2.964048412578882\n",
      "Training loss: 27747445626634.24 159 2.966166164118415\n",
      "Training loss: 27680864238305.28 160 2.968736565540214\n",
      "Training loss: 27612937540949.332 161 2.971745079893406\n",
      "Training loss: 27543693272896.85 162 2.9751773556932406\n",
      "Training loss: 27473159172478.293 163 2.9790187297320982\n",
      "Training loss: 27401358951492.266 164 2.9832546579121373\n",
      "Training loss: 27328334217434.453 165 2.9878709506407537\n",
      "Training loss: 27254121209091.414 166 2.9928528375109313\n",
      "Training loss: 27178760639173.973 167 2.9981857716789193\n",
      "Training loss: 27102296799532.375 168 3.003855245987904\n",
      "Training loss: 27024787851182.08 169 3.0098469181011347\n",
      "Training loss: 26946278980758.188 170 3.0161463835312943\n",
      "Training loss: 26866836402339.84 171 3.0227391618153976\n",
      "Training loss: 26786518724334.934 172 3.0296100209439762\n",
      "Training loss: 26705391266037.76 173 3.0367440194177004\n",
      "Training loss: 26623530531553.28 174 3.0441274203080755\n",
      "Training loss: 26541008998454.613 175 3.051745893434504\n",
      "Training loss: 26457901381277.01 176 3.0595839411874697\n",
      "Training loss: 26374298948075.52 177 3.0676284081637024\n",
      "Training loss: 26290284466449.066 178 3.0758659955467365\n",
      "Training loss: 26205950546629.973 179 3.08428163048062\n",
      "Training loss: 26121390693635.414 180 3.0928624582649498\n",
      "Training loss: 26036703333799.254 181 3.101595581816718\n",
      "Training loss: 25951993156949.332 182 3.1104676830360054\n",
      "Training loss: 25867363958128.64 183 3.1194665129659347\n",
      "Training loss: 25782922664127.15 184 3.128578490388097\n",
      "Training loss: 25698777096519.68 185 3.1377892310210074\n",
      "Training loss: 25615042235159.895 186 3.147086272318465\n",
      "Training loss: 25531832165116.586 187 3.1564570074743963\n",
      "Training loss: 25449262761028.266 188 3.1658878730445448\n",
      "Training loss: 25367441844469.76 189 3.17536669314482\n",
      "Training loss: 25286489316611.414 190 3.1848829010570667\n",
      "Training loss: 25206514788597.76 191 3.194424098515676\n",
      "Training loss: 25127634582459.734 192 3.203976622592768\n",
      "Training loss: 25049960546304.0 193 3.2135286524592046\n",
      "Training loss: 24973605870414.508 194 3.223067787631002\n",
      "Training loss: 24898679718543.36 195 3.2325799294168402\n",
      "Training loss: 24825287227910.83 196 3.2420530321515315\n",
      "Training loss: 24753530851382.613 197 3.2514741373806513\n",
      "Training loss: 24683503646583.465 198 3.26082992375729\n",
      "Training loss: 24615304487239.68 199 3.2701086897389926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "model = RevenuePredictor(X_train.shape[-1])\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 18215725369917.44 0 3.068542724929428\n",
      "Training loss: 18172040183808.0 1 3.06846294670026\n",
      "Training loss: 18128034217328.64 2 3.068413787114408\n",
      "Training loss: 18083717313112.746 3 3.066577605308065\n",
      "Training loss: 18039092602907.31 4 3.065083168326568\n",
      "Training loss: 17994173955877.547 5 3.062306394267033\n",
      "Training loss: 17948956003314.348 6 3.0584116419270133\n",
      "Training loss: 17903449930028.375 7 3.0548575567048815\n",
      "Training loss: 17857660657336.32 8 3.0518029343383115\n",
      "Training loss: 17811592211770.027 9 3.0483435149910143\n",
      "Training loss: 17765254435962.88 10 3.0444855518996867\n",
      "Training loss: 17718643303383.04 11 3.0404458047800658\n",
      "Training loss: 17671769551448.746 12 3.0364399175651196\n",
      "Training loss: 17624640785831.254 13 3.0328092364042583\n",
      "Training loss: 17577261480454.826 14 3.0284761015754325\n",
      "Training loss: 17529643267522.56 15 3.0229887371179243\n",
      "Training loss: 17481792857920.854 16 3.0172808366730366\n",
      "Training loss: 17433709356864.854 17 3.0126988096285614\n",
      "Training loss: 17385401712203.094 18 3.0090545333964664\n",
      "Training loss: 17336879319176.533 19 3.004716166267698\n",
      "Training loss: 17288140835607.893 20 3.0002631972622766\n",
      "Training loss: 17239197446307.84 21 2.99594275387303\n",
      "Training loss: 17190059888694.613 22 2.99133380163674\n",
      "Training loss: 17140732189300.053 23 2.9865413356925323\n",
      "Training loss: 17091226875112.107 24 2.9816159097667465\n",
      "Training loss: 17041541709168.64 25 2.976488727469858\n",
      "Training loss: 16991699061090.986 26 2.9711714039871273\n",
      "Training loss: 16941711457867.094 27 2.965485376574604\n",
      "Training loss: 16891590979092.48 28 2.95939641880875\n",
      "Training loss: 16841354178286.934 29 2.9530659127186705\n",
      "Training loss: 16791007318944.426 30 2.9465368118889184\n",
      "Training loss: 16740560243698.346 31 2.9400685133386517\n",
      "Training loss: 16690025032144.213 32 2.933681021980558\n",
      "Training loss: 16639415553447.254 33 2.9277773262664293\n",
      "Training loss: 16588742545025.707 34 2.9223190670760846\n",
      "Training loss: 16538015849512.96 35 2.91705216805031\n",
      "Training loss: 16487245756934.826 36 2.9116434172101493\n",
      "Training loss: 16436430477721.6 37 2.9060250167978237\n",
      "Training loss: 16385578512329.387 38 2.900127002052329\n",
      "Training loss: 16334699255999.146 39 2.894163636556752\n",
      "Training loss: 16283808814858.24 40 2.888537678553709\n",
      "Training loss: 16232913899793.066 41 2.883276273644399\n",
      "Training loss: 16182023458652.16 42 2.8782653012726787\n",
      "Training loss: 16131168361512.96 43 2.873549369508085\n",
      "Training loss: 16080375451921.066 44 2.8688557618708677\n",
      "Training loss: 16029664415143.254 45 2.8639077565128956\n",
      "Training loss: 15979051804699.307 46 2.8589146243789343\n",
      "Training loss: 15928552384539.307 47 2.854277834459793\n",
      "Training loss: 15878180918613.334 48 2.8501607556311677\n",
      "Training loss: 15827944565200.213 49 2.8465503647581922\n",
      "Training loss: 15777850035186.346 50 2.8433223826200384\n",
      "Training loss: 15727900460318.72 51 2.840183210384747\n",
      "Training loss: 15678105683230.72 52 2.83679777812723\n",
      "Training loss: 15628467046099.627 53 2.8333615331987945\n",
      "Training loss: 15579011839863.467 54 2.829812642340488\n",
      "Training loss: 15529760197181.44 55 2.826155211290773\n",
      "Training loss: 15480748356840.107 56 2.8221970989289455\n",
      "Training loss: 15431975871447.04 57 2.818123054686746\n",
      "Training loss: 15383430214014.293 58 2.814172888813268\n",
      "Training loss: 15335115411073.707 59 2.810137568216644\n",
      "Training loss: 15287045331790.506 60 2.805772533797776\n",
      "Training loss: 15239233845329.92 61 2.8014087544650397\n",
      "Training loss: 15191685425616.213 62 2.797462032766132\n",
      "Training loss: 15144407678320.64 63 2.7932177988033753\n",
      "Training loss: 15097416709570.56 64 2.788685724069789\n",
      "Training loss: 15050732652025.174 65 2.7844054809143906\n",
      "Training loss: 15004364900925.44 66 2.780343151334309\n",
      "Training loss: 14958325088474.453 67 2.776317392881494\n",
      "Training loss: 14912604714216.107 68 2.772148535423593\n",
      "Training loss: 14867183421794.986 69 2.7681108191308614\n",
      "Training loss: 14822069711667.2 70 2.764271936925966\n",
      "Training loss: 14777288861504.854 71 2.760561878956217\n",
      "Training loss: 14732859661789.867 72 2.7568400184846173\n",
      "Training loss: 14688789047104.854 73 2.7533398637954325\n",
      "Training loss: 14645075451576.32 74 2.7503730508315742\n",
      "Training loss: 14601722454343.68 75 2.7476825186430087\n",
      "Training loss: 14558728489533.44 76 2.745274836032528\n",
      "Training loss: 14516096241500.16 77 2.742895628764679\n",
      "Training loss: 14473817880876.373 78 2.739998683089121\n",
      "Training loss: 14431871709129.387 79 2.736760630189407\n",
      "Training loss: 14390236475118.934 80 2.733638046896536\n",
      "Training loss: 14348930969326.934 81 2.731027014124715\n",
      "Training loss: 14307962350032.213 82 2.7290483659889384\n",
      "Training loss: 14267335314855.254 83 2.727554684150813\n",
      "Training loss: 14227057245771.094 84 2.7263096901698933\n",
      "Training loss: 14187148275438.934 85 2.725005093644047\n",
      "Training loss: 14147646655911.254 86 2.723429468428673\n",
      "Training loss: 14108483712450.56 87 2.721651495932341\n",
      "Training loss: 14069613811029.334 88 2.7197088475241777\n",
      "Training loss: 14031053057774.934 89 2.7177849239929364\n",
      "Training loss: 13992832099068.586 90 2.7161014415525138\n",
      "Training loss: 13954958764277.76 91 2.7146375336137325\n",
      "Training loss: 13917425447731.2 92 2.713383110776726\n",
      "Training loss: 13880227675504.64 93 2.712434930311865\n",
      "Training loss: 13843370592610.986 94 2.71193148975794\n",
      "Training loss: 13806846817075.2 95 2.711805412107971\n",
      "Training loss: 13770646058871.467 96 2.7119020994476943\n",
      "Training loss: 13734756238404.268 97 2.7120203286458064\n",
      "Training loss: 13699148946254.506 98 2.712005479390952\n",
      "Training loss: 13663818590016.854 99 2.7118544280307835\n",
      "Training loss: 13628756892931.414 100 2.711841781241533\n",
      "Training loss: 13593950209529.174 101 2.7122319443450684\n",
      "Training loss: 13559401224164.693 102 2.712901999322724\n",
      "Training loss: 13525108594660.693 103 2.7134187040226667\n",
      "Training loss: 13491061359902.72 104 2.713519648125227\n",
      "Training loss: 13457252361611.947 105 2.71314545358818\n",
      "Training loss: 13423645808394.24 106 2.7124400338447523\n",
      "Training loss: 13390230068046.506 107 2.7114839092593765\n",
      "Training loss: 13357025944316.586 108 2.710464063644774\n",
      "Training loss: 13324026502621.867 109 2.7095033975368312\n",
      "Training loss: 13291223913594.88 110 2.708645115473103\n",
      "Training loss: 13258623545944.746 111 2.7078865577128717\n",
      "Training loss: 13226204595923.627 112 2.7071915228782046\n",
      "Training loss: 13193967063531.52 113 2.706529358889723\n",
      "Training loss: 13161901777223.68 114 2.705835445963387\n",
      "Training loss: 13129991512391.68 115 2.705109507021443\n",
      "Training loss: 13098205175261.867 116 2.704394761025526\n",
      "Training loss: 13066525541225.812 117 2.7036847419807546\n",
      "Training loss: 13034924200864.426 118 2.7029687979806374\n",
      "Training loss: 13003370507796.48 119 2.702258185513516\n",
      "Training loss: 12971847461109.76 120 2.7016276135532173\n",
      "Training loss: 12940366021918.72 121 2.7011052257891244\n",
      "Training loss: 12908925295438.506 122 2.700678974514029\n",
      "Training loss: 12877545414328.32 123 2.7003106263557983\n",
      "Training loss: 12846229286638.934 124 2.699972539547243\n",
      "Training loss: 12815071983261.014 125 2.6996886013733374\n",
      "Training loss: 12784058292852.053 126 2.6995622182023804\n",
      "Training loss: 12753158687511.893 127 2.699505174896326\n",
      "Training loss: 12722336033669.12 128 2.6994072030833585\n",
      "Training loss: 12691585633703.254 129 2.6991640774653396\n",
      "Training loss: 12660913527412.053 130 2.6988290271877693\n",
      "Training loss: 12630321280669.014 131 2.6984664761698687\n",
      "Training loss: 12599808669777.92 132 2.698057647253327\n",
      "Training loss: 12569375471042.56 133 2.697618114622251\n",
      "Training loss: 12538999091145.387 134 2.697149910039334\n",
      "Training loss: 12508689596416.0 135 2.6966926426170144\n",
      "Training loss: 12478436025739.947 136 2.6962690251398853\n",
      "Training loss: 12448235918458.88 137 2.6959185021640777\n",
      "Training loss: 12418096209155.414 138 2.695681050359825\n",
      "Training loss: 12388028530032.64 139 2.6955339101419633\n",
      "Training loss: 12358018117140.48 140 2.6954194600697146\n",
      "Training loss: 12328071681365.334 141 2.6952951630868625\n",
      "Training loss: 12298186985745.066 142 2.6951074725054642\n",
      "Training loss: 12268326449315.84 143 2.6948422092182445\n",
      "Training loss: 12238498348837.547 144 2.6945381818744596\n",
      "Training loss: 12208694631246.506 145 2.6942282055820495\n",
      "Training loss: 12178908809352.533 146 2.6939813015023644\n",
      "Training loss: 12149145133383.68 147 2.6938205378534565\n",
      "Training loss: 12119376759794.346 148 2.6937141185188604\n",
      "Training loss: 12089605478154.24 149 2.69362490011434\n",
      "Training loss: 12059838894134.613 150 2.693586411779613\n",
      "Training loss: 12030076784039.254 151 2.693610331133007\n",
      "Training loss: 12000353149692.586 152 2.693721190589026\n",
      "Training loss: 11970664188258.986 153 2.6939137704513385\n",
      "Training loss: 11940960910267.732 154 2.694151606831525\n",
      "Training loss: 11911288502353.92 155 2.6944386066303965\n",
      "Training loss: 11881651214745.6 156 2.694782986977785\n",
      "Training loss: 11852039652201.812 157 2.695180116277912\n",
      "Training loss: 11822431668797.44 158 2.695651539853801\n",
      "Training loss: 11792821672127.146 159 2.6961809790337856\n",
      "Training loss: 11763203175000.746 160 2.6967538975196947\n",
      "Training loss: 11733611297723.732 161 2.6973977526901893\n",
      "Training loss: 11704040224194.56 162 2.698111435278995\n",
      "Training loss: 11674491967679.146 163 2.6988760810593697\n",
      "Training loss: 11644942592682.666 164 2.699658666629363\n",
      "Training loss: 11615409547509.76 165 2.7004608814617623\n",
      "Training loss: 11585889924109.654 166 2.701271831039574\n",
      "Training loss: 11556372985064.107 167 2.702090986307378\n",
      "Training loss: 11526851348398.08 168 2.7029103785608064\n",
      "Training loss: 11497312039731.2 169 2.703756186069971\n",
      "Training loss: 11467759756683.947 170 2.7046390612784394\n",
      "Training loss: 11438219776928.426 171 2.7055906524677757\n",
      "Training loss: 11408690310894.934 172 2.706648044318724\n",
      "Training loss: 11379187241014.613 173 2.707786119555706\n",
      "Training loss: 11349737858225.494 174 2.708969085492411\n",
      "Training loss: 11320343728401.066 175 2.7101382039942186\n",
      "Training loss: 11290991206072.32 176 2.7112701343200465\n",
      "Training loss: 11261662619238.4 177 2.712381519534052\n",
      "Training loss: 11232372508153.174 178 2.713479401591983\n",
      "Training loss: 11203155098337.28 179 2.7146169945527276\n",
      "Training loss: 11174006363258.88 180 2.7158090488722695\n",
      "Training loss: 11144913775930.027 181 2.7170237813315326\n",
      "Training loss: 11115895679440.213 182 2.718230537117472\n",
      "Training loss: 11086949836827.307 183 2.719438362470658\n",
      "Training loss: 11058092130522.453 184 2.7206607488318797\n",
      "Training loss: 11029328152930.986 185 2.721897674616396\n",
      "Training loss: 11000650745774.08 186 2.723149790007205\n",
      "Training loss: 10972087199989.76 187 2.7243571034093668\n",
      "Training loss: 10943556761245.014 188 2.725513979601787\n",
      "Training loss: 10915107076833.28 189 2.726668457689417\n",
      "Training loss: 10886731883260.586 190 2.727833478547621\n",
      "Training loss: 10858430509438.293 191 2.7290244448893795\n",
      "Training loss: 10830164703313.92 192 2.73021275462572\n",
      "Training loss: 10801957058205.014 193 2.73141038370168\n",
      "Training loss: 10773830167429.12 194 2.7326448277427082\n",
      "Training loss: 10745803716253.014 195 2.7339398205386174\n",
      "Training loss: 10717888665791.146 196 2.7352686125779018\n",
      "Training loss: 10690064435991.893 197 2.736629550315252\n",
      "Training loss: 10662367265641.812 198 2.7379916267933493\n",
      "Training loss: 10634798720614.4 199 2.739371510600275\n",
      "Training loss: 10607344260655.787 200 2.7407451117339767\n",
      "Training loss: 10579985318980.268 201 2.742097476700831\n",
      "Training loss: 10552724803638.613 202 2.7434495900804374\n",
      "Training loss: 10525590676657.494 203 2.7448209376391026\n",
      "Training loss: 10498596807202.133 204 2.7462315658281704\n",
      "Training loss: 10471734918512.64 205 2.747673833214794\n",
      "Training loss: 10444987785980.586 206 2.7491355876544583\n",
      "Training loss: 10418349817200.64 207 2.750580307040583\n",
      "Training loss: 10391824591312.213 208 2.7520114465147123\n",
      "Training loss: 10365462887355.732 209 2.753473220581876\n",
      "Training loss: 10339245020064.426 210 2.754958960023259\n",
      "Training loss: 10313159133538.986 211 2.7564434726314233\n",
      "Training loss: 10287198516893.014 212 2.7579248640531113\n",
      "Training loss: 10261381736912.213 213 2.759386485682285\n",
      "Training loss: 10235695148127.574 214 2.7608414448285883\n",
      "Training loss: 10210130250082.986 215 2.7622967261425466\n",
      "Training loss: 10184695095842.133 216 2.763740973077569\n",
      "Training loss: 10159368434264.746 217 2.76516739307396\n",
      "Training loss: 10134143107072.0 218 2.7665795042305468\n",
      "Training loss: 10109041931277.654 219 2.7680024534540637\n",
      "Training loss: 10084054840552.107 220 2.769445724621323\n",
      "Training loss: 10059192124921.174 221 2.7709044016375706\n",
      "Training loss: 10034462955929.6 222 2.772355904011089\n",
      "Training loss: 10009887689932.8 223 2.773780612949738\n",
      "Training loss: 9985444181005.654 224 2.775161389045753\n",
      "Training loss: 9961112967577.6 225 2.7764771416507523\n",
      "Training loss: 9936899418357.76 226 2.777744637124916\n",
      "Training loss: 9912782058509.654 227 2.778995286581393\n",
      "Training loss: 9888780796996.268 228 2.7802444257368593\n",
      "Training loss: 9864893620551.68 229 2.781528279879358\n",
      "Training loss: 9841109120669.014 230 2.78284713598945\n",
      "Training loss: 9817410072739.84 231 2.7841938945174154\n",
      "Training loss: 9793811240714.24 232 2.7855424593623224\n",
      "Training loss: 9770308598060.373 233 2.7869060784997397\n",
      "Training loss: 9746903934347.947 234 2.788279504302252\n",
      "Training loss: 9723604631552.0 235 2.7896681394089886\n",
      "Training loss: 9700401294431.574 236 2.7910828283717937\n",
      "Training loss: 9677294146682.88 237 2.792536790685103\n",
      "Training loss: 9654283188305.92 238 2.7939918184399395\n",
      "Training loss: 9631358129274.88 239 2.79543108740424\n",
      "Training loss: 9608529707008.0 240 2.7968334294220134\n",
      "Training loss: 9585802619125.76 241 2.798216982360973\n",
      "Training loss: 9563172168007.68 242 2.799608211265164\n",
      "Training loss: 9540638801046.188 243 2.8010059737238047\n",
      "Training loss: 9518207215861.76 244 2.8024317596813777\n",
      "Training loss: 9495886583999.146 245 2.8038826556112197\n",
      "Training loss: 9473670865660.586 246 2.8053193228526836\n",
      "Training loss: 9451566771732.48 247 2.8067440441013223\n",
      "Training loss: 9429562670011.732 248 2.808137054769618\n",
      "Training loss: 9407674890321.92 249 2.8095072512278185\n",
      "Training loss: 9385911262030.506 250 2.8108602349437004\n",
      "Training loss: 9364256573794.986 251 2.8121861318898675\n",
      "Training loss: 9342707693868.373 252 2.8134845538862283\n",
      "Training loss: 9321255450705.92 253 2.8147433358789926\n",
      "Training loss: 9299904318231.893 254 2.81595530482513\n",
      "Training loss: 9278635953356.8 255 2.817138436671553\n",
      "Training loss: 9257461764587.52 256 2.818321549197915\n",
      "Training loss: 9236384883671.04 257 2.819511995592934\n",
      "Training loss: 9215394796885.334 258 2.8207039570815504\n",
      "Training loss: 9194492846407.68 259 2.821898589499999\n",
      "Training loss: 9173693125099.52 260 2.823089149967572\n",
      "Training loss: 9152988474682.027 261 2.824289046412997\n",
      "Training loss: 9132390527358.293 262 2.8254836802749077\n",
      "Training loss: 9111896375077.547 263 2.826657041224891\n",
      "Training loss: 9091501991307.947 264 2.8278208327713936\n",
      "Training loss: 9071209613011.627 265 2.8289730944212703\n",
      "Training loss: 9051018569099.947 266 2.83010160323267\n",
      "Training loss: 9030942952434.346 267 2.8312167491671194\n",
      "Training loss: 9010978960179.2 268 2.8323237764003677\n",
      "Training loss: 8991123684283.732 269 2.833407147780507\n",
      "Training loss: 8971374440393.387 270 2.834452736537058\n",
      "Training loss: 8951727649368.746 271 2.835451693471745\n",
      "Training loss: 8932197404071.254 272 2.8364084238248313\n",
      "Training loss: 8912765137715.2 273 2.83733210212355\n",
      "Training loss: 8893433087262.72 274 2.838232762902175\n",
      "Training loss: 8874203937068.373 275 2.839110423843046\n",
      "Training loss: 8855081042575.36 276 2.839977000501095\n",
      "Training loss: 8836069101404.16 277 2.8408584055088673\n",
      "Training loss: 8817152454819.84 278 2.841712954593954\n",
      "Training loss: 8798307614720.0 279 2.8425291299270543\n",
      "Training loss: 8779555161156.267 280 2.843349180528367\n",
      "Training loss: 8760896883698.347 281 2.844201721106517\n",
      "Training loss: 8742344190853.12 282 2.8450948063494415\n",
      "Training loss: 8723900885456.213 283 2.8460008057534028\n",
      "Training loss: 8705559138140.16 284 2.8468670238703995\n",
      "Training loss: 8687311566929.92 285 2.8476751562555673\n",
      "Training loss: 8669160856180.054 286 2.848425055327495\n",
      "Training loss: 8651098952826.88 287 2.84916063432925\n",
      "Training loss: 8633127422743.894 288 2.8498967642697766\n",
      "Training loss: 8615248055500.8 289 2.8506241948614086\n",
      "Training loss: 8597468680465.066 290 2.8513529282567367\n",
      "Training loss: 8579792205687.467 291 2.85209599711924\n",
      "Training loss: 8562219302256.64 292 2.8528507580216944\n",
      "Training loss: 8544743930374.827 293 2.8535827168976473\n",
      "Training loss: 8527369669181.44 294 2.854284990518776\n",
      "Training loss: 8510086452346.88 295 2.8549530270237713\n",
      "Training loss: 8492900767061.333 296 2.8555925272642275\n",
      "Training loss: 8475802770691.413 297 2.8562211995331346\n",
      "Training loss: 8458794923895.467 298 2.8568386974150486\n",
      "Training loss: 8441872976445.44 299 2.857432545930738\n",
      "Training loss: 8425042744442.88 300 2.857991664628763\n",
      "Training loss: 8408288792849.066 301 2.8585000403883654\n",
      "Training loss: 8391609555790.507 302 2.858972220251224\n",
      "Training loss: 8375004138482.347 303 2.859442411764352\n",
      "Training loss: 8358474330494.293 304 2.859914847066985\n",
      "Training loss: 8342023263573.333 305 2.8604007052566023\n",
      "Training loss: 8325650937719.467 306 2.8609034859052866\n",
      "Training loss: 8309357352932.693 307 2.8614097235344547\n",
      "Training loss: 8293142061820.587 308 2.8618919535195877\n",
      "Training loss: 8277001932636.16 309 2.862345400163511\n",
      "Training loss: 8260933386240.0 310 2.8627718119159837\n",
      "Training loss: 8244932396100.267 311 2.8631776920841485\n",
      "Training loss: 8228994264596.48 312 2.8635597530647754\n",
      "Training loss: 8213116978462.72 313 2.8639424297322837\n",
      "Training loss: 8197322012535.467 314 2.8643384509150476\n",
      "Training loss: 8181595721345.707 315 2.8647443290220007\n",
      "Training loss: 8165936539019.946 316 2.8651172609466893\n",
      "Training loss: 8150349163178.667 317 2.865453956181951\n",
      "Training loss: 8134830238378.667 318 2.865765524437301\n",
      "Training loss: 8119383791151.787 319 2.866089953163033\n",
      "Training loss: 8104008479320.747 320 2.866442735279032\n",
      "Training loss: 8088700052657.493 321 2.8668145109739562\n",
      "Training loss: 8073446207870.293 322 2.8671850247256856\n",
      "Training loss: 8058252537364.48 323 2.8675503350614058\n",
      "Training loss: 8043115014608.213 324 2.8679181742171513\n",
      "Training loss: 8028024468056.747 325 2.8682612323310357\n",
      "Training loss: 8012991187735.894 326 2.8685824379009994\n",
      "Training loss: 7998025239975.253 327 2.8689042702060505\n",
      "Training loss: 7983119242799.787 328 2.869228020559135\n",
      "Training loss: 7968275880564.054 329 2.8695376849536487\n",
      "Training loss: 7953489337166.507 330 2.869843250954892\n",
      "Training loss: 7938759612607.146 331 2.870149587407188\n",
      "Training loss: 7924082904050.347 332 2.8704587995597066\n",
      "Training loss: 7909468606737.066 333 2.870774288950532\n",
      "Training loss: 7894912917831.68 334 2.8710846048075718\n",
      "Training loss: 7880417403207.68 335 2.871388006601372\n",
      "Training loss: 7865985865700.693 336 2.8716911149033404\n",
      "Training loss: 7851624121412.267 337 2.8719856680065625\n",
      "Training loss: 7837317853784.747 338 2.8722690893297878\n",
      "Training loss: 7823061694109.014 339 2.872542403975087\n",
      "Training loss: 7808857208258.56 340 2.87281319881199\n",
      "Training loss: 7794709317550.08 341 2.8731022434992948\n",
      "Training loss: 7780616903502.507 342 2.8734154951344935\n",
      "Training loss: 7766589585053.014 343 2.8737322014671194\n",
      "Training loss: 7752634073088.0 344 2.8740372286377625\n",
      "Training loss: 7738739853885.44 345 2.8743194036127004\n",
      "Training loss: 7724903348305.92 346 2.87457259489736\n",
      "Training loss: 7711130148754.773 347 2.8748015283469486\n",
      "Training loss: 7697419360447.146 348 2.8749987813701674\n",
      "Training loss: 7683759127483.733 349 2.875168668488997\n",
      "Training loss: 7670146318117.547 350 2.875330047987618\n",
      "Training loss: 7656581827133.44 351 2.875501123024894\n",
      "Training loss: 7643062522784.427 352 2.8756924984984327\n",
      "Training loss: 7629597352919.04 353 2.8759135856250984\n",
      "Training loss: 7616189001891.84 354 2.8761552941550335\n",
      "Training loss: 7602833666867.2 355 2.87640316463635\n",
      "Training loss: 7589532466326.187 356 2.8766387392924697\n",
      "Training loss: 7576277794597.547 357 2.876839612560545\n",
      "Training loss: 7563066519934.293 358 2.8769963506133895\n",
      "Training loss: 7549895734285.653 359 2.877121296586743\n",
      "Training loss: 7536759845246.293 360 2.8772388026161786\n",
      "Training loss: 7523653707803.307 361 2.877372874376818\n",
      "Training loss: 7510583809146.88 362 2.8775228620145334\n",
      "Training loss: 7497552162542.934 363 2.877684704532887\n",
      "Training loss: 7484560781257.387 364 2.877860661837607\n",
      "Training loss: 7471620626404.693 365 2.8780387405354064\n",
      "Training loss: 7458720960566.613 366 2.8781942303271144\n",
      "Training loss: 7445865362882.56 367 2.878311887499002\n",
      "Training loss: 7433046675073.707 368 2.8783925464186857\n",
      "Training loss: 7420267134102.187 369 2.878453703972645\n",
      "Training loss: 7407523160828.587 370 2.878501736308796\n",
      "Training loss: 7394812518290.773 371 2.878527802379176\n",
      "Training loss: 7382129166690.986 372 2.8785212547900976\n",
      "Training loss: 7369476237776.213 373 2.878499465539782\n",
      "Training loss: 7356856415901.014 374 2.878479351672802\n",
      "Training loss: 7344266569318.4 375 2.878473335230559\n",
      "Training loss: 7331704684762.453 376 2.878474458684377\n",
      "Training loss: 7319177473119.573 377 2.87848183739612\n",
      "Training loss: 7306691645276.16 378 2.878491640490879\n",
      "Training loss: 7294250780371.627 379 2.8785076894333455\n",
      "Training loss: 7281853312532.48 380 2.8785255216557513\n",
      "Training loss: 7269498123277.653 381 2.8785573592486124\n",
      "Training loss: 7257171343441.92 382 2.8786230721088457\n",
      "Training loss: 7244854629935.787 383 2.8787102259687063\n",
      "Training loss: 7232547311670.613 384 2.878799855299162\n",
      "Training loss: 7220252296697.173 385 2.8788511653989857\n",
      "Training loss: 7207983677876.906 386 2.8788121680926597\n",
      "Training loss: 7195746823918.934 387 2.878699308034374\n",
      "Training loss: 7183545313962.667 388 2.878554943442713\n",
      "Training loss: 7171381832362.667 389 2.878427199962954\n",
      "Training loss: 7159253247371.946 390 2.8783414455792786\n",
      "Training loss: 7147157098332.16 391 2.8783165724269426\n",
      "Training loss: 7135094503724.373 392 2.8783363838234735\n",
      "Training loss: 7123060989624.32 393 2.878361915206183\n",
      "Training loss: 7111053871677.44 394 2.878378698812293\n",
      "Training loss: 7099076057934.507 395 2.878394618701063\n",
      "Training loss: 7087125982522.026 396 2.8784242653039356\n",
      "Training loss: 7075203645440.0 397 2.878480222264559\n",
      "Training loss: 7063309494080.854 398 2.878575885227602\n",
      "Training loss: 7051440396697.6 399 2.878688309172262\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 29668031521095.68 0 10.682109533116172\n",
      "Training loss: 29665692553489.066 1 10.222006808833106\n",
      "Training loss: 29663943249100.8 2 9.878126898481188\n",
      "Training loss: 29661785922819.414 3 9.585090572420803\n",
      "Training loss: 29658893083388.586 4 9.325051769834495\n",
      "Training loss: 29655482163527.68 5 9.096994946321598\n",
      "Training loss: 29651522740551.68 6 8.885396035811917\n",
      "Training loss: 29646726693737.812 7 8.682952515777632\n",
      "Training loss: 29640970542776.32 8 8.48642949462596\n",
      "Training loss: 29634018064465.92 9 8.293839412303566\n",
      "Training loss: 29625505976156.16 10 8.1000147531425\n",
      "Training loss: 29614926040050.348 11 7.906502690864819\n",
      "Training loss: 29601846075064.32 12 7.713492088680184\n",
      "Training loss: 29585671049270.613 13 7.519875086743211\n",
      "Training loss: 29565655606886.4 14 7.326184799415659\n",
      "Training loss: 29540914805691.734 15 7.1336038069036665\n",
      "Training loss: 29510470645841.92 16 6.942561219092635\n",
      "Training loss: 29472993477044.906 17 6.753206115929608\n",
      "Training loss: 29426860159576.746 18 6.565831915848025\n",
      "Training loss: 29370438605864.96 19 6.381201361739439\n",
      "Training loss: 29301677074240.85 20 6.199373577129909\n",
      "Training loss: 29218322943836.16 21 6.021770812703377\n",
      "Training loss: 29117849342225.066 22 5.848866143129438\n",
      "Training loss: 28997288715441.492 23 5.680897031063036\n",
      "Training loss: 28853153639519.574 24 5.517861629097117\n",
      "Training loss: 28681756706078.72 25 5.359988501825952\n",
      "Training loss: 28478295157418.668 26 5.207090265923931\n",
      "Training loss: 28237388204823.895 27 5.059344734216816\n",
      "Training loss: 27953685034871.465 28 4.916824305949748\n",
      "Training loss: 27620092240636.586 29 4.77950687361947\n",
      "Training loss: 27230034048232.105 30 4.647552350004591\n",
      "Training loss: 26775889127669.76 31 4.521160300935192\n",
      "Training loss: 26249768608290.133 32 4.400319473622442\n",
      "Training loss: 25644233696215.04 33 4.285263996161333\n",
      "Training loss: 24950819279339.52 34 4.176103431049974\n",
      "Training loss: 24162653409989.973 35 4.073103296603678\n",
      "Training loss: 23276166343994.027 36 3.9763802172907377\n",
      "Training loss: 22289915240775.68 37 3.8861871829958132\n",
      "Training loss: 21205459262941.867 38 3.802459929829622\n",
      "Training loss: 20029587143174.83 39 3.725566761713072\n",
      "Training loss: 18774920716615.68 40 3.6553348390365317\n",
      "Training loss: 17464680992385.707 41 3.591862355045884\n",
      "Training loss: 16132504275298.986 42 3.535241301508248\n",
      "Training loss: 14822421809506.986 43 3.485362602406517\n",
      "Training loss: 13590190323575.467 44 3.4421311683914135\n",
      "Training loss: 12498684783274.666 45 3.4054647416108237\n",
      "Training loss: 11611370488681.812 46 3.3751543568839564\n",
      "Training loss: 10979354418872.32 47 3.3509469248188126\n",
      "Training loss: 10621482308730.88 48 3.3324060209681567\n",
      "Training loss: 10507494324087.467 49 3.319186304475238\n",
      "Training loss: 10553810625058.133 50 3.3107003372495196\n",
      "Training loss: 10649565131352.746 51 3.306212908185476\n",
      "Training loss: 10709474109030.4 52 3.3048379776076424\n",
      "Training loss: 10706477250860.373 53 3.3056803560466177\n",
      "Training loss: 10660540562363.732 54 3.3079774809556954\n",
      "Training loss: 10604735291719.68 55 3.31094559445439\n",
      "Training loss: 10562600767761.066 56 3.313891022341095\n",
      "Training loss: 10540960172386.986 57 3.316281544164756\n",
      "Training loss: 10535202902944.426 58 3.3177782627448065\n",
      "Training loss: 10537062265869.654 59 3.318245029855889\n",
      "Training loss: 10539821782357.334 60 3.31772714894524\n",
      "Training loss: 10540155537107.627 61 3.316393733875437\n",
      "Training loss: 10537732236028.586 62 3.3144630484145856\n",
      "Training loss: 10533841711486.293 63 3.31217825982182\n",
      "Training loss: 10530087641634.133 64 3.309760725740454\n",
      "Training loss: 10527559650727.254 65 3.307390800702481\n",
      "Training loss: 10526547649058.133 66 3.3051821818023717\n",
      "Training loss: 10526734435396.268 67 3.3032030582263356\n",
      "Training loss: 10527521622370.986 68 3.301475876538737\n",
      "Training loss: 10528373010158.934 69 3.2999842398012236\n",
      "Training loss: 10528973187099.307 70 3.2986870385972935\n",
      "Training loss: 10529249451922.773 71 3.2975292436779937\n",
      "Training loss: 10529273834810.027 72 3.296451545239103\n",
      "Training loss: 10529168250197.334 73 3.295399686486204\n",
      "Training loss: 10529022623962.453 74 3.2943294634047398\n",
      "Training loss: 10528881919044.268 75 3.293211058452112\n",
      "Training loss: 10528753741114.027 76 3.2920282837655006\n",
      "Training loss: 10528631379285.334 77 3.2907755459100447\n",
      "Training loss: 10528533176647.68 78 3.289461210024659\n",
      "Training loss: 10528414841350.826 79 3.288098772911433\n",
      "Training loss: 10528322231118.506 80 3.286702423571125\n",
      "Training loss: 10528262504229.547 81 3.2852887837365\n",
      "Training loss: 10528238568734.72 82 3.283872021924998\n",
      "Training loss: 10528246845494.613 83 3.282462753916868\n",
      "Training loss: 10528277939268.268 84 3.2810673302983573\n",
      "Training loss: 10528321336333.654 85 3.2796885524680173\n",
      "Training loss: 10528366970361.174 86 3.278325808598509\n",
      "Training loss: 10528405669806.08 87 3.2769764473033924\n",
      "Training loss: 10528436316187.307 88 3.2756363604088627\n",
      "Training loss: 10528454659276.8 89 3.274301321898051\n",
      "Training loss: 10528463830821.547 90 3.272967428300671\n",
      "Training loss: 10528465620391.254 91 3.2716313048242034\n",
      "Training loss: 10528461146466.986 92 3.27029107161752\n",
      "Training loss: 10528454211884.373 93 3.268945633223966\n",
      "Training loss: 10528459356897.28 94 3.267594818858342\n",
      "Training loss: 10528438329453.227 95 3.266239393180476\n",
      "Training loss: 10528432289655.467 96 3.2648805142824506\n",
      "Training loss: 10528429828997.12 97 3.263519289419319\n",
      "Training loss: 10528429605300.906 98 3.262157184515479\n",
      "Training loss: 10528431842263.04 99 3.2607952134458085\n",
      "Training loss: 10528436539883.52 100 3.2594338505424316\n",
      "Training loss: 10528441237504.0 101 3.258073740294512\n",
      "Training loss: 10528447500997.973 102 3.2567148841364424\n",
      "Training loss: 10528454435580.586 103 3.25535758519059\n",
      "Training loss: 10528458462112.426 104 3.254000797223313\n",
      "Training loss: 10528462488644.268 105 3.2526451025180467\n",
      "Training loss: 10528466067783.68 106 3.25128978583205\n",
      "Training loss: 10528468304745.812 107 3.2499334046110127\n",
      "Training loss: 10528481279126.188 108 3.2485769925282484\n",
      "Training loss: 10528475463024.64 109 3.247220748733516\n",
      "Training loss: 10528476134113.28 110 3.24586389562237\n",
      "Training loss: 10528468528442.027 111 3.2445053344598693\n",
      "Training loss: 10528481502822.4 112 3.243181015059807\n",
      "Training loss: 10528520649659.732 113 3.2418582931995457\n",
      "Training loss: 10528504767228.586 114 3.2405372163393764\n",
      "Training loss: 10528499845911.893 115 3.2392146532039408\n",
      "Training loss: 10528511925507.414 116 3.2378933133806593\n",
      "Training loss: 10528503425051.307 117 3.2365736572315993\n",
      "Training loss: 10528504767228.586 118 3.2352863048889113\n",
      "Training loss: 10528502977658.88 119 3.2339956950502953\n",
      "Training loss: 10528504543532.373 120 3.2327026231725395\n",
      "Training loss: 10528506556798.293 121 3.231408411661952\n",
      "Training loss: 10528509241152.854 122 3.230113590190172\n",
      "Training loss: 10528512820292.268 123 3.2288187424330785\n",
      "Training loss: 10528514609861.973 124 3.2275241925927562\n",
      "Training loss: 10528515952039.254 125 3.2262300248157163\n",
      "Training loss: 10528519083786.24 126 3.2249364975863837\n",
      "Training loss: 10528520425963.52 127 3.223643615728865\n",
      "Training loss: 10528521320748.373 128 3.2223515675697025\n",
      "Training loss: 10528522439229.44 129 3.2210602859358546\n",
      "Training loss: 10528524452495.36 130 3.21976997625514\n",
      "Training loss: 10528525347280.213 131 3.2184806045302117\n",
      "Training loss: 10528526689457.494 132 3.217192349780217\n",
      "Training loss: 10528527584242.346 133 3.2159052538782835\n",
      "Training loss: 10528529373812.053 134 3.214619178434195\n",
      "Training loss: 10528533400343.893 135 3.213334405283764\n",
      "Training loss: 10528533624040.107 136 3.212050915407955\n",
      "Training loss: 10528534742521.174 137 3.2107688847786076\n",
      "Training loss: 10528535861002.24 138 3.20948820166771\n",
      "Training loss: 10528537874268.16 139 3.2082089305781234\n",
      "Training loss: 10528539887534.08 140 3.206931094918722\n",
      "Training loss: 10528540782318.934 141 3.205654775889634\n",
      "Training loss: 10528541900800.0 142 3.2043797721019183\n",
      "Training loss: 10528544137762.133 143 3.203106321572135\n",
      "Training loss: 10528545703635.627 144 3.201833716190869\n",
      "Training loss: 10528552190825.812 145 3.200562462221693\n",
      "Training loss: 10528547269509.12 146 3.199287133267476\n",
      "Training loss: 10528566954775.893 147 3.1980369765389356\n",
      "Training loss: 10528553756699.307 148 3.1967873765196027\n",
      "Training loss: 10528556441053.867 149 3.195538875846941\n",
      "Training loss: 10528558006927.36 150 3.1942915666947793\n",
      "Training loss: 10528559349104.64 151 3.1930455029244613\n",
      "Training loss: 10528561362370.56 152 3.1918008280935775\n",
      "Training loss: 10528560691281.92 153 3.190557279531571\n",
      "Training loss: 10528562480851.627 154 3.1893151895294207\n",
      "Training loss: 10528562928244.053 155 3.1880743980932085\n",
      "Training loss: 10528563375636.48 156 3.1868338014967974\n",
      "Training loss: 10528575231535.787 157 3.185594930333527\n",
      "Training loss: 10528570533915.307 158 3.1843577470167848\n",
      "Training loss: 10528568520649.387 159 3.1831222854480967\n",
      "Training loss: 10528568520649.387 160 3.1818888170882342\n",
      "Training loss: 10528569191738.027 161 3.1806575639394414\n",
      "Training loss: 10528570981307.732 162 3.1794282709214268\n",
      "Training loss: 10528571652396.373 163 3.178201176285588\n",
      "Training loss: 10528573441966.08 164 3.1769763166500535\n",
      "Training loss: 10528576126320.64 165 3.175753137767112\n",
      "Training loss: 10528576573713.066 166 3.174531955922074\n",
      "Training loss: 10528577692194.133 167 3.1733127404671935\n",
      "Training loss: 10528579258067.627 168 3.1720950985663565\n",
      "Training loss: 10528580152852.48 169 3.1708793733415988\n",
      "Training loss: 10528580600244.906 170 3.1696652684766744\n",
      "Training loss: 10528581942422.188 171 3.1684528043842666\n",
      "Training loss: 10528581942422.188 172 3.1672421961449393\n",
      "Training loss: 10528582613510.826 173 3.1660334516904935\n",
      "Training loss: 10528582613510.826 174 3.164826400688703\n",
      "Training loss: 10528584626776.746 175 3.1636210673090965\n",
      "Training loss: 10528585074169.174 176 3.1624127105196007\n",
      "Training loss: 10528627576449.707 177 3.1612067113729663\n",
      "Training loss: 10528604312043.52 178 3.1600034846392195\n",
      "Training loss: 10528595140498.773 179 3.1588012091569015\n",
      "Training loss: 10528591337663.146 180 3.157601412845491\n",
      "Training loss: 10528590666574.506 181 3.1564045678149935\n",
      "Training loss: 10528591337663.146 182 3.1552107357431036\n",
      "Training loss: 10528594022017.707 183 3.1540197731006625\n",
      "Training loss: 10528597153764.693 184 3.152831290008162\n",
      "Training loss: 10528584403080.533 185 3.1516358607007944\n",
      "Training loss: 10528606549005.654 186 3.15047315833913\n",
      "Training loss: 10528610799233.707 187 3.1493133134558398\n",
      "Training loss: 10528614825765.547 188 3.1481553880484006\n",
      "Training loss: 10528615720550.4 189 3.146999015958907\n",
      "Training loss: 10528614825765.547 190 3.145843698413638\n",
      "Training loss: 10528612812499.627 191 3.1446893422347215\n",
      "Training loss: 10528610575537.494 192 3.1435360313275416\n",
      "Training loss: 10528607891182.934 193 3.142383754787809\n",
      "Training loss: 10528605877917.014 194 3.141232824186228\n",
      "Training loss: 10528605430524.586 195 3.1400834808258433\n",
      "Training loss: 10528604983132.16 196 3.138936070311417\n",
      "Training loss: 10528605430524.586 197 3.1377908353645805\n",
      "Training loss: 10528607220094.293 198 3.136647832586802\n",
      "Training loss: 10528608785967.787 199 3.135507071929296\n"
     ]
    }
   ],
   "source": [
    " model = RevenuePredictor(X_train.shape[-1])\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10528609009664.0 0 3.134368727176203\n",
      "Training loss: 10528611022929.92 1 3.1332326704857514\n",
      "Training loss: 10528611470322.346 2 3.132098764570639\n",
      "Training loss: 10528612588803.414 3 3.1309667965219425\n",
      "Training loss: 10528612812499.627 4 3.1298369018768613\n",
      "Training loss: 10528613259892.053 5 3.1287089535259938\n",
      "Training loss: 10528613259892.053 6 3.1275830245621137\n",
      "Training loss: 10528613707284.48 7 3.126458931758273\n",
      "Training loss: 10528613930980.693 8 3.1253368314462815\n",
      "Training loss: 10528615049461.76 9 3.1242167973055848\n",
      "Training loss: 10528615049461.76 10 3.1230987552369935\n",
      "Training loss: 10528614378373.12 11 3.121982822358223\n",
      "Training loss: 10528615049461.76 12 3.120868948544418\n",
      "Training loss: 10528615273157.973 13 3.1197571870792213\n",
      "Training loss: 10528616615335.254 14 3.1186473925255283\n",
      "Training loss: 10528616615335.254 15 3.1175397432645706\n",
      "Training loss: 10528617510120.107 16 3.1164340861757633\n",
      "Training loss: 10528617957512.533 17 3.115330683443122\n",
      "Training loss: 10528618181208.746 18 3.1142292111663115\n",
      "Training loss: 10528618181208.746 19 3.1131297308850434\n",
      "Training loss: 10528619299689.812 20 3.1120323662096148\n",
      "Training loss: 10528619075993.6 21 3.1109370217425845\n",
      "Training loss: 10528619523386.027 22 3.1098437466937012\n",
      "Training loss: 10528620194474.666 23 3.108752561380333\n",
      "Training loss: 10528619523386.027 24 3.107663426144258\n",
      "Training loss: 10528620418170.88 25 3.106576422515433\n",
      "Training loss: 10528621089259.52 26 3.1054914911142157\n",
      "Training loss: 10528620641867.094 27 3.1044087090799812\n",
      "Training loss: 10528621312955.732 28 3.10332779456119\n",
      "Training loss: 10528621536651.947 29 3.1022490922265846\n",
      "Training loss: 10528621760348.16 30 3.1011720928293736\n",
      "Training loss: 10528622431436.8 31 3.1000973532899927\n",
      "Training loss: 10528621984044.373 32 3.0990246456822583\n",
      "Training loss: 10528622655133.014 33 3.0979539728512315\n",
      "Training loss: 10528623102525.44 34 3.0968853666495724\n",
      "Training loss: 10528622878829.227 35 3.095818794369855\n",
      "Training loss: 10528623102525.44 36 3.094754202385981\n",
      "Training loss: 10528623773614.08 37 3.0936915841803034\n",
      "Training loss: 10528623326221.654 38 3.092631050792731\n",
      "Training loss: 10528623773614.08 39 3.091572463397853\n",
      "Training loss: 10528625115791.36 40 3.0905159902429133\n",
      "Training loss: 10528624444702.72 41 3.0894613467003333\n",
      "Training loss: 10528624668398.934 42 3.088408940956044\n",
      "Training loss: 10528624892095.146 43 3.0873584233769984\n",
      "Training loss: 10528625786880.0 44 3.086309964220925\n",
      "Training loss: 10528625339487.574 45 3.0852633711953166\n",
      "Training loss: 10528626234272.426 46 3.084219011867315\n",
      "Training loss: 10528625786880.0 47 3.0831764260598895\n",
      "Training loss: 10528626457968.64 48 3.0821360717274304\n",
      "Training loss: 10528625563183.787 49 3.0810974430746\n",
      "Training loss: 10528626010576.213 50 3.0800610222333904\n",
      "Training loss: 10528626010576.213 51 3.0790264565980094\n",
      "Training loss: 10528626010576.213 52 3.07799384623089\n",
      "Training loss: 10528626457968.64 53 3.0769633486397314\n",
      "Training loss: 10528626905361.066 54 3.075934789535812\n",
      "Training loss: 10528626681664.854 55 3.074908133051359\n",
      "Training loss: 10528626905361.066 56 3.0738834461862736\n",
      "Training loss: 10528626905361.066 57 3.0728608122917183\n",
      "Training loss: 10528626681664.854 58 3.071840018134671\n",
      "Training loss: 10528626010576.213 59 3.0708211905312277\n",
      "Training loss: 10528626905361.066 60 3.0698043916279145\n",
      "Training loss: 10528627129057.28 61 3.068789639250672\n",
      "Training loss: 10528626905361.066 62 3.0677767378442256\n",
      "Training loss: 10528627129057.28 63 3.0667658520039183\n",
      "Training loss: 10528627129057.28 64 3.0657567651054975\n",
      "Training loss: 10528627576449.707 65 3.0647497777188613\n",
      "Training loss: 10528627352753.494 66 3.0637448223086454\n",
      "Training loss: 10528627352753.494 67 3.06274158187689\n",
      "Training loss: 10528627352753.494 68 3.061740454565921\n",
      "Training loss: 10528627129057.28 69 3.0607411609302146\n",
      "Training loss: 10528627352753.494 70 3.05974379065074\n",
      "Training loss: 10528628247538.346 71 3.0587484764072084\n",
      "Training loss: 10528627352753.494 72 3.0577549938344166\n",
      "Training loss: 10528626905361.066 73 3.0567634084877344\n",
      "Training loss: 10528627129057.28 74 3.055773726880016\n",
      "Training loss: 10528627352753.494 75 3.054786046988878\n",
      "Training loss: 10528627129057.28 76 3.0538002749983812\n",
      "Training loss: 10528626905361.066 77 3.0528163328945013\n",
      "Training loss: 10528626905361.066 78 3.0518343055383004\n",
      "Training loss: 10528627352753.494 79 3.050854186914884\n",
      "Training loss: 10528626681664.854 80 3.0498759495998904\n",
      "Training loss: 10528626457968.64 81 3.048899669981224\n",
      "Training loss: 10528626457968.64 82 3.0479251797342872\n",
      "Training loss: 10528627129057.28 83 3.046952614075267\n",
      "Training loss: 10528626234272.426 84 3.0459819376663098\n",
      "Training loss: 10528626234272.426 85 3.0450131853513414\n",
      "Training loss: 10528626234272.426 86 3.044046177983061\n",
      "Training loss: 10528626457968.64 87 3.043081099065168\n",
      "Training loss: 10528625786880.0 88 3.042117885403949\n",
      "Training loss: 10528625563183.787 89 3.0411565585399725\n",
      "Training loss: 10528625563183.787 90 3.040197104843506\n",
      "Training loss: 10528625786880.0 91 3.039239389847619\n",
      "Training loss: 10528625563183.787 92 3.038283564369457\n",
      "Training loss: 10528624444702.72 93 3.037329578394549\n",
      "Training loss: 10528624892095.146 94 3.0363775115655716\n",
      "Training loss: 10528624668398.934 95 3.0354271747106\n",
      "Training loss: 10528624221006.506 96 3.0344787502857606\n",
      "Training loss: 10528623326221.654 97 3.033531982223723\n",
      "Training loss: 10528623773614.08 98 3.032587223511067\n",
      "Training loss: 10528623326221.654 99 3.0316442196899387\n",
      "Training loss: 10528622878829.227 100 3.030702972415441\n",
      "Training loss: 10528624892095.146 101 3.029763436692727\n",
      "Training loss: 10528623326221.654 102 3.028825715563317\n",
      "Training loss: 10528622878829.227 103 3.027889679326754\n",
      "Training loss: 10528622655133.014 104 3.0269553442282784\n",
      "Training loss: 10528623326221.654 105 3.0260232905866147\n",
      "Training loss: 10528622655133.014 106 3.0250931730205\n",
      "Training loss: 10528622655133.014 107 3.0241649025627786\n",
      "Training loss: 10528622655133.014 108 3.023238463587574\n",
      "Training loss: 10528622207740.586 109 3.022313790196811\n",
      "Training loss: 10528621089259.52 110 3.021390873291052\n",
      "Training loss: 10528620418170.88 111 3.020469697706961\n",
      "Training loss: 10528621984044.373 112 3.0195504637141823\n",
      "Training loss: 10528620641867.094 113 3.018632800926132\n",
      "Training loss: 10528620641867.094 114 3.0177170735346097\n",
      "Training loss: 10528621089259.52 115 3.016802964582799\n",
      "Training loss: 10528620194474.666 116 3.015890815981831\n",
      "Training loss: 10528620194474.666 117 3.0149802396480134\n",
      "Training loss: 10528620194474.666 118 3.014071566007263\n",
      "Training loss: 10528619299689.812 119 3.01316445030919\n",
      "Training loss: 10528619299689.812 120 3.0122592066021574\n",
      "Training loss: 10528617510120.107 121 3.011355561201368\n",
      "Training loss: 10528618852297.387 122 3.010453935551318\n",
      "Training loss: 10528617957512.533 123 3.0095537571316684\n",
      "Training loss: 10528617957512.533 124 3.008655435454596\n",
      "Training loss: 10528617957512.533 125 3.0077587302410067\n",
      "Training loss: 10528617062727.68 126 3.006863837257256\n",
      "Training loss: 10528617062727.68 127 3.0059706237177477\n",
      "Training loss: 10528616167942.826 128 3.005079137819899\n",
      "Training loss: 10528616167942.826 129 3.0041893117320386\n",
      "Training loss: 10528615273157.973 130 3.003301220947271\n",
      "Training loss: 10528615496854.188 131 3.002414801872674\n",
      "Training loss: 10528615273157.973 132 3.001530109836766\n",
      "Training loss: 10528614378373.12 133 3.000647065134849\n",
      "Training loss: 10528614378373.12 134 2.999765701757695\n",
      "Training loss: 10528613930980.693 135 2.9988859696386525\n",
      "Training loss: 10528613483588.268 136 2.9980080095841233\n",
      "Training loss: 10528613036195.84 137 2.997131597059515\n",
      "Training loss: 10528611917714.773 138 2.9962569743058247\n",
      "Training loss: 10528612588803.414 139 2.9953839911374516\n",
      "Training loss: 10528612141410.986 140 2.9945125122314438\n",
      "Training loss: 10528611694018.56 141 2.993642845162032\n",
      "Training loss: 10528611470322.346 142 2.9927747708794197\n",
      "Training loss: 10528610128145.066 143 2.991908421710933\n",
      "Training loss: 10528610128145.066 144 2.9910436423383433\n",
      "Training loss: 10528609904448.854 145 2.9901804314857077\n",
      "Training loss: 10528609680752.64 146 2.9893188661108123\n",
      "Training loss: 10528608562271.574 147 2.9884588878653116\n",
      "Training loss: 10528608562271.574 148 2.9876005322195556\n",
      "Training loss: 10528608338575.36 149 2.986743753271943\n",
      "Training loss: 10528606772701.867 150 2.9858886427191904\n",
      "Training loss: 10528607667486.72 151 2.9850351285426897\n",
      "Training loss: 10528607443790.506 152 2.9841830545912926\n",
      "Training loss: 10528606772701.867 153 2.983332586531156\n",
      "Training loss: 10528605430524.586 154 2.982483719987782\n",
      "Training loss: 10528604983132.16 155 2.9816364089786385\n",
      "Training loss: 10528604759435.947 156 2.980790668845274\n",
      "Training loss: 10528604983132.16 157 2.9799464982555017\n",
      "Training loss: 10528604088347.307 158 2.9791038933257328\n",
      "Training loss: 10528603417258.666 159 2.9782628621677762\n",
      "Training loss: 10528602746170.027 160 2.9774234768461554\n",
      "Training loss: 10528601627688.96 161 2.9765856298149918\n",
      "Training loss: 10528602075081.387 162 2.9757493404437563\n",
      "Training loss: 10528601403992.746 163 2.974914522327635\n",
      "Training loss: 10528600732904.107 164 2.9740815136239913\n",
      "Training loss: 10528600285511.68 165 2.973250003536829\n",
      "Training loss: 10528599838119.254 166 2.9724199399539883\n",
      "Training loss: 10528599614423.04 167 2.9715914911121533\n",
      "Training loss: 10528598272245.76 168 2.970764620230612\n",
      "Training loss: 10528597824853.334 169 2.9699396724625347\n",
      "Training loss: 10528598719638.188 170 2.969115605914756\n",
      "Training loss: 10528597824853.334 171 2.9682933343017592\n",
      "Training loss: 10528596258979.84 172 2.9674726422066176\n",
      "Training loss: 10528595587891.2 173 2.966653545032521\n",
      "Training loss: 10528594693106.346 174 2.9658358653501193\n",
      "Training loss: 10528594245713.92 175 2.9650197029110004\n",
      "Training loss: 10528593574625.28 176 2.964205118716444\n",
      "Training loss: 10528594022017.707 177 2.963392064096097\n",
      "Training loss: 10528592903536.64 178 2.9625804180841837\n",
      "Training loss: 10528593350929.066 179 2.9617703759043836\n",
      "Training loss: 10528592456144.213 180 2.9609617514390165\n",
      "Training loss: 10528591337663.146 181 2.9601546766414217\n",
      "Training loss: 10528590666574.506 182 2.959349012241565\n",
      "Training loss: 10528590219182.08 183 2.958544945373217\n",
      "Training loss: 10528589771789.654 184 2.9577422540016203\n",
      "Training loss: 10528589100701.014 185 2.9569410960900155\n",
      "Training loss: 10528588205916.16 186 2.9561414233768692\n",
      "Training loss: 10528587982219.947 187 2.9553432060273415\n",
      "Training loss: 10528587982219.947 188 2.9545463391802502\n",
      "Training loss: 10528586192650.24 189 2.953751069383372\n",
      "Training loss: 10528586192650.24 190 2.9529572549762904\n",
      "Training loss: 10528585968954.027 191 2.95216467185793\n",
      "Training loss: 10528585297865.387 192 2.951373505935208\n",
      "Training loss: 10528584403080.533 193 2.9505837919834876\n",
      "Training loss: 10528583955688.107 194 2.949794638817993\n",
      "Training loss: 10528629142323.2 195 2.9490018384923125\n",
      "Training loss: 10528602522473.812 196 2.9482129257695795\n",
      "Training loss: 10528591785055.574 197 2.9474259637803097\n",
      "Training loss: 10528585968954.027 198 2.9466407370116876\n",
      "Training loss: 10528583284599.467 199 2.945857561444528\n",
      "Training loss: 10528582613510.826 200 2.945076036570091\n",
      "Training loss: 10528583284599.467 201 2.94429622623683\n",
      "Training loss: 10528584179384.32 202 2.9435180597532815\n",
      "Training loss: 10528577468497.92 203 2.942772339105931\n",
      "Training loss: 10528729805619.2 204 2.9420348407235384\n",
      "Training loss: 10528673657869.654 205 2.9413090452892323\n",
      "Training loss: 10528649274982.4 206 2.9405870920100803\n",
      "Training loss: 10528631379285.334 207 2.939866336125032\n",
      "Training loss: 10528610351841.28 208 2.939138717144092\n",
      "Training loss: 10528572323485.014 209 2.9384110328362874\n",
      "Training loss: 10528568744345.6 210 2.937679511145781\n",
      "Training loss: 10528568296953.174 211 2.9369460311318263\n",
      "Training loss: 10528569639130.453 212 2.9362119387062613\n",
      "Training loss: 10528570981307.732 213 2.935477911211526\n",
      "Training loss: 10528573218269.867 214 2.9347446163585778\n",
      "Training loss: 10528576797409.28 215 2.93401214109207\n",
      "Training loss: 10528576797409.28 216 2.9332806368501907\n",
      "Training loss: 10528576797409.28 217 2.9325500851882143\n",
      "Training loss: 10528575678928.213 218 2.9318204305103532\n",
      "Training loss: 10528573889358.506 219 2.9310916320944735\n",
      "Training loss: 10528571876092.586 220 2.930363592236038\n",
      "Training loss: 10528570086522.88 221 2.9296365935013946\n",
      "Training loss: 10528568744345.6 222 2.9289105486027496\n",
      "Training loss: 10528567178472.107 223 2.928185567553234\n",
      "Training loss: 10528566731079.68 224 2.9274616210342597\n",
      "Training loss: 10528565836294.826 225 2.9267388528386182\n",
      "Training loss: 10528564941509.973 226 2.926017179121694\n",
      "Training loss: 10528564270421.334 227 2.9253050400056653\n",
      "Training loss: 10528620194474.666 228 2.9246205016044224\n",
      "Training loss: 10528565388902.4 229 2.923938773180642\n",
      "Training loss: 10528563599332.693 230 2.923251738337718\n",
      "Training loss: 10528562480851.627 231 2.922561473073966\n",
      "Training loss: 10528561138674.346 232 2.9218690174399837\n",
      "Training loss: 10528560020193.28 233 2.921175346995882\n",
      "Training loss: 10528559349104.64 234 2.9204810584196785\n",
      "Training loss: 10528559125408.426 235 2.919786734589272\n",
      "Training loss: 10528558230623.574 236 2.9190926675148354\n",
      "Training loss: 10528557783231.146 237 2.918399035075281\n",
      "Training loss: 10528557335838.72 238 2.9177050253488632\n",
      "Training loss: 10528567849560.746 239 2.9170114543562233\n",
      "Training loss: 10528559125408.426 240 2.9163192986430557\n",
      "Training loss: 10528556217357.654 241 2.9156281482648447\n",
      "Training loss: 10528554427787.947 242 2.9149379350520936\n",
      "Training loss: 10528515952039.254 243 2.9142739415534575\n",
      "Training loss: 10528573665662.293 244 2.9136329451385072\n",
      "Training loss: 10528600285511.68 245 2.9129891747180823\n",
      "Training loss: 10528614825765.547 246 2.912344612057591\n",
      "Training loss: 10528618404904.96 247 2.911699377376275\n",
      "Training loss: 10528612588803.414 248 2.911053114064009\n",
      "Training loss: 10528599390726.826 249 2.910405154631419\n",
      "Training loss: 10528583955688.107 250 2.90975595161105\n",
      "Training loss: 10528568968041.812 251 2.909105797966646\n",
      "Training loss: 10528557783231.146 252 2.9084551804270617\n",
      "Training loss: 10528551296040.96 253 2.907805061987095\n",
      "Training loss: 10528548164293.973 254 2.907156059137829\n",
      "Training loss: 10528548387990.188 255 2.9065085896048757\n",
      "Training loss: 10528551296040.96 256 2.9058629602787556\n",
      "Training loss: 10528554875180.373 257 2.9052190910496316\n",
      "Training loss: 10528557112142.506 258 2.9045769103374646\n",
      "Training loss: 10528559572800.854 259 2.9039363361047617\n",
      "Training loss: 10528558901712.213 260 2.903296898549921\n",
      "Training loss: 10528556664750.08 261 2.9026586756020096\n",
      "Training loss: 10528554651484.16 262 2.9020212981181905\n",
      "Training loss: 10528552190825.812 263 2.901384976166091\n",
      "Training loss: 10528549953863.68 264 2.900749533028232\n",
      "Training loss: 10528547493205.334 265 2.9001152171346063\n",
      "Training loss: 10528545479939.414 266 2.8994819782111465\n",
      "Training loss: 10528543466673.494 267 2.8988498418971402\n",
      "Training loss: 10528543019281.066 268 2.898218971878874\n",
      "Training loss: 10528543019281.066 269 2.8975893628997316\n",
      "Training loss: 10528541900800.0 270 2.8969611367813557\n",
      "Training loss: 10528541453407.574 271 2.8963338701816723\n",
      "Training loss: 10528541006015.146 272 2.8957079473222036\n",
      "Training loss: 10528540334926.506 273 2.8950832213152196\n",
      "Training loss: 10528540111230.293 274 2.8944595962125814\n",
      "Training loss: 10528538992749.227 275 2.8938370547399637\n",
      "Training loss: 10528536755787.094 276 2.893215714916313\n",
      "Training loss: 10528535637306.027 277 2.892595426334505\n",
      "Training loss: 10528534518824.96 278 2.8919761355907063\n",
      "Training loss: 10528533176647.68 279 2.8913579874319173\n",
      "Training loss: 10528532952951.467 280 2.8907409848872265\n",
      "Training loss: 10528531610774.188 281 2.8901250413241972\n",
      "Training loss: 10528530939685.547 282 2.8895101117838284\n",
      "Training loss: 10528530044900.693 283 2.888896462081998\n",
      "Training loss: 10528528479027.2 284 2.8882837780937094\n",
      "Training loss: 10528527807938.56 285 2.8876722287459047\n",
      "Training loss: 10528527584242.346 286 2.8870618590136226\n",
      "Training loss: 10528526242065.066 287 2.886452526344096\n",
      "Training loss: 10528525570976.426 288 2.885844298782463\n",
      "Training loss: 10528523781406.72 289 2.885237065504275\n",
      "Training loss: 10528523110318.08 290 2.8846309925609384\n",
      "Training loss: 10528522439229.44 291 2.8840259768728176\n",
      "Training loss: 10528521544444.586 292 2.883421992188537\n",
      "Training loss: 10528520649659.732 293 2.8828191599599884\n",
      "Training loss: 10528519531178.666 294 2.882217222024119\n",
      "Training loss: 10528519083786.24 295 2.8816164467769303\n",
      "Training loss: 10528517517912.746 296 2.881016695410014\n",
      "Training loss: 10528516846824.107 297 2.880417981970179\n",
      "Training loss: 10528514609861.973 298 2.879820194456282\n",
      "Training loss: 10528514609861.973 299 2.879223513107301\n",
      "Training loss: 10528513043988.48 300 2.8786277004234697\n",
      "Training loss: 10528511925507.414 301 2.8780330477930147\n",
      "Training loss: 10528511925507.414 302 2.8774392275157736\n",
      "Training loss: 10528511254418.773 303 2.8768463846877896\n",
      "Training loss: 10528510359633.92 304 2.87625444100768\n",
      "Training loss: 10528508570064.213 305 2.87566352689866\n",
      "Training loss: 10528507898975.574 306 2.875073477982911\n",
      "Training loss: 10528506780494.506 307 2.87448428238122\n",
      "Training loss: 10528506109405.867 308 2.873895936955767\n",
      "Training loss: 10528504543532.373 309 2.8733085018005213\n",
      "Training loss: 10528503425051.307 310 2.872721809740374\n",
      "Training loss: 10528502306570.24 311 2.872135982662762\n",
      "Training loss: 10528499174823.254 312 2.871565297495292\n",
      "Training loss: 10528599614423.04 313 2.871009571435583\n",
      "Training loss: 10528502753962.666 314 2.8704646653748136\n",
      "Training loss: 10528499622215.68 315 2.8699139432833936\n",
      "Training loss: 10528497832645.973 316 2.8693593755866442\n",
      "Training loss: 10528497385253.547 317 2.8688020425206635\n",
      "Training loss: 10528495595683.84 318 2.868243180099759\n",
      "Training loss: 10528494700898.986 319 2.867683219592829\n",
      "Training loss: 10528494253506.56 320 2.8671230401742838\n",
      "Training loss: 10528493582417.92 321 2.866562772119227\n",
      "Training loss: 10528492687633.066 322 2.866002873202176\n",
      "Training loss: 10528492687633.066 323 2.8654433560197834\n",
      "Training loss: 10528491569152.0 324 2.864884489052594\n",
      "Training loss: 10528489779582.293 325 2.8643262654589425\n",
      "Training loss: 10528489779582.293 326 2.86376877648741\n",
      "Training loss: 10528488213708.8 327 2.8632119196272035\n",
      "Training loss: 10528486424139.094 328 2.8626559799391984\n",
      "Training loss: 10528486647835.307 329 2.8621007763159048\n",
      "Training loss: 10528484634569.387 330 2.8615464691659476\n",
      "Training loss: 10528483963480.746 331 2.8609930355396678\n",
      "Training loss: 10528482621303.467 332 2.860440477941009\n",
      "Training loss: 10528481502822.4 333 2.8598887885844\n",
      "Training loss: 10528480384341.334 334 2.8593379388637223\n",
      "Training loss: 10528480608037.547 335 2.8587880563980512\n",
      "Training loss: 10528479265860.268 336 2.8582390252706587\n",
      "Training loss: 10528478594771.627 337 2.8576908958772917\n",
      "Training loss: 10528477699986.773 338 2.857143572331147\n",
      "Training loss: 10528476134113.28 339 2.8565969486444316\n",
      "Training loss: 10528475686720.854 340 2.8560511927179997\n",
      "Training loss: 10528474344543.574 341 2.8555061099394776\n",
      "Training loss: 10528473897151.146 342 2.8549617822326194\n",
      "Training loss: 10528471883885.227 343 2.8544183622397634\n",
      "Training loss: 10528470989100.373 344 2.853875440234756\n",
      "Training loss: 10528470094315.52 345 2.853333398730892\n",
      "Training loss: 10528468304745.812 346 2.8527918248248354\n",
      "Training loss: 10528466738872.32 347 2.852251058023731\n",
      "Training loss: 10528466067783.68 348 2.8517112742310853\n",
      "Training loss: 10528453764491.947 349 2.8512230319018848\n",
      "Training loss: 10528690658781.867 350 2.8507079186639253\n",
      "Training loss: 10528601851385.174 351 2.8502098895974974\n",
      "Training loss: 10528564270421.334 352 2.8497171398908536\n",
      "Training loss: 10528526018368.854 353 2.849213285124307\n",
      "Training loss: 10528454435580.586 354 2.8487151970652183\n",
      "Training loss: 10528448395782.826 355 2.84821280092038\n",
      "Training loss: 10528445711428.268 356 2.8477073698986963\n",
      "Training loss: 10528445935124.48 357 2.8472005617698146\n",
      "Training loss: 10528448843175.254 358 2.846692999418342\n",
      "Training loss: 10528453093403.307 359 2.846185363847631\n",
      "Training loss: 10528456001454.08 360 2.8456779723333643\n",
      "Training loss: 10528457791023.787 361 2.8451709416215296\n",
      "Training loss: 10528459133201.066 362 2.8446642180758333\n",
      "Training loss: 10528458462112.426 363 2.8441578773607605\n",
      "Training loss: 10528456001454.08 364 2.8436520044059073\n",
      "Training loss: 10528452646010.88 365 2.843146419967682\n",
      "Training loss: 10528450632744.96 366 2.842641551474413\n",
      "Training loss: 10528449737960.107 367 2.8421368282780697\n",
      "Training loss: 10528447500997.973 368 2.8416338507387557\n",
      "Training loss: 10528444816643.414 369 2.8411315355845885\n",
      "Training loss: 10528444816643.414 370 2.8406301003611683\n",
      "Training loss: 10528444145554.773 371 2.840129343208159\n",
      "Training loss: 10528443474466.133 372 2.839629566174524\n",
      "Training loss: 10528443027073.707 373 2.8391305437621703\n",
      "Training loss: 10528442579681.28 374 2.8386323746279367\n",
      "Training loss: 10528442579681.28 375 2.838134965838096\n",
      "Training loss: 10528441461200.213 376 2.8376384569230413\n",
      "Training loss: 10528439895326.72 377 2.8371425453489287\n",
      "Training loss: 10528438329453.227 378 2.8366474486775934\n",
      "Training loss: 10528437434668.373 379 2.836153140315367\n",
      "Training loss: 10528436092491.094 380 2.835659618011878\n",
      "Training loss: 10528435197706.24 381 2.835166716827687\n",
      "Training loss: 10528433631832.746 382 2.834674521306564\n",
      "Training loss: 10528432513351.68 383 2.8341831834111684\n",
      "Training loss: 10528432065959.254 384 2.83369262339987\n",
      "Training loss: 10528431171174.4 385 2.8332027892142166\n",
      "Training loss: 10528430276389.547 386 2.832713712075163\n",
      "Training loss: 10528429381604.693 387 2.832225319300707\n",
      "Training loss: 10528428263123.627 388 2.8317378514177394\n",
      "Training loss: 10528427592034.986 389 2.831251000633851\n",
      "Training loss: 10528425131376.64 390 2.830764990662934\n",
      "Training loss: 10528424683984.213 391 2.8302795720516936\n",
      "Training loss: 10528423341806.934 392 2.829794935372552\n",
      "Training loss: 10528422670718.293 393 2.8293109430705137\n",
      "Training loss: 10528420881148.586 394 2.8288277257538295\n",
      "Training loss: 10528420881148.586 395 2.8283451802257\n",
      "Training loss: 10528419986363.732 396 2.8278633224626084\n",
      "Training loss: 10528417973097.812 397 2.8273821875793987\n",
      "Training loss: 10528417749401.6 398 2.826901698647779\n",
      "Training loss: 10528415736135.68 399 2.8264219426186354\n",
      "Training loss: 10528415736135.68 400 2.825942932893398\n",
      "Training loss: 10528414393958.4 401 2.825464447854077\n",
      "Training loss: 10528412828084.906 402 2.824986720473641\n",
      "Training loss: 10528412156996.268 403 2.8245094359875864\n",
      "Training loss: 10528411038515.2 404 2.824032956493075\n",
      "Training loss: 10528409025249.28 405 2.8235569794439157\n",
      "Training loss: 10528408354160.64 406 2.823081680661571\n",
      "Training loss: 10528407235679.574 407 2.8226067882688057\n",
      "Training loss: 10528406340894.72 408 2.822132341351271\n",
      "Training loss: 10528404775021.227 409 2.8216579587085877\n",
      "Training loss: 10528402538059.094 410 2.8211836710289497\n",
      "Training loss: 10528398511527.254 411 2.820711205110442\n",
      "Training loss: 10528396274565.12 412 2.8202326921321457\n",
      "Training loss: 10528395603476.48 413 2.8197567170508306\n",
      "Training loss: 10528393590210.56 414 2.8192836149297307\n",
      "Training loss: 10528399630008.32 415 2.818804118024781\n",
      "Training loss: 10528399182615.893 416 2.8183339480229477\n",
      "Training loss: 10528398064134.826 417 2.817865970146083\n",
      "Training loss: 10528397616742.4 418 2.817399387965426\n",
      "Training loss: 10528396498261.334 419 2.8169341603305775\n",
      "Training loss: 10528395603476.48 420 2.8164700225528616\n",
      "Training loss: 10528394261299.2 421 2.8160070015323213\n",
      "Training loss: 10528393366514.346 422 2.815544831634681\n",
      "Training loss: 10528392248033.28 423 2.8150837523621495\n",
      "Training loss: 10528391800640.854 424 2.814623310241387\n",
      "Training loss: 10528390682159.787 425 2.8141637492785656\n",
      "Training loss: 10528389787374.934 426 2.8137048864426935\n",
      "Training loss: 10528388445197.654 427 2.8132468305187692\n",
      "Training loss: 10528387774109.014 428 2.8127894362266406\n",
      "Training loss: 10528386431931.732 429 2.812332686967037\n",
      "Training loss: 10528385313450.666 430 2.8118766585275483\n",
      "Training loss: 10528383747577.174 431 2.8114212951837505\n",
      "Training loss: 10528383076488.533 432 2.8109666317858464\n",
      "Training loss: 10528381958007.467 433 2.8105125919210834\n",
      "Training loss: 10528381063222.613 434 2.81005920945305\n",
      "Training loss: 10528379497349.12 435 2.809606493524955\n",
      "Training loss: 10528378155171.84 436 2.809154393959142\n",
      "Training loss: 10528377036690.773 437 2.8087029529036003\n",
      "Training loss: 10528376365602.133 438 2.8082521231992463\n",
      "Training loss: 10528375023424.854 439 2.8078019398554805\n",
      "Training loss: 10528373681247.574 440 2.807352437039524\n",
      "Training loss: 10528373457551.36 441 2.8069035115552947\n",
      "Training loss: 10528371667981.654 442 2.8064552818903135\n",
      "Training loss: 10528371220589.227 443 2.8060075176852717\n",
      "Training loss: 10528370549500.586 444 2.805560548785027\n",
      "Training loss: 10528368312538.453 445 2.8051140519647024\n",
      "Training loss: 10528368088842.24 446 2.8046682082740335\n",
      "Training loss: 10528366075576.32 447 2.8042228845924657\n",
      "Training loss: 10528365180791.467 448 2.8037782727545633\n",
      "Training loss: 10528365404487.68 449 2.803334215070807\n",
      "Training loss: 10528363167525.547 450 2.8028908289952965\n",
      "Training loss: 10528361377955.84 451 2.8024480007921806\n",
      "Training loss: 10528361377955.84 452 2.8020057493138095\n",
      "Training loss: 10528360483170.986 453 2.8015642022211966\n",
      "Training loss: 10528358469905.066 454 2.801123288685694\n",
      "Training loss: 10528357351424.0 455 2.8006828632174345\n",
      "Training loss: 10528356680335.36 456 2.8002431427908485\n",
      "Training loss: 10528356009246.72 457 2.7998040621749114\n",
      "Training loss: 10528354443373.227 458 2.7993655619330475\n",
      "Training loss: 10528354219677.014 459 2.798927660538032\n",
      "Training loss: 10528351535322.453 460 2.7984903853549836\n",
      "Training loss: 10528350864233.812 461 2.798053717283742\n",
      "Training loss: 10528350640537.6 462 2.797617671183553\n",
      "Training loss: 10528349074664.107 463 2.797182184816281\n",
      "Training loss: 10528348179879.254 464 2.7967472915952944\n",
      "Training loss: 10528346837701.973 465 2.79631307207276\n",
      "Training loss: 10528345271828.48 466 2.795879431166828\n",
      "Training loss: 10528344153347.414 467 2.7954463389386124\n",
      "Training loss: 10528343929651.2 468 2.7950138025076323\n",
      "Training loss: 10528342363777.707 469 2.7945819047943186\n",
      "Training loss: 10528340797904.213 470 2.7941505828841997\n",
      "Training loss: 10528339903119.36 471 2.7937198444731015\n",
      "Training loss: 10528339008334.506 472 2.793289734578735\n",
      "Training loss: 10528337442461.014 473 2.7928601457711224\n",
      "Training loss: 10528337218764.8 474 2.7924310827584558\n",
      "Training loss: 10528335429195.094 475 2.7920025681005907\n",
      "Training loss: 10528334534410.24 476 2.7915746540827953\n",
      "Training loss: 10528333192232.96 477 2.791147329468668\n",
      "Training loss: 10528332073751.893 478 2.790720397824879\n",
      "Training loss: 10528330507878.4 479 2.7902941106986106\n",
      "Training loss: 10528329836789.76 480 2.7898683326407974\n",
      "Training loss: 10528328718308.693 481 2.7894431463807976\n",
      "Training loss: 10528328270916.268 482 2.7890184967263996\n",
      "Training loss: 10528326481346.56 483 2.7885944640750604\n",
      "Training loss: 10528325362865.494 484 2.788170873240136\n",
      "Training loss: 10528324244384.426 485 2.787747937209734\n",
      "Training loss: 10528322902207.146 486 2.7873255979819964\n",
      "Training loss: 10528321336333.654 487 2.786903753678391\n",
      "Training loss: 10528320888941.227 488 2.7864824583707652\n",
      "Training loss: 10528319994156.373 489 2.786061725558662\n",
      "Training loss: 10528318651979.094 490 2.785641591449646\n",
      "Training loss: 10528317757194.24 491 2.785222032987157\n",
      "Training loss: 10528315743928.32 492 2.7848028512778416\n",
      "Training loss: 10528315072839.68 493 2.7843843533270847\n",
      "Training loss: 10528313506966.188 494 2.783966366229031\n",
      "Training loss: 10528313283269.973 495 2.7835489330460748\n",
      "Training loss: 10528312164788.906 496 2.7831319660974336\n",
      "Training loss: 10528310151522.986 497 2.7827155377012556\n",
      "Training loss: 10528309704130.56 498 2.7822997124650013\n",
      "Training loss: 10528308585649.494 499 2.781884428241374\n",
      "Training loss: 10528306572383.574 500 2.781469561310412\n",
      "Training loss: 10528305006510.08 501 2.7810553254180976\n",
      "Training loss: 10528304335421.44 502 2.7806416630620565\n",
      "Training loss: 10528303664332.8 503 2.7802284826126114\n",
      "Training loss: 10528302545851.732 504 2.779815824566617\n",
      "Training loss: 10528300979978.24 505 2.7794037659043775\n",
      "Training loss: 10528299190408.533 506 2.778992262391829\n",
      "Training loss: 10528298743016.107 507 2.778581221245038\n",
      "Training loss: 10528297400838.826 508 2.7781707714573476\n",
      "Training loss: 10528296282357.76 509 2.7777608537968264\n",
      "Training loss: 10528295163876.693 510 2.7773513900617344\n",
      "Training loss: 10528294045395.627 511 2.7769424573718684\n",
      "Training loss: 10528292703218.346 512 2.7765341552954856\n",
      "Training loss: 10528291361041.066 513 2.7761261793387666\n",
      "Training loss: 10528290913648.64 514 2.775718818336373\n",
      "Training loss: 10528289347775.146 515 2.775311966213208\n",
      "Training loss: 10528287334509.227 516 2.7749056698825285\n",
      "Training loss: 10528287110813.014 517 2.774499851861137\n",
      "Training loss: 10528285544939.52 518 2.7740944852779252\n",
      "Training loss: 10528284873850.88 519 2.7736897386755968\n",
      "Training loss: 10528283307977.387 520 2.7732854576181314\n",
      "Training loss: 10528282860584.96 521 2.77288168967064\n",
      "Training loss: 10528280399926.613 522 2.7724783656523306\n",
      "Training loss: 10528279505141.76 523 2.7720755204690146\n",
      "Training loss: 10528278610356.906 524 2.7716732371956887\n",
      "Training loss: 10528277491875.84 525 2.771271366592965\n",
      "Training loss: 10528276149698.56 526 2.7708699659322855\n",
      "Training loss: 10528274136432.64 527 2.770469164423275\n",
      "Training loss: 10528273912736.426 528 2.770068711292861\n",
      "Training loss: 10528281294711.467 529 2.769668799340988\n",
      "Training loss: 10528271675774.293 530 2.769269508701027\n",
      "Training loss: 10528270109900.8 531 2.7688707560808945\n",
      "Training loss: 10528268096634.88 532 2.768472448595324\n",
      "Training loss: 10528267649242.453 533 2.7680745728935885\n",
      "Training loss: 10528266978153.812 534 2.7676772647673404\n",
      "Training loss: 10528265188584.107 535 2.767280395766867\n",
      "Training loss: 10528264293799.254 536 2.766884022681577\n",
      "Training loss: 10528263175318.188 537 2.766488005337602\n",
      "Training loss: 10528261385748.48 538 2.766092455796973\n",
      "Training loss: 10528259596178.773 539 2.765697492906656\n",
      "Training loss: 10528259596178.773 540 2.765302842620867\n",
      "Training loss: 10528258030305.28 541 2.7649087216029775\n",
      "Training loss: 10528257135520.426 542 2.764515041229089\n",
      "Training loss: 10528254674862.08 543 2.764121823602341\n",
      "Training loss: 10528253780077.227 544 2.763729116706659\n",
      "Training loss: 10528252214203.732 545 2.7633368253158808\n",
      "Training loss: 10528251766811.307 546 2.7629450901316632\n",
      "Training loss: 10528249753545.387 547 2.762553542703944\n",
      "Training loss: 10528248187671.893 548 2.762162523525194\n",
      "Training loss: 10528247292887.04 549 2.7617718851019775\n",
      "Training loss: 10528246398102.188 550 2.7613816835414595\n",
      "Training loss: 10528244832228.693 551 2.7609918698724374\n",
      "Training loss: 10528242818962.773 552 2.760602234362219\n",
      "Training loss: 10528241700481.707 553 2.7602130136898846\n",
      "Training loss: 10528240358304.426 554 2.759823901375506\n",
      "Training loss: 10528238121342.293 555 2.7594346783658286\n",
      "Training loss: 10528234765899.094 556 2.759045497984528\n",
      "Training loss: 10528227160227.84 557 2.7586592803252095\n",
      "Training loss: 10528190921441.28 558 2.758325112315636\n",
      "Training loss: 10528249306152.96 559 2.757982172654267\n",
      "Training loss: 10528305230206.293 560 2.7576317728679838\n",
      "Training loss: 10528341692689.066 561 2.7572759774859454\n",
      "Training loss: 10528355785550.506 562 2.7569166453490332\n",
      "Training loss: 10528350864233.812 563 2.7565547933991037\n",
      "Training loss: 10528331850055.68 564 2.7561910451012968\n",
      "Training loss: 10528306124991.146 565 2.755825981421876\n",
      "Training loss: 10528280399926.613 566 2.7554600056534735\n",
      "Training loss: 10528258701393.92 567 2.755093611170311\n",
      "Training loss: 10528245727013.547 568 2.754726922709767\n",
      "Training loss: 10528239910912.0 569 2.7543605449340083\n",
      "Training loss: 10528240358304.426 570 2.753994457179114\n",
      "Training loss: 10528245950709.76 571 2.753628905632293\n",
      "Training loss: 10528251766811.307 572 2.7532638012784747\n",
      "Training loss: 10528257135520.426 573 2.7528992770882956\n",
      "Training loss: 10528258477697.707 574 2.7525352003129315\n",
      "Training loss: 10528258477697.707 575 2.752171565838471\n",
      "Training loss: 10528256017039.36 576 2.751808273799761\n",
      "Training loss: 10528251319418.88 577 2.7514454063973153\n",
      "Training loss: 10528246398102.188 578 2.7510828739101325\n",
      "Training loss: 10528242147874.133 579 2.7507207525898534\n",
      "Training loss: 10528238345038.506 580 2.7503589671933124\n",
      "Training loss: 10528234318506.666 581 2.7499977062068397\n",
      "Training loss: 10528233200025.6 582 2.7496368125761306\n",
      "Training loss: 10528232081544.533 583 2.7492764038982846\n",
      "Training loss: 10528231186759.68 584 2.7489165296354616\n",
      "Training loss: 10528230515671.04 585 2.748557007567011\n",
      "Training loss: 10528228949797.547 586 2.7481979562804124\n",
      "Training loss: 10528228278708.906 587 2.747839406661909\n",
      "Training loss: 10528226489139.2 588 2.747481335571905\n",
      "Training loss: 10528225146961.92 589 2.747123617683765\n",
      "Training loss: 10528223133696.0 590 2.746766322619549\n",
      "Training loss: 10528221120430.08 591 2.7464095282559517\n",
      "Training loss: 10528219554556.586 592 2.7460530837126504\n",
      "Training loss: 10528217317594.453 593 2.7456971602277824\n",
      "Training loss: 10528215304328.533 594 2.745341584053915\n",
      "Training loss: 10528214185847.467 595 2.744986501002948\n",
      "Training loss: 10528213067366.4 596 2.7446318342050913\n",
      "Training loss: 10528210830404.268 597 2.7442775821738583\n",
      "Training loss: 10528210159315.627 598 2.743923813689745\n",
      "Training loss: 10528209040834.56 599 2.7435705036184133\n",
      "Training loss: 10528207251264.854 600 2.7432175879817233\n",
      "Training loss: 10528205909087.574 601 2.7428650745162724\n",
      "Training loss: 10528203895821.654 602 2.7425129339913585\n",
      "Training loss: 10528202777340.586 603 2.742161278602902\n",
      "Training loss: 10528200987770.88 604 2.7418100988636955\n",
      "Training loss: 10528199645593.6 605 2.741459277689842\n",
      "Training loss: 10528197408631.467 606 2.7411088820355314\n",
      "Training loss: 10528196290150.4 607 2.7407589127166774\n",
      "Training loss: 10528195619061.76 608 2.7404094374849146\n",
      "Training loss: 10528193382099.627 609 2.7400603158791137\n",
      "Training loss: 10528192039922.346 610 2.739711618021567\n",
      "Training loss: 10528190250352.64 611 2.739363389456958\n",
      "Training loss: 10528189579264.0 612 2.7390155734351396\n",
      "Training loss: 10528187565998.08 613 2.738668099297418\n",
      "Training loss: 10528186447517.014 614 2.738321090259938\n",
      "Training loss: 10528185105339.732 615 2.7379744969896844\n",
      "Training loss: 10528183539466.24 616 2.7376283646797193\n",
      "Training loss: 10528182420985.174 617 2.7372825727097583\n",
      "Training loss: 10528180855111.68 618 2.7369371901789536\n",
      "Training loss: 10528179960326.826 619 2.736592243679216\n",
      "Training loss: 10528177947060.906 620 2.736247754301151\n",
      "Training loss: 10528176828579.84 621 2.7359036281583187\n",
      "Training loss: 10528174591617.707 622 2.7355598273368384\n",
      "Training loss: 10528173249440.426 623 2.7352165204162238\n",
      "Training loss: 10528173249440.426 624 2.734873627348927\n",
      "Training loss: 10528170788782.08 625 2.7345311712258327\n",
      "Training loss: 10528169670301.014 626 2.7341890042495356\n",
      "Training loss: 10528167657035.094 627 2.7338474012402334\n",
      "Training loss: 10528166762250.24 628 2.7335060256089267\n",
      "Training loss: 10528166091161.6 629 2.733165207104225\n",
      "Training loss: 10528163854199.467 630 2.732824646070036\n",
      "Training loss: 10528162288325.973 631 2.7324845944025395\n",
      "Training loss: 10528161169844.906 632 2.7321448571043225\n",
      "Training loss: 10528159827667.627 633 2.731805537563973\n",
      "Training loss: 10528158709186.56 634 2.731466711832494\n",
      "Training loss: 10528157590705.494 635 2.731128157449481\n",
      "Training loss: 10528157143313.066 636 2.730790029922938\n",
      "Training loss: 10528154235262.293 637 2.7304523273312595\n",
      "Training loss: 10528153116781.227 638 2.7301150486509687\n",
      "Training loss: 10528151550907.732 639 2.729778131990172\n",
      "Training loss: 10528149761338.027 640 2.7294416413027984\n",
      "Training loss: 10528148419160.746 641 2.7291054965586072\n",
      "Training loss: 10528146629591.04 642 2.7287697174791754\n",
      "Training loss: 10528145958502.4 643 2.7284344551879025\n",
      "Training loss: 10528144168932.693 644 2.7280995072549\n",
      "Training loss: 10528142603059.2 645 2.7277648490362973\n",
      "Training loss: 10528141708274.346 646 2.7274306604349885\n",
      "Training loss: 10528140589793.28 647 2.7270968536153566\n",
      "Training loss: 10528139023919.787 648 2.7267634307629214\n",
      "Training loss: 10528137458046.293 649 2.726430378166547\n",
      "Training loss: 10528136563261.44 650 2.726097794634143\n",
      "Training loss: 10528135221084.16 651 2.725765530573655\n",
      "Training loss: 10528133878906.88 652 2.725433646369264\n",
      "Training loss: 10528132760425.812 653 2.725102113826837\n",
      "Training loss: 10528131194552.32 654 2.724771007222079\n",
      "Training loss: 10528129404982.613 655 2.724440247429181\n",
      "Training loss: 10528127615412.906 656 2.724109893920111\n",
      "Training loss: 10528127391716.693 657 2.723779891782309\n",
      "Training loss: 10528125154754.56 658 2.7234502589803156\n",
      "Training loss: 10528124259969.707 659 2.7231210012402163\n",
      "Training loss: 10528123141488.64 660 2.722792127921495\n",
      "Training loss: 10528121575615.146 661 2.7224635852940158\n",
      "Training loss: 10528120457134.08 662 2.7221354757032805\n",
      "Training loss: 10528118891260.586 663 2.721807659257692\n",
      "Training loss: 10528117101690.88 664 2.7214802787034924\n",
      "Training loss: 10528115312121.174 665 2.721153292985886\n",
      "Training loss: 10528114417336.32 666 2.720826611916869\n",
      "Training loss: 10528113522551.467 667 2.72050033773077\n",
      "Training loss: 10528111956677.973 668 2.7201744388299542\n",
      "Training loss: 10528111061893.12 669 2.7198488622938544\n",
      "Training loss: 10528108824930.986 670 2.7195236312128346\n",
      "Training loss: 10528107706449.92 671 2.7191987858667845\n",
      "Training loss: 10528106587968.854 672 2.7188743021190573\n",
      "Training loss: 10528105469487.787 673 2.718550132217138\n",
      "Training loss: 10528102785133.227 674 2.7182263172609527\n",
      "Training loss: 10528102561437.014 675 2.7179028479439173\n",
      "Training loss: 10528100324474.88 676 2.7175797380733657\n",
      "Training loss: 10528099653386.24 677 2.717257013456539\n",
      "Training loss: 10528098311208.96 678 2.7169346854830008\n",
      "Training loss: 10528095850550.613 679 2.7166125938664694\n",
      "Training loss: 10528094955765.76 680 2.7162908626834854\n",
      "Training loss: 10528093613588.48 681 2.7159695772196835\n",
      "Training loss: 10528091824018.773 682 2.7156486888321436\n",
      "Training loss: 10528090929233.92 683 2.7153280647262696\n",
      "Training loss: 10528089363360.426 684 2.715007746663504\n",
      "Training loss: 10528087797486.934 685 2.714687892434118\n",
      "Training loss: 10528087350094.506 686 2.7143683521205264\n",
      "Training loss: 10528085560524.8 687 2.714049244296378\n",
      "Training loss: 10528084665739.947 688 2.71373029083152\n",
      "Training loss: 10528082428777.812 689 2.713411859615086\n",
      "Training loss: 10528081310296.746 690 2.7130936604119653\n",
      "Training loss: 10528080191815.68 691 2.7127759032731054\n",
      "Training loss: 10528078178549.76 692 2.7124585184414625\n",
      "Training loss: 10528077283764.906 693 2.7121413654419175\n",
      "Training loss: 10528076165283.84 694 2.711824611377338\n",
      "Training loss: 10528074375714.133 695 2.7115082376852926\n",
      "Training loss: 10528072809840.64 696 2.7111921981031086\n",
      "Training loss: 10528071691359.574 697 2.7108764245112047\n",
      "Training loss: 10528069901789.867 698 2.7105610302697287\n",
      "Training loss: 10528069007005.014 699 2.710245951057247\n",
      "Training loss: 10528067441131.52 700 2.709931274494909\n",
      "Training loss: 10528065875258.027 701 2.709616831411374\n",
      "Training loss: 10528064085688.32 702 2.709302727522569\n",
      "Training loss: 10528062967207.254 703 2.708988957690391\n",
      "Training loss: 10528061848726.188 704 2.7086755570835375\n",
      "Training loss: 10528060282852.693 705 2.7083624424979442\n",
      "Training loss: 10528059388067.84 706 2.708049597666537\n",
      "Training loss: 10528057374801.92 707 2.7077369331796617\n",
      "Training loss: 10528056032624.64 708 2.7074246897993013\n",
      "Training loss: 10528054914143.574 709 2.707112643010422\n",
      "Training loss: 10528053348270.08 710 2.7068008792235014\n",
      "Training loss: 10528052229789.014 711 2.7064895191048253\n",
      "Training loss: 10528050663915.52 712 2.706178409106328\n",
      "Training loss: 10528048426953.387 713 2.7058675538903842\n",
      "Training loss: 10528047532168.533 714 2.705557013034186\n",
      "Training loss: 10528046413687.467 715 2.7052466864965137\n",
      "Training loss: 10528044176725.334 716 2.7049367074604995\n",
      "Training loss: 10528042834548.053 717 2.7046270316774135\n",
      "Training loss: 10528041268674.56 718 2.7043176065484986\n",
      "Training loss: 10528039479104.854 719 2.7040084563165805\n",
      "Training loss: 10528038360623.787 720 2.7036994524988875\n",
      "Training loss: 10528036571054.08 721 2.703390870634061\n",
      "Training loss: 10528034334091.947 722 2.703082584270163\n",
      "Training loss: 10528033215610.88 723 2.702774917728095\n",
      "Training loss: 10528029189079.04 724 2.702469251985351\n",
      "Training loss: 10527936355150.506 725 2.7022656469385566\n",
      "Training loss: 10528081533992.96 726 2.702015895994692\n",
      "Training loss: 10528151774603.947 727 2.701747706317845\n",
      "Training loss: 10528188908175.36 728 2.70147054424509\n",
      "Training loss: 10528193829492.053 729 2.701188805620111\n",
      "Training loss: 10528172578351.787 730 2.7009050963887318\n",
      "Training loss: 10528136563261.44 731 2.700620729640149\n",
      "Training loss: 10528096521639.254 732 2.700336604911567\n",
      "Training loss: 10528060506548.906 733 2.700052552379204\n",
      "Training loss: 10528035005180.586 734 2.699768813559999\n",
      "Training loss: 10528022254496.426 735 2.6994851403841182\n",
      "Training loss: 10528021136015.36 736 2.699201442926267\n",
      "Training loss: 10528027623205.547 737 2.698917550266739\n",
      "Training loss: 10528037913231.36 738 2.6986335055211863\n",
      "Training loss: 10528047084776.107 739 2.6983493672994987\n",
      "Training loss: 10528053795662.506 740 2.6980652404401892\n",
      "Training loss: 10528055808928.426 741 2.69778133475312\n",
      "Training loss: 10528054243054.934 742 2.6974977705519616\n",
      "Training loss: 10528048426953.387 743 2.697214460175227\n",
      "Training loss: 10528041939763.2 744 2.696931547082282\n",
      "Training loss: 10528034557788.16 745 2.696648996871713\n",
      "Training loss: 10528028070597.973 746 2.696366816373935\n",
      "Training loss: 10528024491458.56 747 2.6960851159773003\n",
      "Training loss: 10528022030800.213 748 2.6958034894436778\n",
      "Training loss: 10528021136015.36 749 2.6955221897919643\n",
      "Training loss: 10528020241230.506 750 2.695241017617996\n",
      "Training loss: 10528020912319.146 751 2.6949601278051594\n",
      "Training loss: 10528020241230.506 752 2.694679498280204\n",
      "Training loss: 10528019346445.654 753 2.6943989776993127\n",
      "Training loss: 10528016885787.307 754 2.6941189441821276\n",
      "Training loss: 10528015096217.6 755 2.6938391496050715\n",
      "Training loss: 10528011964470.613 756 2.6935601719340134\n",
      "Training loss: 10527973488721.92 757 2.693318484364328\n",
      "Training loss: 10528026281028.268 758 2.6930675112932168\n",
      "Training loss: 10528052677181.44 759 2.6928094193305885\n",
      "Training loss: 10528066546346.666 760 2.692547557658278\n",
      "Training loss: 10528068112220.16 761 2.692283745452658\n",
      "Training loss: 10528059611764.053 762 2.6920191672371057\n",
      "Training loss: 10528044624117.76 763 2.691754402874746\n",
      "Training loss: 10528027846901.76 764 2.6914898447216773\n",
      "Training loss: 10528012859255.467 765 2.6912254380856426\n",
      "Training loss: 10528001898141.014 766 2.6909613569185358\n",
      "Training loss: 10527995858343.254 767 2.6906974466615057\n",
      "Training loss: 10527994068773.547 768 2.6904336382212106\n",
      "Training loss: 10527995410950.826 769 2.6901698689401834\n",
      "Training loss: 10527998542697.812 770 2.689906192379371\n",
      "Training loss: 10528000779659.947 771 2.689642600585285\n",
      "Training loss: 10528002345533.44 772 2.6893791692048965\n",
      "Training loss: 10528002345533.44 773 2.68911595529196\n",
      "Training loss: 10527999884875.094 774 2.6888530108997086\n",
      "Training loss: 10527997647912.96 775 2.6885904440903627\n",
      "Training loss: 10527994516165.973 776 2.6883281790362608\n",
      "Training loss: 10527990042241.707 777 2.688066216795006\n",
      "Training loss: 10527986686798.506 778 2.6878046522749868\n",
      "Training loss: 10527985344621.227 779 2.6875433609636397\n",
      "Training loss: 10527982436570.453 780 2.6872823898499894\n",
      "Training loss: 10527980647000.746 781 2.687021658079113\n",
      "Training loss: 10527979752215.893 782 2.686761134394766\n",
      "Training loss: 10527978186342.4 783 2.6865009205016825\n",
      "Training loss: 10527977738949.973 784 2.686241068221161\n",
      "Training loss: 10527976396772.693 785 2.6859813670229693\n",
      "Training loss: 10527975501987.84 786 2.685721924986875\n",
      "Training loss: 10527973041329.494 787 2.685462850201619\n",
      "Training loss: 10527971699152.213 788 2.6852041090670995\n",
      "Training loss: 10527969909582.506 789 2.684945514239408\n",
      "Training loss: 10527967896316.586 790 2.6846873863191787\n",
      "Training loss: 10527965659354.453 791 2.6844294493357044\n",
      "Training loss: 10527964093480.96 792 2.6841718487493855\n",
      "Training loss: 10527962527607.467 793 2.683914341940744\n",
      "Training loss: 10527961632822.613 794 2.6836572931222804\n",
      "Training loss: 10527959172164.268 795 2.6834006099884653\n",
      "Training loss: 10527957606290.773 796 2.683143968062292\n",
      "Training loss: 10527956487809.707 797 2.6828876433735234\n",
      "Training loss: 10527954921936.213 798 2.6826316111111472\n",
      "Training loss: 10527953132366.506 799 2.682375890079204\n",
      "Training loss: 10527952013885.44 800 2.682120394146388\n",
      "Training loss: 10527950448011.947 801 2.6818652050254665\n",
      "Training loss: 10527948211049.812 802 2.6816102299088174\n",
      "Training loss: 10527946868872.533 803 2.681355532561139\n",
      "Training loss: 10527945526695.254 804 2.681101182841551\n",
      "Training loss: 10527944408214.188 805 2.68084708441612\n",
      "Training loss: 10527942394948.268 806 2.6805931438514787\n",
      "Training loss: 10527941052770.986 807 2.680339520194384\n",
      "Training loss: 10527939039505.066 808 2.680086231441982\n",
      "Training loss: 10527937921024.0 809 2.679832975879491\n",
      "Training loss: 10527936131454.293 810 2.6795801081202875\n",
      "Training loss: 10527934789277.014 811 2.679327460834159\n",
      "Training loss: 10527932999707.307 812 2.6790751999608315\n",
      "Training loss: 10527931433833.812 813 2.678822982418797\n",
      "Training loss: 10527929420567.893 814 2.67857111755411\n",
      "Training loss: 10527928525783.04 815 2.6783194688059297\n",
      "Training loss: 10527926512517.12 816 2.6780680447990015\n",
      "Training loss: 10527924499251.2 817 2.677816931961983\n",
      "Training loss: 10527923604466.346 818 2.677565952572885\n",
      "Training loss: 10527922038592.854 819 2.6773152611217896\n",
      "Training loss: 10527920920111.787 820 2.6770648489757245\n",
      "Training loss: 10527919130542.08 821 2.676814573702368\n",
      "Training loss: 10527917564668.586 822 2.6765645596635506\n",
      "Training loss: 10527916222491.307 823 2.6763148518673807\n",
      "Training loss: 10527914880314.027 824 2.67606537859535\n",
      "Training loss: 10527910853782.188 825 2.675821224461268\n",
      "Training loss: 10527931210137.6 826 2.675568560110428\n",
      "Training loss: 10527911524870.826 827 2.675322522576453\n",
      "Training loss: 10527909735301.12 828 2.67507621222629\n",
      "Training loss: 10527907274642.773 829 2.6748296141398673\n",
      "Training loss: 10527905037680.64 830 2.6745830661414396\n",
      "Training loss: 10527904142895.787 831 2.6743364196586548\n",
      "Training loss: 10527902129629.867 832 2.67409001567006\n",
      "Training loss: 10527901011148.8 833 2.6738436538570234\n",
      "Training loss: 10527899221579.094 834 2.6735975540133152\n",
      "Training loss: 10527898103098.027 835 2.673351615927079\n",
      "Training loss: 10527896313528.32 836 2.6731061120894597\n",
      "Training loss: 10527892063300.268 837 2.672863670426147\n",
      "Training loss: 10527905485073.066 838 2.6726138798008576\n",
      "Training loss: 10527893181781.334 839 2.6723702403041436\n",
      "Training loss: 10527892510692.693 840 2.6721265166460437\n",
      "Training loss: 10527889826338.133 841 2.671882829000198\n",
      "Training loss: 10527888484160.854 842 2.6716392269879803\n",
      "Training loss: 10527887141983.574 843 2.6713959007163357\n",
      "Training loss: 10527886247198.72 844 2.671152839536716\n",
      "Training loss: 10527884010236.586 845 2.6709098640026157\n",
      "Training loss: 10527882891755.52 846 2.67066719082221\n",
      "Training loss: 10527881773274.453 847 2.670424650631739\n",
      "Training loss: 10527880207400.96 848 2.6701824147508675\n",
      "Training loss: 10527878865223.68 849 2.6699403974707585\n",
      "Training loss: 10527877746742.613 850 2.6696986493151855\n",
      "Training loss: 10527875957172.906 851 2.669457102042223\n",
      "Training loss: 10527874391299.414 852 2.669215781873385\n",
      "Training loss: 10527872378033.494 853 2.668974751838567\n",
      "Training loss: 10527871259552.426 854 2.6687339381236033\n",
      "Training loss: 10527869693678.934 855 2.6684933837620157\n",
      "Training loss: 10527868351501.654 856 2.6682530841007495\n",
      "Training loss: 10527867680413.014 857 2.668012988591609\n",
      "Training loss: 10527864996058.453 858 2.6677731458151097\n",
      "Training loss: 10527863877577.387 859 2.667533548733692\n",
      "Training loss: 10527862759096.32 860 2.6672941559721517\n",
      "Training loss: 10527860522134.188 861 2.6670549211626273\n",
      "Training loss: 10527859179956.906 862 2.66681593240542\n",
      "Training loss: 10527857614083.414 863 2.6665771875329844\n",
      "Training loss: 10527856719298.56 864 2.6663386638062803\n",
      "Training loss: 10527854706032.64 865 2.666100357421025\n",
      "Training loss: 10527854258640.213 866 2.665862325543723\n",
      "Training loss: 10527851797981.867 867 2.6656245147356135\n",
      "Training loss: 10527850903197.014 868 2.6653868676047945\n",
      "Training loss: 10527848666234.88 869 2.665149499645628\n",
      "Training loss: 10527847324057.6 870 2.6649123852412298\n",
      "Training loss: 10527846205576.533 871 2.6646755063402616\n",
      "Training loss: 10527844639703.04 872 2.664438844310768\n",
      "Training loss: 10527843521221.973 873 2.6642023874142167\n",
      "Training loss: 10527841731652.268 874 2.663966149795327\n",
      "Training loss: 10527839942082.56 875 2.6637301343843536\n",
      "Training loss: 10527838599905.28 876 2.663494471401003\n",
      "Training loss: 10527837257728.0 877 2.663258927901098\n",
      "Training loss: 10527835915550.72 878 2.6630236622736447\n",
      "Training loss: 10527834349677.227 879 2.662788677245176\n",
      "Training loss: 10527833007499.947 880 2.6625537930590726\n",
      "Training loss: 10527831217930.24 881 2.6623191259172945\n",
      "Training loss: 10527830099449.174 882 2.662084797386358\n",
      "Training loss: 10527828309879.467 883 2.661850746742375\n",
      "Training loss: 10527826072917.334 884 2.661616691047926\n",
      "Training loss: 10527825178132.48 885 2.6613829415592005\n",
      "Training loss: 10527823388562.773 886 2.6611494229709174\n",
      "Training loss: 10527822046385.494 887 2.6609161356075273\n",
      "Training loss: 10527820927904.426 888 2.660683063497688\n",
      "Training loss: 10527819585727.146 889 2.660450038135012\n",
      "Training loss: 10527817572461.227 890 2.6602173980754937\n",
      "Training loss: 10527816677676.373 891 2.659984910695055\n",
      "Training loss: 10527815111802.88 892 2.659752633889705\n",
      "Training loss: 10527812874840.746 893 2.65952062190732\n",
      "Training loss: 10527811532663.467 894 2.6592887909709564\n",
      "Training loss: 10527809966789.973 895 2.6590571797791096\n",
      "Training loss: 10527809072005.12 896 2.658825658545186\n",
      "Training loss: 10527806835042.986 897 2.6585943746725196\n",
      "Training loss: 10527806163954.346 898 2.658363365963848\n",
      "Training loss: 10527802361118.72 899 2.6581325214342404\n",
      "Training loss: 10527801690030.08 900 2.6579019451785806\n",
      "Training loss: 10527799676764.16 901 2.657671581210688\n",
      "Training loss: 10527798110890.666 902 2.657441308465075\n",
      "Training loss: 10527795873928.533 903 2.65721129641058\n",
      "Training loss: 10527793860662.613 904 2.6569815453545074\n",
      "Training loss: 10527786926080.0 905 2.6567535557543596\n",
      "Training loss: 10527773728003.414 906 2.6565191446350704\n",
      "Training loss: 10527783570636.8 907 2.656280069291459\n",
      "Training loss: 10527789834130.773 908 2.6560458248631615\n",
      "Training loss: 10527788939345.92 909 2.6558140549506652\n",
      "Training loss: 10527788939345.92 910 2.6555837938535665\n",
      "Training loss: 10527786478687.574 911 2.6553543071604335\n",
      "Training loss: 10527786031295.146 912 2.655125652077355\n",
      "Training loss: 10527785136510.293 913 2.6548976297971527\n",
      "Training loss: 10527783794333.014 914 2.6546701915716584\n",
      "Training loss: 10527781333674.666 915 2.654443188715369\n",
      "Training loss: 10527780662586.027 916 2.6542164264179764\n",
      "Training loss: 10527779096712.533 917 2.6539901062322007\n",
      "Training loss: 10527777978231.467 918 2.6537641356391384\n",
      "Training loss: 10527776188661.76 919 2.6535383476558647\n",
      "Training loss: 10527774399092.053 920 2.6533128692481007\n",
      "Training loss: 10527773280610.986 921 2.6530876885017234\n",
      "Training loss: 10527772162129.92 922 2.652862703473232\n",
      "Training loss: 10527770148864.0 923 2.652637954949635\n",
      "Training loss: 10527769254079.146 924 2.6524134622407316\n",
      "Training loss: 10527766569724.586 925 2.652189209490883\n",
      "Training loss: 10527765674939.732 926 2.6519650980697778\n",
      "Training loss: 10527764332762.453 927 2.6517411922658356\n",
      "Training loss: 10527762319496.533 928 2.6515175035502603\n",
      "Training loss: 10527761424711.68 929 2.651294034308282\n",
      "Training loss: 10527759635141.973 930 2.651070675818085\n",
      "Training loss: 10527758069268.48 931 2.6508475550189297\n",
      "Training loss: 10527756279698.773 932 2.6506245517841163\n",
      "Training loss: 10527754490129.066 933 2.650401759923228\n",
      "Training loss: 10527753819040.426 934 2.65017918294984\n",
      "Training loss: 10527751582078.293 935 2.6499567698555393\n",
      "Training loss: 10527750910989.654 936 2.6497345682781974\n",
      "Training loss: 10527748674027.52 937 2.649512576312801\n",
      "Training loss: 10527748002938.88 938 2.6492906031025787\n",
      "Training loss: 10527745542280.533 939 2.6490689322070486\n",
      "Training loss: 10527744423799.467 940 2.648847536799479\n",
      "Training loss: 10527741963141.12 941 2.6486262000813743\n",
      "Training loss: 10527740620963.84 942 2.6484050735053506\n",
      "Training loss: 10527738384001.707 943 2.6481842268571145\n",
      "Training loss: 10527737041824.426 944 2.6479635966522572\n",
      "Training loss: 10527734581166.08 945 2.6477430038370664\n",
      "Training loss: 10527731673115.307 946 2.647522867507187\n",
      "Training loss: 10527728093975.893 947 2.6473030783504337\n",
      "Training loss: 10527715790684.16 948 2.647096009641121\n",
      "Training loss: 10527736818128.213 949 2.6468514812061055\n",
      "Training loss: 10527730554634.24 950 2.646629763356042\n",
      "Training loss: 10527729436153.174 951 2.646411291837943\n",
      "Training loss: 10527728988760.746 952 2.6461933385015723\n",
      "Training loss: 10527727199191.04 953 2.645975299035828\n",
      "Training loss: 10527725857013.76 954 2.645757823460603\n",
      "Training loss: 10527724067444.053 955 2.6455406690040353\n",
      "Training loss: 10527723843747.84 956 2.6453237500079543\n",
      "Training loss: 10527722277874.346 957 2.645107116327022\n",
      "Training loss: 10527720935697.066 958 2.6448906872368694\n",
      "Training loss: 10527718475038.72 959 2.644674539959501\n",
      "Training loss: 10527717580253.867 960 2.6444587137962143\n",
      "Training loss: 10527716238076.586 961 2.6442430609908407\n",
      "Training loss: 10527714224810.666 962 2.644027698148337\n",
      "Training loss: 10527713106329.6 963 2.6438123961256674\n",
      "Training loss: 10527711764152.32 964 2.6435973753398105\n",
      "Training loss: 10527710198278.826 965 2.643382446269042\n",
      "Training loss: 10527708185012.906 966 2.643167896780469\n",
      "Training loss: 10527706842835.627 967 2.6429533633394415\n",
      "Training loss: 10527705724354.56 968 2.642739068019184\n",
      "Training loss: 10527703487392.426 969 2.6425248751241313\n",
      "Training loss: 10527702816303.787 970 2.64231080949491\n",
      "Training loss: 10527701026734.08 971 2.642097013465282\n",
      "Training loss: 10527699684556.8 972 2.6418834174186867\n",
      "Training loss: 10527697671290.88 973 2.641670029648236\n",
      "Training loss: 10527696552809.812 974 2.6414568303265704\n",
      "Training loss: 10527694763240.107 975 2.6412437792686583\n",
      "Training loss: 10527693421062.826 976 2.6410308449840816\n",
      "Training loss: 10527691855189.334 977 2.6408180862231783\n",
      "Training loss: 10527690289315.84 978 2.6406056968818716\n",
      "Training loss: 10527688499746.133 979 2.6403933720784476\n",
      "Training loss: 10527686933872.64 980 2.640181147151331\n",
      "Training loss: 10527684696910.506 981 2.6399690618779825\n",
      "Training loss: 10527683131037.014 982 2.6397572135970004\n",
      "Training loss: 10527681341467.307 983 2.6395456741351704\n",
      "Training loss: 10527679551897.6 984 2.63933459302567\n",
      "Training loss: 10527677091239.254 985 2.639124457801771\n",
      "Training loss: 10527671946226.346 986 2.63891897866482\n",
      "Training loss: 10527498357964.8 987 2.6388707859333778\n",
      "Training loss: 10527754713825.28 988 2.6387245022148718\n",
      "Training loss: 10527859179956.906 989 2.6385473338272174\n",
      "Training loss: 10527909511604.906 990 2.638354966825198\n",
      "Training loss: 10527910182693.547 991 2.6381568583717216\n",
      "Training loss: 10527877075653.973 992 2.637958398500925\n",
      "Training loss: 10527824059651.414 993 2.637762522719282\n",
      "Training loss: 10527765451243.52 994 2.6375699164913695\n",
      "Training loss: 10527714448506.88 995 2.6373801625726414\n",
      "Training loss: 10527679999290.027 996 2.637192039031412\n",
      "Training loss: 10527664564251.307 997 2.6370040973383877\n",
      "Training loss: 10527666353821.014 998 2.6368152376796483\n",
      "Training loss: 10527678657112.746 999 2.636624794591033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1000\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10527694539543.893 0 2.6364326649845373\n",
      "Training loss: 10527708856101.547 1 2.6362392139518236\n",
      "Training loss: 10527718698734.934 2 2.6360450043945414\n",
      "Training loss: 10527720264608.426 3 2.6358506788393896\n",
      "Training loss: 10527716014380.373 4 2.635656698106159\n",
      "Training loss: 10527707961316.693 5 2.635463574871557\n",
      "Training loss: 10527697671290.88 6 2.635271091757364\n",
      "Training loss: 10527687381265.066 7 2.6350793595633824\n",
      "Training loss: 10527680894074.88 8 2.6348882553193755\n",
      "Training loss: 10527675972758.188 9 2.6346974741763405\n",
      "Training loss: 10527673735796.053 10 2.6345068245662513\n",
      "Training loss: 10527673735796.053 11 2.634316181410067\n",
      "Training loss: 10527673959492.268 12 2.634125450856341\n",
      "Training loss: 10527674630580.906 13 2.633934723810274\n",
      "Training loss: 10527674630580.906 14 2.6337440334724334\n",
      "Training loss: 10527673288403.627 15 2.633553415029524\n",
      "Training loss: 10527671722530.133 16 2.633363014321915\n",
      "Training loss: 10527668814479.36 17 2.6331728880416065\n",
      "Training loss: 10527665459036.16 18 2.6329830034477273\n",
      "Training loss: 10527663669466.453 19 2.632793311420683\n",
      "Training loss: 10527660314023.254 20 2.632603942793579\n",
      "Training loss: 10527658300757.334 21 2.6324148328133683\n",
      "Training loss: 10527656063795.2 22 2.6322258536992353\n",
      "Training loss: 10527654274225.494 23 2.6320369444475076\n",
      "Training loss: 10527652932048.213 24 2.6318483510290798\n",
      "Training loss: 10527651366174.72 25 2.6316598459107197\n",
      "Training loss: 10527649352908.8 26 2.6314714095100653\n",
      "Training loss: 10527648234427.732 27 2.631283145508138\n",
      "Training loss: 10527645773769.387 28 2.631095122798311\n",
      "Training loss: 10527644431592.107 29 2.63090736790411\n",
      "Training loss: 10527642865718.613 30 2.6307195646805788\n",
      "Training loss: 10527640852452.693 31 2.6305320801601355\n",
      "Training loss: 10527639062882.986 32 2.630344823988185\n",
      "Training loss: 10527637049617.066 33 2.6301577352403225\n",
      "Training loss: 10527635483743.574 34 2.6299707297953994\n",
      "Training loss: 10527633246781.44 35 2.6297840065379914\n",
      "Training loss: 10527631904604.16 36 2.629597403415139\n",
      "Training loss: 10527629891338.24 37 2.6294109271133332\n",
      "Training loss: 10527628325464.746 38 2.6292247199133154\n",
      "Training loss: 10527626759591.254 39 2.6290387495435863\n",
      "Training loss: 10527625641110.188 40 2.628852716205803\n",
      "Training loss: 10527623627844.268 41 2.6286669224215427\n",
      "Training loss: 10527621614578.346 42 2.628481321765848\n",
      "Training loss: 10527620272401.066 43 2.6282960217743687\n",
      "Training loss: 10527618482831.36 44 2.628110736657454\n",
      "Training loss: 10527617140654.08 45 2.6279256668758313\n",
      "Training loss: 10527615351084.373 46 2.6277407622186835\n",
      "Training loss: 10527614008907.094 47 2.6275559700754236\n",
      "Training loss: 10527612443033.6 48 2.6273714580358494\n",
      "Training loss: 10527610206071.467 49 2.627187105281769\n",
      "Training loss: 10527608416501.76 50 2.6270028193243364\n",
      "Training loss: 10527607745413.12 51 2.626818745713498\n",
      "Training loss: 10527605508450.986 52 2.626634864680231\n",
      "Training loss: 10527604166273.707 53 2.6264511458256354\n",
      "Training loss: 10527602376704.0 54 2.6262675131312725\n",
      "Training loss: 10527600810830.506 55 2.626084142005854\n",
      "Training loss: 10527599244957.014 56 2.625900913589878\n",
      "Training loss: 10527597679083.52 57 2.6257178367724876\n",
      "Training loss: 10527596336906.24 58 2.625534978486224\n",
      "Training loss: 10527594099944.107 59 2.6253522428279257\n",
      "Training loss: 10527592534070.613 60 2.625169666046952\n",
      "Training loss: 10527591862981.973 61 2.624987251900592\n",
      "Training loss: 10527589402323.627 62 2.624805037217848\n",
      "Training loss: 10527588283842.56 63 2.6246229601954183\n",
      "Training loss: 10527587165361.494 64 2.624441036477574\n",
      "Training loss: 10527585152095.574 65 2.6242592857486504\n",
      "Training loss: 10527583586222.08 66 2.624077661459254\n",
      "Training loss: 10527582244044.8 67 2.623896273689364\n",
      "Training loss: 10527580230778.88 68 2.623715074160851\n",
      "Training loss: 10527578441209.174 69 2.6235339576254364\n",
      "Training loss: 10527576651639.467 70 2.623352945008559\n",
      "Training loss: 10527575309462.188 71 2.6231721579546203\n",
      "Training loss: 10527573296196.268 72 2.6229916193882947\n",
      "Training loss: 10527571954018.986 73 2.6228111389566124\n",
      "Training loss: 10527570835537.92 74 2.6226308538222023\n",
      "Training loss: 10527568598575.787 75 2.6224506847768545\n",
      "Training loss: 10527567703790.934 76 2.622270740889035\n",
      "Training loss: 10527565914221.227 77 2.622090871159214\n",
      "Training loss: 10527563900955.307 78 2.6219112631308272\n",
      "Training loss: 10527563006170.453 79 2.62173171157107\n",
      "Training loss: 10527561663993.174 80 2.621552365222856\n",
      "Training loss: 10527560098119.68 81 2.6213732285827622\n",
      "Training loss: 10527558755942.4 82 2.621194166678454\n",
      "Training loss: 10527557190068.906 83 2.6210152435061915\n",
      "Training loss: 10527554953106.773 84 2.62083661122915\n",
      "Training loss: 10527554058321.92 85 2.62065811415976\n",
      "Training loss: 10527552492448.426 86 2.620479620522004\n",
      "Training loss: 10527550702878.72 87 2.6203013866993574\n",
      "Training loss: 10527549584397.654 88 2.6201233406817197\n",
      "Training loss: 10527548242220.373 89 2.619945334146888\n",
      "Training loss: 10527545110473.387 90 2.6197674801686843\n",
      "Training loss: 10527544215688.533 91 2.619589802637232\n",
      "Training loss: 10527542649815.04 92 2.619412261581162\n",
      "Training loss: 10527541307637.76 93 2.619234818883233\n",
      "Training loss: 10527540189156.693 94 2.6190575827328417\n",
      "Training loss: 10527537952194.56 95 2.6188805158413624\n",
      "Training loss: 10527537057409.707 96 2.6187035576664406\n",
      "Training loss: 10527535491536.213 97 2.618526745105585\n",
      "Training loss: 10527533701966.506 98 2.618350086844328\n",
      "Training loss: 10527531912396.8 99 2.618173573254075\n",
      "Training loss: 10527531241308.16 100 2.617997340131793\n",
      "Training loss: 10527529004346.027 101 2.6178211498738233\n",
      "Training loss: 10527528333257.387 102 2.617645027834259\n",
      "Training loss: 10527526543687.68 103 2.617469092117614\n",
      "Training loss: 10527524977814.188 104 2.6172934078415966\n",
      "Training loss: 10527522517155.84 105 2.617117881665863\n",
      "Training loss: 10527521398674.773 106 2.6169423696563885\n",
      "Training loss: 10527519832801.28 107 2.6167671492826994\n",
      "Training loss: 10527518714320.213 108 2.616592033811193\n",
      "Training loss: 10527518043231.574 109 2.616417034182965\n",
      "Training loss: 10527514911484.586 110 2.6162422725279044\n",
      "Training loss: 10527513345611.094 111 2.6160675451873385\n",
      "Training loss: 10527512674522.453 112 2.615893087779331\n",
      "Training loss: 10527510884952.746 113 2.615718801249193\n",
      "Training loss: 10527509319079.254 114 2.615544427964801\n",
      "Training loss: 10527507529509.547 115 2.61537039153303\n",
      "Training loss: 10527506411028.48 116 2.6151964848453866\n",
      "Training loss: 10527505292547.414 117 2.6150227193381292\n",
      "Training loss: 10527503726673.92 118 2.6148491368456117\n",
      "Training loss: 10527502160800.426 119 2.614675656502618\n",
      "Training loss: 10527500371230.72 120 2.6145022933437314\n",
      "Training loss: 10527498805357.227 121 2.614329166405665\n",
      "Training loss: 10527497463179.947 122 2.6141561856845104\n",
      "Training loss: 10527496121002.666 123 2.613983327282741\n",
      "Training loss: 10527494107736.746 124 2.6138105764736705\n",
      "Training loss: 10527492765559.467 125 2.6136379597309847\n",
      "Training loss: 10527491199685.973 126 2.613465589733248\n",
      "Training loss: 10527490304901.12 127 2.6132933336754998\n",
      "Training loss: 10527487620546.56 128 2.613121148997589\n",
      "Training loss: 10527487396850.346 129 2.6129490978961303\n",
      "Training loss: 10527484265103.36 130 2.612777295009101\n",
      "Training loss: 10527483594014.72 131 2.6126056390589585\n",
      "Training loss: 10527482251837.44 132 2.612434012348243\n",
      "Training loss: 10527480462267.732 133 2.6122625005357416\n",
      "Training loss: 10527478896394.24 134 2.612091244063477\n",
      "Training loss: 10527477554216.96 135 2.611920193106445\n",
      "Training loss: 10527475988343.467 136 2.611749200269262\n",
      "Training loss: 10527473303988.906 137 2.611578200810406\n",
      "Training loss: 10527473080292.693 138 2.6114076321688633\n",
      "Training loss: 10527471290722.986 139 2.6112370196529455\n",
      "Training loss: 10527469948545.707 140 2.6110665176599026\n",
      "Training loss: 10527468606368.426 141 2.610896309909829\n",
      "Training loss: 10527466369406.293 142 2.6107262626377543\n",
      "Training loss: 10527465474621.44 143 2.610556191279053\n",
      "Training loss: 10527463461355.52 144 2.6103862679067\n",
      "Training loss: 10527463013963.094 145 2.61021665577629\n",
      "Training loss: 10527460777000.96 146 2.6100471072332403\n",
      "Training loss: 10527458540038.826 147 2.6098776597901625\n",
      "Training loss: 10527457868950.188 148 2.6097083702002353\n",
      "Training loss: 10527456750469.12 149 2.6095392628358947\n",
      "Training loss: 10527454289810.773 150 2.6093702142196036\n",
      "Training loss: 10527453842418.346 151 2.609201425064379\n",
      "Training loss: 10527452052848.64 152 2.6090326955304763\n",
      "Training loss: 10527449815886.506 153 2.608864092768797\n",
      "Training loss: 10527449144797.867 154 2.608695627933673\n",
      "Training loss: 10527447355228.16 155 2.6085274117686557\n",
      "Training loss: 10527445789354.666 156 2.608359174553188\n",
      "Training loss: 10527443999784.96 157 2.608191210305175\n",
      "Training loss: 10527442433911.467 158 2.6080233197288893\n",
      "Training loss: 10527441091734.188 159 2.6078555801971843\n",
      "Training loss: 10527440196949.334 160 2.607687942360251\n",
      "Training loss: 10527438183683.414 161 2.6075205586639356\n",
      "Training loss: 10527436617809.92 162 2.607353204950863\n",
      "Training loss: 10527435499328.854 163 2.607186032984234\n",
      "Training loss: 10527433038670.506 164 2.6070190178269512\n",
      "Training loss: 10527432367581.867 165 2.6068520869859184\n",
      "Training loss: 10527429906923.52 166 2.606685338207041\n",
      "Training loss: 10527428564746.24 167 2.6065187185077705\n",
      "Training loss: 10527427669961.387 168 2.606352226115864\n",
      "Training loss: 10527425880391.68 169 2.60618589309537\n",
      "Training loss: 10527424314518.188 170 2.6060196893334684\n",
      "Training loss: 10527422972340.906 171 2.6058536250860813\n",
      "Training loss: 10527420511682.56 172 2.6056876977021584\n",
      "Training loss: 10527419616897.707 173 2.605521871873458\n",
      "Training loss: 10527417827328.0 174 2.6053561480489686\n",
      "Training loss: 10527416708846.934 175 2.60519056983517\n",
      "Training loss: 10527415366669.654 176 2.6050251549435806\n",
      "Training loss: 10527414024492.373 177 2.604859922003824\n",
      "Training loss: 10527412458618.88 178 2.604694728767993\n",
      "Training loss: 10527410892745.387 179 2.6045296334265076\n",
      "Training loss: 10527409550568.107 180 2.6043647746687872\n",
      "Training loss: 10527407313605.973 181 2.604200055132224\n",
      "Training loss: 10527405300340.053 182 2.604035363633214\n",
      "Training loss: 10527404405555.2 183 2.603870902163852\n",
      "Training loss: 10527403510770.346 184 2.603706607095377\n",
      "Training loss: 10527401721200.64 185 2.603542296428799\n",
      "Training loss: 10527400155327.146 186 2.60337821567117\n",
      "Training loss: 10527398142061.227 187 2.6032142988640428\n",
      "Training loss: 10527396128795.307 188 2.603050458899441\n",
      "Training loss: 10527395234010.453 189 2.6028867467780494\n",
      "Training loss: 10527393668136.96 190 2.6027231625535574\n",
      "Training loss: 10527392549655.893 191 2.602559755512368\n",
      "Training loss: 10527390983782.4 192 2.6023964200582426\n",
      "Training loss: 10527389417908.906 193 2.6022331984428595\n",
      "Training loss: 10527387628339.2 194 2.6020702212624154\n",
      "Training loss: 10527386957250.56 195 2.6019073650637368\n",
      "Training loss: 10527385167680.854 196 2.6017445084914352\n",
      "Training loss: 10527382483326.293 197 2.601581739257415\n",
      "Training loss: 10527381812237.654 198 2.6014192597783157\n",
      "Training loss: 10527380022667.947 199 2.601256836024743\n",
      "Training loss: 10527378680490.666 200 2.6010943817205994\n",
      "Training loss: 10527377114617.174 201 2.600932188873834\n",
      "Training loss: 10527375101351.254 202 2.6007701845021503\n",
      "Training loss: 10527374206566.4 203 2.6006082035871496\n",
      "Training loss: 10527372416996.693 204 2.600446355499305\n",
      "Training loss: 10527371074819.414 205 2.6002847118966868\n",
      "Training loss: 10527369061553.494 206 2.6001231426424525\n",
      "Training loss: 10527367943072.426 207 2.5999617052604775\n",
      "Training loss: 10527365706110.293 208 2.5998003548981674\n",
      "Training loss: 10527364811325.44 209 2.5996390809960137\n",
      "Training loss: 10527363692844.373 210 2.599478068199423\n",
      "Training loss: 10527361679578.453 211 2.5993169816390678\n",
      "Training loss: 10527359890008.746 212 2.599156037030664\n",
      "Training loss: 10527357876742.826 213 2.5989952980349478\n",
      "Training loss: 10527356310869.334 214 2.5988347722579426\n",
      "Training loss: 10527355192388.268 215 2.598674255965284\n",
      "Training loss: 10527353850210.986 216 2.5985137678464683\n",
      "Training loss: 10527351836945.066 217 2.598353521830267\n",
      "Training loss: 10527350942160.213 218 2.5981933563808606\n",
      "Training loss: 10527348705198.08 219 2.5980332023642045\n",
      "Training loss: 10527348034109.44 220 2.5978731963550468\n",
      "Training loss: 10527346244539.732 221 2.597713373874164\n",
      "Training loss: 10527344231273.812 222 2.5975536118780944\n",
      "Training loss: 10527342665400.32 223 2.5973939752927557\n",
      "Training loss: 10527340652134.4 224 2.5972344435501484\n",
      "Training loss: 10527339309957.12 225 2.5970752609097705\n",
      "Training loss: 10527336849298.773 226 2.59691623496291\n",
      "Training loss: 10527334836032.854 227 2.596757630424967\n",
      "Training loss: 10527331033197.227 228 2.5966007296225744\n",
      "Training loss: 10527301728993.28 229 2.5965206684129045\n",
      "Training loss: 10527386733554.346 230 2.596391841080031\n",
      "Training loss: 10527421853859.84 231 2.596249052683957\n",
      "Training loss: 10527440420645.547 232 2.596099078558005\n",
      "Training loss: 10527439973253.12 233 2.5959466684100154\n",
      "Training loss: 10527423867125.76 234 2.595794651207298\n",
      "Training loss: 10527398365757.44 235 2.5956445194548206\n",
      "Training loss: 10527371745908.053 236 2.595496684965609\n",
      "Training loss: 10527348928894.293 237 2.5953508559006924\n",
      "Training loss: 10527332151678.293 238 2.5952061000544315\n",
      "Training loss: 10527324546007.04 239 2.59506149574695\n",
      "Training loss: 10527324098614.613 240 2.5949163104571764\n",
      "Training loss: 10527327901450.24 241 2.5947702160370216\n",
      "Training loss: 10527333941248.0 242 2.59462299089263\n",
      "Training loss: 10527339086260.906 243 2.5944750048536287\n",
      "Training loss: 10527341546919.254 244 2.594327352623948\n",
      "Training loss: 10527342889096.533 245 2.594178637189909\n",
      "Training loss: 10527340875830.613 246 2.594030108657172\n",
      "Training loss: 10527336625602.56 247 2.5938820759331858\n",
      "Training loss: 10527330809501.014 248 2.5937346487552038\n",
      "Training loss: 10527324993399.467 249 2.5935878178901737\n",
      "Training loss: 10527320743171.414 250 2.593441212477746\n",
      "Training loss: 10527317611424.426 251 2.5932948972478904\n",
      "Training loss: 10527314703373.654 252 2.5931486515176663\n",
      "Training loss: 10527314703373.654 253 2.5930025051445478\n",
      "Training loss: 10527312913803.947 254 2.5928563091414545\n",
      "Training loss: 10527312242715.307 255 2.5927099582732986\n",
      "Training loss: 10527311347930.453 256 2.5925636721653453\n",
      "Training loss: 10527310005753.174 257 2.592417551475554\n",
      "Training loss: 10527308216183.467 258 2.592271549555346\n",
      "Training loss: 10527306202917.547 259 2.592125452781297\n",
      "Training loss: 10527303742259.2 260 2.591979555954675\n",
      "Training loss: 10527301505297.066 261 2.5918339669490327\n",
      "Training loss: 10527299268334.934 262 2.5916883706108815\n",
      "Training loss: 10527297478765.227 263 2.5915429221612563\n",
      "Training loss: 10527295241803.094 264 2.5913977415525054\n",
      "Training loss: 10527293675929.6 265 2.5912525427351754\n",
      "Training loss: 10527291215271.254 266 2.591107401387146\n",
      "Training loss: 10527289202005.334 267 2.590962602922413\n",
      "Training loss: 10527287188739.414 268 2.590818109326633\n",
      "Training loss: 10527284951777.28 269 2.5906740880607106\n",
      "Training loss: 10527281148941.654 270 2.5905342502459967\n",
      "Training loss: 10527164603214.506 271 2.5904821420227417\n",
      "Training loss: 10527318729905.494 272 2.590374445828715\n",
      "Training loss: 10527379575275.52 273 2.5902473036616365\n",
      "Training loss: 10527409774264.32 274 2.590110456210676\n",
      "Training loss: 10527411787530.24 275 2.58996997669748\n",
      "Training loss: 10527393668136.96 276 2.5898301839179902\n",
      "Training loss: 10527361903274.666 277 2.5896932103231465\n",
      "Training loss: 10527327006665.387 278 2.589559718108316\n",
      "Training loss: 10527296583980.373 279 2.5894288630938687\n",
      "Training loss: 10527276675017.387 280 2.5892996476059316\n",
      "Training loss: 10527266161295.36 281 2.5891707274352784\n",
      "Training loss: 10527266608687.787 282 2.5890410345164963\n",
      "Training loss: 10527273095877.973 283 2.5889098666857118\n",
      "Training loss: 10527282491118.934 284 2.5887772417798525\n",
      "Training loss: 10527289425701.547 285 2.588643328451649\n",
      "Training loss: 10527293899625.812 286 2.5885087185399174\n",
      "Training loss: 10527294347018.24 287 2.5883739684867084\n",
      "Training loss: 10527290991575.04 288 2.5882394709833925\n",
      "Training loss: 10527285846562.133 289 2.5881055757216918\n",
      "Training loss: 10527278688283.307 290 2.5879725084819207\n",
      "Training loss: 10527273095877.973 291 2.587840001605153\n",
      "Training loss: 10527267056080.213 292 2.5877078762552146\n",
      "Training loss: 10527263700637.014 293 2.587576091434824\n",
      "Training loss: 10527262134763.52 294 2.5874442789794676\n",
      "Training loss: 10527261016282.453 295 2.5873125331169704\n",
      "Training loss: 10527260121497.6 296 2.5871805767089397\n",
      "Training loss: 10527259003016.533 297 2.5870486666671453\n",
      "Training loss: 10527258555624.107 298 2.586916753911207\n",
      "Training loss: 10527256542358.188 299 2.5867851724863695\n",
      "Training loss: 10527254081699.84 300 2.5866536998844682\n",
      "Training loss: 10527251397345.28 301 2.586522593990138\n",
      "Training loss: 10527246923421.014 302 2.5863920590746483\n",
      "Training loss: 10527242002104.32 303 2.5862626856791455\n",
      "Training loss: 10527228580331.52 304 2.5861396913905654\n",
      "Training loss: 10527167511265.28 305 2.5861963719140877\n",
      "Training loss: 10527416261454.506 306 2.5861169754627134\n",
      "Training loss: 10527494107736.746 307 2.5860038758469193\n",
      "Training loss: 10527529899130.88 308 2.585874908736991\n",
      "Training loss: 10527520503889.92 309 2.5857416745736415\n",
      "Training loss: 10527479567482.88 310 2.5856115062831244\n",
      "Training loss: 10527417379935.574 311 2.585487981445704\n",
      "Training loss: 10527354297603.414 312 2.585371425584068\n",
      "Training loss: 10527301728993.28 313 2.5852601555852455\n",
      "Training loss: 10527267056080.213 314 2.5851515159917766\n",
      "Training loss: 10527253634307.414 315 2.5850427211557716\n",
      "Training loss: 10527257213446.826 316 2.5849319420989723\n",
      "Training loss: 10527271530004.48 317 2.5848181820604306\n",
      "Training loss: 10527288754612.906 318 2.5847013910535526\n",
      "Training loss: 10527301505297.066 319 2.584582359193689\n",
      "Training loss: 10527307992487.254 320 2.584462228055306\n",
      "Training loss: 10527306650309.973 321 2.5843428347714346\n",
      "Training loss: 10527302400081.92 322 2.5842231627241317\n",
      "Training loss: 10527292333752.32 323 2.584104681006144\n",
      "Training loss: 10527280254156.8 324 2.5839875489604722\n",
      "Training loss: 10527270187827.2 325 2.5838713511338236\n",
      "Training loss: 10527263029548.373 326 2.583755892083823\n",
      "Training loss: 10527257660839.254 327 2.5836406847727087\n",
      "Training loss: 10527255647573.334 328 2.5835253447537845\n",
      "Training loss: 10527255423877.12 329 2.5834097800239744\n",
      "Training loss: 10527256318661.973 330 2.583294044056928\n",
      "Training loss: 10527256766054.4 331 2.5831779565292283\n",
      "Training loss: 10527255647573.334 332 2.58306160488242\n",
      "Training loss: 10527254529092.268 333 2.5829454881166245\n",
      "Training loss: 10527251173649.066 334 2.5828296200919456\n",
      "Training loss: 10527248265598.293 335 2.582713894996995\n",
      "Training loss: 10527245357547.52 336 2.582598401693492\n",
      "Training loss: 10527241331015.68 337 2.582483290833271\n",
      "Training loss: 10527238646661.12 338 2.582368284194642\n",
      "Training loss: 10527236186002.773 339 2.582253478228003\n",
      "Training loss: 10527233725344.426 340 2.5821388383194686\n",
      "Training loss: 10527231488382.293 341 2.5820242573379755\n",
      "Training loss: 10527230369901.227 342 2.5819096127745182\n",
      "Training loss: 10527227909242.88 343 2.5817951486806354\n",
      "Training loss: 10527226119673.174 344 2.581680687231376\n",
      "Training loss: 10527224553799.68 345 2.581566311621776\n",
      "Training loss: 10527222987926.188 346 2.581452119222979\n",
      "Training loss: 10527220303571.627 347 2.581338077474296\n",
      "Training loss: 10527218066609.494 348 2.5812241063576042\n",
      "Training loss: 10527216500736.0 349 2.5811102482165933\n",
      "Training loss: 10527214040077.654 350 2.5809966263403363\n",
      "Training loss: 10527212921596.586 351 2.5808830338856166\n",
      "Training loss: 10527210013545.812 352 2.5807695440437324\n",
      "Training loss: 10527208447672.32 353 2.580656213192175\n",
      "Training loss: 10527207776583.68 354 2.5805430104537685\n",
      "Training loss: 10527205539621.547 355 2.5804298995283026\n",
      "Training loss: 10527203526355.627 356 2.5803168044231244\n",
      "Training loss: 10527201513089.707 357 2.5802038937527003\n",
      "Training loss: 10527199723520.0 358 2.5800909847028377\n",
      "Training loss: 10527197710254.08 359 2.5799783179142532\n",
      "Training loss: 10527196368076.8 360 2.5798657921515153\n",
      "Training loss: 10527193907418.453 361 2.579753256762794\n",
      "Training loss: 10527193460026.027 362 2.579640825029428\n",
      "Training loss: 10527189880886.613 363 2.5795285753531347\n",
      "Training loss: 10527188986101.76 364 2.5794163755168324\n",
      "Training loss: 10527186749139.627 365 2.5793043149350092\n",
      "Training loss: 10527185406962.346 366 2.5791924415767946\n",
      "Training loss: 10527183617392.64 367 2.579080572121268\n",
      "Training loss: 10527182051519.146 368 2.5789687788769524\n",
      "Training loss: 10527180261949.44 369 2.578857137724534\n",
      "Training loss: 10527178919772.16 370 2.578745648848573\n",
      "Training loss: 10527177353898.666 371 2.5786342344069064\n",
      "Training loss: 10527174893240.32 372 2.578522871492116\n",
      "Training loss: 10527173998455.467 373 2.578411614430019\n",
      "Training loss: 10527171761493.334 374 2.578300540016628\n",
      "Training loss: 10527169748227.414 375 2.578189525879668\n",
      "Training loss: 10527167958657.707 376 2.5780786455294193\n",
      "Training loss: 10527166392784.213 377 2.5779679002228155\n",
      "Training loss: 10527165050606.934 378 2.5778571389874267\n",
      "Training loss: 10527163708429.654 379 2.577746500870771\n",
      "Training loss: 10527161918859.947 380 2.5776360599663337\n",
      "Training loss: 10527159681897.812 381 2.5775257253428867\n",
      "Training loss: 10527158563416.746 382 2.5774153725319637\n",
      "Training loss: 10527156326454.613 383 2.577305154727403\n",
      "Training loss: 10527154984277.334 384 2.577195116892501\n",
      "Training loss: 10527153194707.627 385 2.577085158089643\n",
      "Training loss: 10527152523618.986 386 2.5769752362445217\n",
      "Training loss: 10527150510353.066 387 2.5768654483560236\n",
      "Training loss: 10527148497087.146 388 2.576755806708299\n",
      "Training loss: 10527147378606.08 389 2.576646201203496\n",
      "Training loss: 10527145589036.373 390 2.5765367548605984\n",
      "Training loss: 10527144246859.094 391 2.5764272851079455\n",
      "Training loss: 10527142009896.96 392 2.576318011236638\n",
      "Training loss: 10527140444023.467 393 2.5762089805895614\n",
      "Training loss: 10527139101846.188 394 2.5760998130473185\n",
      "Training loss: 10527137535972.693 395 2.5759907400357753\n",
      "Training loss: 10527136193795.414 396 2.5758819041849064\n",
      "Training loss: 10527134627921.92 397 2.575773111581293\n",
      "Training loss: 10527132838352.213 398 2.5756643728039768\n",
      "Training loss: 10527131048782.506 399 2.575555835229797\n",
      "Training loss: 10527128811820.373 400 2.575447336455279\n",
      "Training loss: 10527127693339.307 401 2.575338892248723\n",
      "Training loss: 10527125903769.6 402 2.575230614910572\n",
      "Training loss: 10527125008984.746 403 2.575122403038703\n",
      "Training loss: 10527122995718.826 404 2.5750143035835125\n",
      "Training loss: 10527122324630.188 405 2.574906304098005\n",
      "Training loss: 10527120311364.268 406 2.5747984321112756\n",
      "Training loss: 10527118745490.773 407 2.5746905618518703\n",
      "Training loss: 10527116284832.426 408 2.5745828163935487\n",
      "Training loss: 10527115166351.36 409 2.5744752130400306\n",
      "Training loss: 10527113600477.867 410 2.574367725590568\n",
      "Training loss: 10527112481996.8 411 2.5742602886914887\n",
      "Training loss: 10527111587211.947 412 2.5741529139787365\n",
      "Training loss: 10527108679161.174 413 2.5740456260357356\n",
      "Training loss: 10527107784376.32 414 2.573938551402217\n",
      "Training loss: 10527105994806.613 415 2.573831550117946\n",
      "Training loss: 10527105323717.973 416 2.5737245159425712\n",
      "Training loss: 10527103086755.84 417 2.573617601976072\n",
      "Training loss: 10527101968274.773 418 2.5735109102311617\n",
      "Training loss: 10527100178705.066 419 2.573404165159739\n",
      "Training loss: 10527098389135.36 420 2.573297562629291\n",
      "Training loss: 10527097494350.506 421 2.573191183051124\n",
      "Training loss: 10527095481084.586 422 2.57308474412132\n",
      "Training loss: 10527094586299.732 423 2.572978393016767\n",
      "Training loss: 10527093020426.24 424 2.5728722010373235\n",
      "Training loss: 10527091230856.533 425 2.572766085779281\n",
      "Training loss: 10527089217590.613 426 2.572660116002362\n",
      "Training loss: 10527088322805.76 427 2.572554121442698\n",
      "Training loss: 10527086085843.627 428 2.5724483149616\n",
      "Training loss: 10527084967362.56 429 2.5723425879296364\n",
      "Training loss: 10527082954096.64 430 2.57223689546372\n",
      "Training loss: 10527081835615.574 431 2.572131313927126\n",
      "Training loss: 10527081388223.146 432 2.5720258599537273\n",
      "Training loss: 10527078480172.373 433 2.571920470220351\n",
      "Training loss: 10527077137995.094 434 2.5718152260192824\n",
      "Training loss: 10527075348425.387 435 2.5717100358127487\n",
      "Training loss: 10527074901032.96 436 2.5716048738178108\n",
      "Training loss: 10527073111463.254 437 2.571499857353443\n",
      "Training loss: 10527070874501.12 438 2.5713948863994958\n",
      "Training loss: 10527069756020.053 439 2.5712901852722188\n",
      "Training loss: 10527068637538.986 440 2.571185367307655\n",
      "Training loss: 10527067295361.707 441 2.571080713356228\n",
      "Training loss: 10527065058399.574 442 2.57097613627439\n",
      "Training loss: 10527063716222.293 443 2.570871632983107\n",
      "Training loss: 10527062597741.227 444 2.570767230067275\n",
      "Training loss: 10527061255563.947 445 2.5706629794470635\n",
      "Training loss: 10527059913386.666 446 2.5705588000884476\n",
      "Training loss: 10527057452728.32 447 2.5704546045687713\n",
      "Training loss: 10527056557943.467 448 2.5703505404726035\n",
      "Training loss: 10527055663158.613 449 2.5702466592737627\n",
      "Training loss: 10527053649892.693 450 2.5701429043126973\n",
      "Training loss: 10527052531411.627 451 2.570039007843217\n",
      "Training loss: 10527050294449.494 452 2.569935289073201\n",
      "Training loss: 10527048504879.787 453 2.569831777583676\n",
      "Training loss: 10527047162702.506 454 2.569728408813116\n",
      "Training loss: 10527047162702.506 455 2.569624876629949\n",
      "Training loss: 10527044925740.373 456 2.5695215018316424\n",
      "Training loss: 10527044030955.52 457 2.569418272745852\n",
      "Training loss: 10527041346600.96 458 2.569315170651282\n",
      "Training loss: 10527040451816.107 459 2.569212147825312\n",
      "Training loss: 10527038885942.613 460 2.5691091728910846\n",
      "Training loss: 10527037096372.906 461 2.5690061889356475\n",
      "Training loss: 10527035977891.84 462 2.568903438020956\n",
      "Training loss: 10527034859410.773 463 2.568800816659755\n",
      "Training loss: 10527033517233.494 464 2.568698118884647\n",
      "Training loss: 10527031951360.0 465 2.568595491452521\n",
      "Training loss: 10527029938094.08 466 2.56849301209532\n",
      "Training loss: 10527028372220.586 467 2.568390772180647\n",
      "Training loss: 10527027477435.732 468 2.5682885888044633\n",
      "Training loss: 10527025464169.812 469 2.5681862614810944\n",
      "Training loss: 10527024793081.174 470 2.5680840951741297\n",
      "Training loss: 10527022332422.826 471 2.5679820874062456\n",
      "Training loss: 10527020990245.547 472 2.567880199182888\n",
      "Training loss: 10527019648068.268 473 2.567778433709212\n",
      "Training loss: 10527018529587.2 474 2.5676766063121703\n",
      "Training loss: 10527017411106.133 475 2.567574900669127\n",
      "Training loss: 10527015174144.0 476 2.5674732724913807\n",
      "Training loss: 10527014055662.934 477 2.5673718247862873\n",
      "Training loss: 10527012713485.654 478 2.5672703872862783\n",
      "Training loss: 10527011147612.16 479 2.5671690345546563\n",
      "Training loss: 10527009805434.88 480 2.567067733598384\n",
      "Training loss: 10527008239561.387 481 2.566966552544161\n",
      "Training loss: 10527007344776.533 482 2.566865441686932\n",
      "Training loss: 10527005107814.4 483 2.566764420115434\n",
      "Training loss: 10527003765637.12 484 2.566663528475349\n",
      "Training loss: 10527003094548.48 485 2.566562671243473\n",
      "Training loss: 10527000857586.346 486 2.566461903519362\n",
      "Training loss: 10526999739105.28 487 2.5663612157874525\n",
      "Training loss: 10526998396928.0 488 2.5662606463200355\n",
      "Training loss: 10526996607358.293 489 2.56616013958118\n",
      "Training loss: 10526995265181.014 490 2.566059677018757\n",
      "Training loss: 10526994594092.373 491 2.5659593604369793\n",
      "Training loss: 10526992133434.027 492 2.565859070145048\n",
      "Training loss: 10526990791256.746 493 2.565758984610151\n",
      "Training loss: 10526989672775.68 494 2.5656588845876502\n",
      "Training loss: 10526987435813.547 495 2.5655587937592403\n",
      "Training loss: 10526986093636.268 496 2.56545888878125\n",
      "Training loss: 10526984751458.986 497 2.5653590325343134\n",
      "Training loss: 10526983185585.494 498 2.56525931354214\n",
      "Training loss: 10526982514496.854 499 2.5651596551368567\n",
      "Training loss: 10526980948623.36 500 2.565060038885003\n",
      "Training loss: 10526979159053.654 501 2.5649603827605563\n",
      "Training loss: 10526978487965.014 502 2.564861093932239\n",
      "Training loss: 10526976474699.094 503 2.564761692748673\n",
      "Training loss: 10526975356218.027 504 2.5646623879872625\n",
      "Training loss: 10526973566648.32 505 2.5645632187789835\n",
      "Training loss: 10526972000774.826 506 2.5644640536602603\n",
      "Training loss: 10526970882293.76 507 2.5643649986554786\n",
      "Training loss: 10526969540116.48 508 2.564266085591359\n",
      "Training loss: 10526968197939.2 509 2.564167277732678\n",
      "Training loss: 10526966632065.707 510 2.564068394534347\n",
      "Training loss: 10526965737280.854 511 2.5639696857946324\n",
      "Training loss: 10526963947711.146 512 2.563871069056709\n",
      "Training loss: 10526961934445.227 513 2.5637725432543665\n",
      "Training loss: 10526960592267.947 514 2.563674019365879\n",
      "Training loss: 10526957907913.387 515 2.563575622904799\n",
      "Training loss: 10526958131609.6 516 2.5634773616640594\n",
      "Training loss: 10526956342039.893 517 2.5633790832926717\n",
      "Training loss: 10526955670951.254 518 2.5632809386020083\n",
      "Training loss: 10526953433989.12 519 2.563182847172304\n",
      "Training loss: 10526952091811.84 520 2.563084884894595\n",
      "Training loss: 10526950749634.56 521 2.5629870811545037\n",
      "Training loss: 10526949631153.494 522 2.5628892539419694\n",
      "Training loss: 10526948065280.0 523 2.5627913611631943\n",
      "Training loss: 10526945828317.867 524 2.5626936335119797\n",
      "Training loss: 10526944486140.586 525 2.562596107949872\n",
      "Training loss: 10526943367659.52 526 2.562498786238271\n",
      "Training loss: 10526941130697.387 527 2.5624011499758543\n",
      "Training loss: 10526940459608.746 528 2.562303766086545\n",
      "Training loss: 10526938893735.254 529 2.5622065482692657\n",
      "Training loss: 10526937327861.76 530 2.562109321156013\n",
      "Training loss: 10526935985684.48 531 2.5620122151146396\n",
      "Training loss: 10526934196114.773 532 2.561915183654423\n",
      "Training loss: 10526933972418.56 533 2.5618181891372567\n",
      "Training loss: 10526931288064.0 534 2.561721296754154\n",
      "Training loss: 10526930393279.146 535 2.5616244783346223\n",
      "Training loss: 10526929051101.867 536 2.561527697597374\n",
      "Training loss: 10526927485228.373 537 2.561431041279415\n",
      "Training loss: 10526927037835.947 538 2.5613345034567483\n",
      "Training loss: 10526925024570.027 539 2.561237938948965\n",
      "Training loss: 10526923682392.746 540 2.5611414910286077\n",
      "Training loss: 10526922116519.254 541 2.5610451679781514\n",
      "Training loss: 10526921221734.4 542 2.5609489659499047\n",
      "Training loss: 10526919208468.48 543 2.560852830670085\n",
      "Training loss: 10526916971506.346 544 2.5607565708061486\n",
      "Training loss: 10526916076721.494 545 2.560660451554115\n",
      "Training loss: 10526914734544.213 546 2.5605645872585274\n",
      "Training loss: 10526913392366.934 547 2.5604688428536764\n",
      "Training loss: 10526912273885.867 548 2.5603729295530924\n",
      "Training loss: 10526910708012.373 549 2.560277119055435\n",
      "Training loss: 10526909365835.094 550 2.560181447768302\n",
      "Training loss: 10526907352569.174 551 2.5600858704412386\n",
      "Training loss: 10526907128872.96 552 2.559990391430436\n",
      "Training loss: 10526904891910.826 553 2.55989503835982\n",
      "Training loss: 10526903326037.334 554 2.559799572984358\n",
      "Training loss: 10526903326037.334 555 2.5597042055010877\n",
      "Training loss: 10526901089075.2 556 2.5596090881791054\n",
      "Training loss: 10526899075809.28 557 2.5595140626509707\n",
      "Training loss: 10526897733632.0 558 2.5594188739696966\n",
      "Training loss: 10526896167758.506 559 2.5593238703893175\n",
      "Training loss: 10526895049277.44 560 2.5592289665991323\n",
      "Training loss: 10526893036011.52 561 2.5591342450075123\n",
      "Training loss: 10526892364922.88 562 2.559039522097868\n",
      "Training loss: 10526891470138.027 563 2.5589447059119315\n",
      "Training loss: 10526889904264.533 564 2.5588500197573874\n",
      "Training loss: 10526887667302.4 565 2.5587555952046923\n",
      "Training loss: 10526886996213.76 566 2.5586613127361675\n",
      "Training loss: 10526885654036.48 567 2.5585668004108477\n",
      "Training loss: 10526884088162.986 568 2.558472362015321\n",
      "Training loss: 10526882745985.707 569 2.5583781718742573\n",
      "Training loss: 10526880732719.787 570 2.558284111111868\n",
      "Training loss: 10526879837934.934 571 2.558189933381285\n",
      "Training loss: 10526877377276.586 572 2.5580959119540676\n",
      "Training loss: 10526876929884.16 573 2.5580020441896565\n",
      "Training loss: 10526875140314.453 574 2.557908141150848\n",
      "Training loss: 10526873574440.96 575 2.5578143265707456\n",
      "Training loss: 10526872679656.107 576 2.5577206362068536\n",
      "Training loss: 10526907352569.174 577 2.557627715377955\n",
      "Training loss: 10526860152668.16 578 2.5575354976743494\n",
      "Training loss: 10526864179200.0 579 2.557442917828148\n",
      "Training loss: 10526866416162.133 580 2.5573489124369195\n",
      "Training loss: 10526869100516.693 581 2.5572548935796293\n",
      "Training loss: 10526869100516.693 582 2.5571609760025154\n",
      "Training loss: 10526868205731.84 583 2.5570667807345226\n",
      "Training loss: 10526864626592.426 584 2.5569731628411922\n",
      "Training loss: 10526862613326.506 585 2.5568798593246442\n",
      "Training loss: 10526859257883.307 586 2.556786869254543\n",
      "Training loss: 10526855902440.107 587 2.5566941906807874\n",
      "Training loss: 10526853889174.188 588 2.5566016237735685\n",
      "Training loss: 10526851875908.268 589 2.5565090855182504\n",
      "Training loss: 10526850533730.986 590 2.556416601787668\n",
      "Training loss: 10526849638946.133 591 2.556324184498717\n",
      "Training loss: 10526849638946.133 592 2.5562316492716706\n",
      "Training loss: 10526848296768.854 593 2.5561390694492454\n",
      "Training loss: 10526846507199.146 594 2.556046396816779\n",
      "Training loss: 10526846730895.36 595 2.5559539306896286\n",
      "Training loss: 10526844717629.44 596 2.555861765578379\n",
      "Training loss: 10526842928059.732 597 2.555769397320499\n",
      "Training loss: 10526841362186.24 598 2.555677130965745\n",
      "Training loss: 10526840691097.6 599 2.5555851767087567\n",
      "Training loss: 10526838901527.893 600 2.555493247934472\n",
      "Training loss: 10526837111958.188 601 2.5554013390288794\n",
      "Training loss: 10526835322388.48 602 2.5553095527001326\n",
      "Training loss: 10526834427603.627 603 2.555217730099884\n",
      "Training loss: 10526833309122.56 604 2.5551259877372647\n",
      "Training loss: 10526831072160.426 605 2.5550346011959317\n",
      "Training loss: 10526830177375.574 606 2.554942885077408\n",
      "Training loss: 10526828835198.293 607 2.554851259730289\n",
      "Training loss: 10526827493021.014 608 2.5547598306504047\n",
      "Training loss: 10526826150843.732 609 2.5546685186977944\n",
      "Training loss: 10526824361274.027 610 2.5545773661976328\n",
      "Training loss: 10526824361274.027 611 2.554486088959134\n",
      "Training loss: 10526821900615.68 612 2.554394828869721\n",
      "Training loss: 10526820111045.973 613 2.554303861005776\n",
      "Training loss: 10526819439957.334 614 2.554212852652264\n",
      "Training loss: 10526818321476.268 615 2.554121819996412\n",
      "Training loss: 10526815637121.707 616 2.5540310373380812\n",
      "Training loss: 10526814071248.213 617 2.553940348951167\n",
      "Training loss: 10526812952767.146 618 2.5538495398486956\n",
      "Training loss: 10526811610589.867 619 2.553758819915956\n",
      "Training loss: 10526810715805.014 620 2.553668313807646\n",
      "Training loss: 10526808926235.307 621 2.5535778203998794\n",
      "Training loss: 10526807360361.812 622 2.5534873961592717\n",
      "Training loss: 10526806465576.96 623 2.553396981006348\n",
      "Training loss: 10526804899703.467 624 2.553306678475632\n",
      "Training loss: 10526803557526.188 625 2.553216390363372\n",
      "Training loss: 10526801767956.48 626 2.553126235347524\n",
      "Training loss: 10526800649475.414 627 2.5530361752903037\n",
      "Training loss: 10526798412513.28 628 2.552946165546212\n",
      "Training loss: 10526797741424.64 629 2.5528561680674575\n",
      "Training loss: 10526796622943.574 630 2.552766260984877\n",
      "Training loss: 10526795280766.293 631 2.5526764359744663\n",
      "Training loss: 10526793938589.014 632 2.5525867124222166\n",
      "Training loss: 10526791701626.88 633 2.552496994376377\n",
      "Training loss: 10526790583145.812 634 2.552407344805778\n",
      "Training loss: 10526789240968.533 635 2.5523177818481284\n",
      "Training loss: 10526788569879.893 636 2.5522283472674037\n",
      "Training loss: 10526786109221.547 637 2.552138903342643\n",
      "Training loss: 10526784990740.48 638 2.5520495582907246\n",
      "Training loss: 10526783648563.2 639 2.55196028543532\n",
      "Training loss: 10526781858993.494 640 2.5518710319508\n",
      "Training loss: 10526780964208.64 641 2.5517818977644584\n",
      "Training loss: 10526778950942.72 642 2.5516928900984834\n",
      "Training loss: 10526778503550.293 643 2.5516038621982626\n",
      "Training loss: 10526776042891.947 644 2.551514835874649\n",
      "Training loss: 10526774700714.666 645 2.5514259632962544\n",
      "Training loss: 10526773582233.6 646 2.551337270886942\n",
      "Training loss: 10526773582233.6 647 2.5512484723244433\n",
      "Training loss: 10526770450486.613 648 2.5511596760404065\n",
      "Training loss: 10526769332005.547 649 2.551071199217943\n",
      "Training loss: 10526767989828.268 650 2.5509826068419748\n",
      "Training loss: 10526766871347.2 651 2.5508940766040125\n",
      "Training loss: 10526765752866.133 652 2.5508057130528754\n",
      "Training loss: 10526763515904.0 653 2.5507173676408654\n",
      "Training loss: 10526762844815.36 654 2.5506291446466878\n",
      "Training loss: 10526761055245.654 655 2.5505409184607393\n",
      "Training loss: 10526759489372.16 656 2.5504527124256686\n",
      "Training loss: 10526758594587.307 657 2.55036453510188\n",
      "Training loss: 10526757476106.24 658 2.55027668892635\n",
      "Training loss: 10526756581321.387 659 2.5501888437297455\n",
      "Training loss: 10526754344359.254 660 2.5501007950943\n",
      "Training loss: 10526752778485.76 661 2.550012996268841\n",
      "Training loss: 10526751883700.906 662 2.5499252971196045\n",
      "Training loss: 10526749646738.773 663 2.5498375657650043\n",
      "Training loss: 10526748975650.133 664 2.549749938803568\n",
      "Training loss: 10526747186080.426 665 2.5496624897874667\n",
      "Training loss: 10526745620206.934 666 2.5495749386090796\n",
      "Training loss: 10526744278029.654 667 2.549487520529663\n",
      "Training loss: 10526742935852.373 668 2.5494001056318636\n",
      "Training loss: 10526741593675.094 669 2.5493128684535358\n",
      "Training loss: 10526741146282.666 670 2.549225675950662\n",
      "Training loss: 10526738461928.107 671 2.5491385912864666\n",
      "Training loss: 10526737343447.04 672 2.5490513176203144\n",
      "Training loss: 10526736448662.188 673 2.548964319270299\n",
      "Training loss: 10526734435396.268 674 2.5488774663336384\n",
      "Training loss: 10526733316915.2 675 2.548790480418043\n",
      "Training loss: 10526731527345.494 676 2.5487035893919083\n",
      "Training loss: 10526730632560.64 677 2.5486168939376577\n",
      "Training loss: 10526728842990.934 678 2.548530171376605\n",
      "Training loss: 10526727724509.867 679 2.5484434901046322\n",
      "Training loss: 10526726382332.586 680 2.5483569467105425\n",
      "Training loss: 10526724369066.666 681 2.548270473147075\n",
      "Training loss: 10526722579496.96 682 2.5481839218375755\n",
      "Training loss: 10526721908408.32 683 2.548097567845549\n",
      "Training loss: 10526719895142.4 684 2.548011301002188\n",
      "Training loss: 10526719447749.973 685 2.547925024240994\n",
      "Training loss: 10526717658180.268 686 2.5478387958176225\n",
      "Training loss: 10526715644914.346 687 2.5477526723149584\n",
      "Training loss: 10526714973825.707 688 2.547666617735011\n",
      "Training loss: 10526713631648.426 689 2.547580695396127\n",
      "Training loss: 10526712065774.934 690 2.5474946195947874\n",
      "Training loss: 10526710947293.867 691 2.547408779118603\n",
      "Training loss: 10526710276205.227 692 2.547322951356281\n",
      "Training loss: 10526707144458.24 693 2.547237319361613\n",
      "Training loss: 10526706473369.6 694 2.547151546651613\n",
      "Training loss: 10526705131192.32 695 2.547065897306523\n",
      "Training loss: 10526704236407.467 696 2.546980324075592\n",
      "Training loss: 10526702223141.547 697 2.5468948484384155\n",
      "Training loss: 10526700209875.627 698 2.5468094384882214\n",
      "Training loss: 10526699538786.986 699 2.5467241007921575\n",
      "Training loss: 10526698196609.707 700 2.546638743106071\n",
      "Training loss: 10526696407040.0 701 2.5465534433034986\n",
      "Training loss: 10526695735951.36 702 2.546468369286195\n",
      "Training loss: 10526693275293.014 703 2.5463832152927304\n",
      "Training loss: 10526692827900.586 704 2.546298104213954\n",
      "Training loss: 10526691262027.094 705 2.54621313078043\n",
      "Training loss: 10526689919849.812 706 2.5461282367422333\n",
      "Training loss: 10526689025064.96 707 2.5460433005177014\n",
      "Training loss: 10526686788102.826 708 2.545958513622006\n",
      "Training loss: 10526685669621.76 709 2.54587382393315\n",
      "Training loss: 10526684551140.693 710 2.5457890882885303\n",
      "Training loss: 10526682537874.773 711 2.54570442179194\n",
      "Training loss: 10526680972001.28 712 2.5456198751732533\n",
      "Training loss: 10526680300912.64 713 2.5455354055196273\n",
      "Training loss: 10526678511342.934 714 2.5454510416866634\n",
      "Training loss: 10526676945469.44 715 2.5453666015232232\n",
      "Training loss: 10526675379595.947 716 2.545282251919235\n",
      "Training loss: 10526674708507.307 717 2.545197968561912\n",
      "Training loss: 10526672918937.6 718 2.5451138131623487\n",
      "Training loss: 10526671576760.32 719 2.545029787827262\n",
      "Training loss: 10526669339798.188 720 2.5449456715357464\n",
      "Training loss: 10526668668709.547 721 2.5448616683070258\n",
      "Training loss: 10526667326532.268 722 2.544777626285174\n",
      "Training loss: 10526665984354.986 723 2.5446937516382895\n",
      "Training loss: 10526664418481.494 724 2.54461001187601\n",
      "Training loss: 10526663300000.426 725 2.5445263347106732\n",
      "Training loss: 10526661510430.72 726 2.544442418617011\n",
      "Training loss: 10526660168253.44 727 2.5443587769137164\n",
      "Training loss: 10526658826076.16 728 2.544275326493943\n",
      "Training loss: 10526656812810.24 729 2.544191909308645\n",
      "Training loss: 10526656589114.027 730 2.5441081997075115\n",
      "Training loss: 10526653681063.254 731 2.5440248237410614\n",
      "Training loss: 10526653457367.04 732 2.5439417255050785\n",
      "Training loss: 10526652115189.76 733 2.5438582477755185\n",
      "Training loss: 10526649878227.627 734 2.5437749775191194\n",
      "Training loss: 10526650101923.84 735 2.543691954298673\n",
      "Training loss: 10526647193873.066 736 2.5436087124046165\n",
      "Training loss: 10526646075392.0 737 2.5435257492110526\n",
      "Training loss: 10526645404303.36 738 2.5434428529720465\n",
      "Training loss: 10526643167341.227 739 2.5433599157175975\n",
      "Training loss: 10526642048860.16 740 2.5432769392721126\n",
      "Training loss: 10526640706682.88 741 2.543194179059784\n",
      "Training loss: 10526639140809.387 742 2.5431113934218637\n",
      "Training loss: 10526637574935.893 743 2.543028869069985\n",
      "Training loss: 10526636456454.826 744 2.5429462972107237\n",
      "Training loss: 10526635337973.76 745 2.5428634878866396\n",
      "Training loss: 10526633324707.84 746 2.542780976535285\n",
      "Training loss: 10526631758834.346 747 2.5426988137504773\n",
      "Training loss: 10526631311441.92 748 2.5426162968465333\n",
      "Training loss: 10526629298176.0 749 2.542533844593247\n",
      "Training loss: 10526628179694.934 750 2.5424517579748045\n",
      "Training loss: 10526626613821.44 751 2.5423694905804908\n",
      "Training loss: 10526624824251.732 752 2.5422872696623626\n",
      "Training loss: 10526623705770.666 753 2.5422052846933516\n",
      "Training loss: 10526621916200.96 754 2.5421232605537\n",
      "Training loss: 10526620797719.893 755 2.5420412629317863\n",
      "Training loss: 10526619008150.188 756 2.541959330285389\n",
      "Training loss: 10526618113365.334 757 2.54187751179963\n",
      "Training loss: 10526616323795.627 758 2.541795681386414\n",
      "Training loss: 10526615205314.56 759 2.5417139278264673\n",
      "Training loss: 10526614086833.494 760 2.541632304444267\n",
      "Training loss: 10526612744656.213 761 2.541550709651081\n",
      "Training loss: 10526610507694.08 762 2.5414690589142555\n",
      "Training loss: 10526610060301.654 763 2.541387594568931\n",
      "Training loss: 10526607823339.52 764 2.541306244718607\n",
      "Training loss: 10526606928554.666 765 2.5412247562261867\n",
      "Training loss: 10526605362681.174 766 2.541143422869864\n",
      "Training loss: 10526604467896.32 767 2.541062161987969\n",
      "Training loss: 10526602902022.826 768 2.5409810495739684\n",
      "Training loss: 10526601112453.12 769 2.5408998955908118\n",
      "Training loss: 10526600441364.48 770 2.5408187550017436\n",
      "Training loss: 10526597757009.92 771 2.540737603699996\n",
      "Training loss: 10526597085921.28 772 2.5406566661821604\n",
      "Training loss: 10526596191136.426 773 2.540575959808035\n",
      "Training loss: 10526594848959.146 774 2.540494953721762\n",
      "Training loss: 10526591940908.373 775 2.5404140076198622\n",
      "Training loss: 10526591940908.373 776 2.540333355366513\n",
      "Training loss: 10526590375034.88 777 2.54025285987822\n",
      "Training loss: 10526589703946.24 778 2.5401720814401485\n",
      "Training loss: 10526587466984.107 779 2.540091473109086\n",
      "Training loss: 10526585901110.613 780 2.5400109584640744\n",
      "Training loss: 10526583664148.48 781 2.539930588519393\n",
      "Training loss: 10526582993059.84 782 2.5398501240504823\n",
      "Training loss: 10526582545667.414 783 2.539769713375907\n",
      "Training loss: 10526580979793.92 784 2.539689402265042\n",
      "Training loss: 10526578519135.574 785 2.5396092516780766\n",
      "Training loss: 10526577848046.934 786 2.5395291950844094\n",
      "Training loss: 10526575611084.8 787 2.539448931575469\n",
      "Training loss: 10526574492603.732 788 2.5393687933259366\n",
      "Training loss: 10526572703034.027 789 2.5392888858149303\n",
      "Training loss: 10526572255641.6 790 2.539209029578857\n",
      "Training loss: 10526570242375.68 791 2.5391289813841826\n",
      "Training loss: 10526569123894.613 792 2.5390491416741243\n",
      "Training loss: 10526568452805.973 793 2.538969388900741\n",
      "Training loss: 10526566663236.268 794 2.5388896885202716\n",
      "Training loss: 10526564873666.56 795 2.5388099406084828\n",
      "Training loss: 10526563531489.28 796 2.53873049824208\n",
      "Training loss: 10526561965615.787 797 2.5386508439138082\n",
      "Training loss: 10526560847134.72 798 2.5385712278199644\n",
      "Training loss: 10526560176046.08 799 2.538491835715699\n",
      "Training loss: 10526558386476.373 800 2.538412595525754\n",
      "Training loss: 10526556596906.666 801 2.5383331577783603\n",
      "Training loss: 10526555254729.387 802 2.538253864503415\n",
      "Training loss: 10526553688855.893 803 2.538174757813868\n",
      "Training loss: 10526552570374.826 804 2.5380955114079606\n",
      "Training loss: 10526551228197.547 805 2.53801634423986\n",
      "Training loss: 10526548991235.414 806 2.5379373288995524\n",
      "Training loss: 10526548543842.986 807 2.5378583270562363\n",
      "Training loss: 10526546530577.066 808 2.537779505140512\n",
      "Training loss: 10526545412096.0 809 2.5377004927208504\n",
      "Training loss: 10526543846222.506 810 2.5376215519304006\n",
      "Training loss: 10526542951437.654 811 2.537542932738277\n",
      "Training loss: 10526540714475.52 812 2.537464261711663\n",
      "Training loss: 10526539595994.453 813 2.5373854225271466\n",
      "Training loss: 10526538924905.812 814 2.537306815106244\n",
      "Training loss: 10526536687943.68 815 2.5372283649363583\n",
      "Training loss: 10526535345766.4 816 2.5371498509862977\n",
      "Training loss: 10526534003589.12 817 2.5370713521175086\n",
      "Training loss: 10526532437715.627 818 2.5369929962899276\n",
      "Training loss: 10526531766626.986 819 2.536914681825005\n",
      "Training loss: 10526529977057.28 820 2.5368363541965144\n",
      "Training loss: 10526528634880.0 821 2.536758151694381\n",
      "Training loss: 10526527740095.146 822 2.536679960194657\n",
      "Training loss: 10526525726829.227 823 2.5366018571394786\n",
      "Training loss: 10526525055740.586 824 2.536523790407882\n",
      "Training loss: 10526523489867.094 825 2.5364458118764412\n",
      "Training loss: 10526521700297.387 826 2.5363677836611327\n",
      "Training loss: 10526520134423.893 827 2.5362898558592457\n",
      "Training loss: 10526519239639.04 828 2.536212073918062\n",
      "Training loss: 10526517673765.547 829 2.5361343360701145\n",
      "Training loss: 10526516331588.268 830 2.5360565378268976\n",
      "Training loss: 10526514542018.56 831 2.5359787980381334\n",
      "Training loss: 10526513423537.494 832 2.53590121917522\n",
      "Training loss: 10526512081360.213 833 2.535823682690856\n",
      "Training loss: 10526510068094.293 834 2.535746052027757\n",
      "Training loss: 10526508725917.014 835 2.535668640690516\n",
      "Training loss: 10526508278524.586 836 2.5355912381213113\n",
      "Training loss: 10526506936347.307 837 2.5355138624655935\n",
      "Training loss: 10526504923081.387 838 2.535436545757974\n",
      "Training loss: 10526503357207.893 839 2.5353592420569715\n",
      "Training loss: 10526501791334.4 840 2.5352820166615104\n",
      "Training loss: 10526500896549.547 841 2.5352050031884197\n",
      "Training loss: 10526498883283.627 842 2.535128026145506\n",
      "Training loss: 10526498212194.986 843 2.535050787105953\n",
      "Training loss: 10526495975232.854 844 2.534973648960589\n",
      "Training loss: 10526494185663.146 845 2.5348968801553333\n",
      "Training loss: 10526492843485.867 846 2.5348202569194935\n",
      "Training loss: 10526491725004.8 847 2.5347432174863225\n",
      "Training loss: 10526490830219.947 848 2.534666263402\n",
      "Training loss: 10526488816954.027 849 2.5345896448983605\n",
      "Training loss: 10526487922169.174 850 2.5345131285346834\n",
      "Training loss: 10526486579991.893 851 2.5344363791729765\n",
      "Training loss: 10526485237814.613 852 2.534359838202604\n",
      "Training loss: 10526484119333.547 853 2.5342833270557814\n",
      "Training loss: 10526482553460.053 854 2.534206852068326\n",
      "Training loss: 10526481211282.773 855 2.534130487395072\n",
      "Training loss: 10526479645409.28 856 2.5340542018601475\n",
      "Training loss: 10526478526928.213 857 2.53397778826544\n",
      "Training loss: 10526477184750.934 858 2.5339014968462594\n",
      "Training loss: 10526476066269.867 859 2.5338253743556427\n",
      "Training loss: 10526472934522.88 860 2.533749223661734\n",
      "Training loss: 10526472487130.453 861 2.533673152877185\n",
      "Training loss: 10526470921256.96 862 2.533597112310614\n",
      "Training loss: 10526470250168.32 863 2.533521085290896\n",
      "Training loss: 10526468013206.188 864 2.5334450986450885\n",
      "Training loss: 10526466671028.906 865 2.533369280427474\n",
      "Training loss: 10526464881459.2 866 2.5332934909827434\n",
      "Training loss: 10526464657762.986 867 2.5332177301250947\n",
      "Training loss: 10526463315585.707 868 2.533141907857991\n",
      "Training loss: 10526461302319.787 869 2.5330662581669223\n",
      "Training loss: 10526459512750.08 870 2.532990635497904\n",
      "Training loss: 10526458394269.014 871 2.5329150833152942\n",
      "Training loss: 10526456828395.52 872 2.5328394917352046\n",
      "Training loss: 10526455486218.24 873 2.532764058507135\n",
      "Training loss: 10526453920344.746 874 2.5326887587850546\n",
      "Training loss: 10526452130775.04 875 2.5326132747103394\n",
      "Training loss: 10526451012293.973 876 2.53253779265696\n",
      "Training loss: 10526449670116.693 877 2.53246278889803\n",
      "Training loss: 10526449222724.268 878 2.532387660585175\n",
      "Training loss: 10526447209458.346 879 2.5323123077418703\n",
      "Training loss: 10526445867281.066 880 2.5322371928360834\n",
      "Training loss: 10526444301407.574 881 2.532162130982237\n",
      "Training loss: 10526442959230.293 882 2.532087138832862\n",
      "Training loss: 10526441169660.586 883 2.5320122831305656\n",
      "Training loss: 10526440498571.947 884 2.531937394873072\n",
      "Training loss: 10526439827483.307 885 2.531862335220213\n",
      "Training loss: 10526437143128.746 886 2.5317875208193708\n",
      "Training loss: 10526435577255.254 887 2.5317130135488815\n",
      "Training loss: 10526434235077.973 888 2.531638241114573\n",
      "Training loss: 10526433116596.906 889 2.5315622086247123\n",
      "Training loss: 10526441617053.014 890 2.531486351132942\n",
      "Training loss: 10526433340293.12 891 2.531411719411826\n",
      "Training loss: 10526429761153.707 892 2.531337412298481\n",
      "Training loss: 10526428195280.213 893 2.5312634256764994\n",
      "Training loss: 10526425958318.08 894 2.5311894320240342\n",
      "Training loss: 10526425510925.654 895 2.5311152351649158\n",
      "Training loss: 10526424616140.8 896 2.5310408689307895\n",
      "Training loss: 10526423721355.947 897 2.5309665617272543\n",
      "Training loss: 10526422602874.88 898 2.530892264505229\n",
      "Training loss: 10526421260697.6 899 2.5308179591034445\n",
      "Training loss: 10526419918520.32 900 2.530743704409614\n",
      "Training loss: 10526418128950.613 901 2.5306696900399746\n",
      "Training loss: 10526416339380.906 902 2.530595531638852\n",
      "Training loss: 10526414773507.414 903 2.5305215593284776\n",
      "Training loss: 10526412983937.707 904 2.530447856456029\n",
      "Training loss: 10526410746975.574 905 2.5303742906982163\n",
      "Training loss: 10526408957405.867 906 2.530300325679593\n",
      "Training loss: 10526408957405.867 907 2.530226520712289\n",
      "Training loss: 10526407167836.16 908 2.530153038948927\n",
      "Training loss: 10526406720443.732 909 2.530079491627363\n",
      "Training loss: 10526404707177.812 910 2.5300056763779653\n",
      "Training loss: 10526403812392.96 911 2.529932067465401\n",
      "Training loss: 10526401799127.04 912 2.5298587489721607\n",
      "Training loss: 10526399562164.906 913 2.529785225756715\n",
      "Training loss: 10526399785861.12 914 2.5297116490659444\n",
      "Training loss: 10526397101506.56 915 2.529638484517379\n",
      "Training loss: 10526395983025.494 916 2.5295652358534313\n",
      "Training loss: 10526394417152.0 917 2.529492021539046\n",
      "Training loss: 10526393298670.934 918 2.529418737827726\n",
      "Training loss: 10526391285405.014 919 2.5293455704802805\n",
      "Training loss: 10526389495835.307 920 2.529272536385713\n",
      "Training loss: 10526388824746.666 921 2.5291994519641774\n",
      "Training loss: 10526387482569.387 922 2.5291264481745905\n",
      "Training loss: 10526385469303.467 923 2.529053563638162\n",
      "Training loss: 10526383456037.547 924 2.528980764604744\n",
      "Training loss: 10526383456037.547 925 2.528907768277182\n",
      "Training loss: 10526381442771.627 926 2.5288347921311973\n",
      "Training loss: 10526379876898.133 927 2.5287621016973967\n",
      "Training loss: 10526378758417.066 928 2.5286895265533476\n",
      "Training loss: 10526376968847.36 929 2.528616951269137\n",
      "Training loss: 10526376297758.72 930 2.528544127922736\n",
      "Training loss: 10526374508189.014 931 2.528471606426408\n",
      "Training loss: 10526373166011.732 932 2.5283991615793937\n",
      "Training loss: 10526371600138.24 933 2.5283265704123785\n",
      "Training loss: 10526370705353.387 934 2.5282541528765803\n",
      "Training loss: 10526368244695.04 935 2.5281819140514186\n",
      "Training loss: 10526368020998.826 936 2.528109635063848\n",
      "Training loss: 10526365112948.053 937 2.52803724850768\n",
      "Training loss: 10526364889251.84 938 2.5279649702774076\n",
      "Training loss: 10526362652289.707 939 2.527892783990977\n",
      "Training loss: 10526361533808.64 940 2.527820778690946\n",
      "Training loss: 10526359744238.934 941 2.527748801253862\n",
      "Training loss: 10526359073150.293 942 2.527676583211186\n",
      "Training loss: 10526357283580.586 943 2.5276045321891973\n",
      "Training loss: 10526356388795.732 944 2.527532700124814\n",
      "Training loss: 10526355270314.666 945 2.5274608496782043\n",
      "Training loss: 10526353257048.746 946 2.527389009511324\n",
      "Training loss: 10526352362263.893 947 2.527317161316691\n",
      "Training loss: 10526349901605.547 948 2.527245380693259\n",
      "Training loss: 10526349230516.906 949 2.527173645617942\n",
      "Training loss: 10526347664643.414 950 2.5271021839591787\n",
      "Training loss: 10526346098769.92 951 2.5270304854251635\n",
      "Training loss: 10526344756592.64 952 2.5269588350183105\n",
      "Training loss: 10526344085504.0 953 2.5268873916896633\n",
      "Training loss: 10526342295934.293 954 2.5268159518813325\n",
      "Training loss: 10526340282668.373 955 2.5267445028050464\n",
      "Training loss: 10526339164187.307 956 2.5266731597832357\n",
      "Training loss: 10526337374617.6 957 2.526601824815683\n",
      "Training loss: 10526336256136.533 958 2.526530472844658\n",
      "Training loss: 10526334690263.04 959 2.526459278764498\n",
      "Training loss: 10526333571781.973 960 2.526388199836247\n",
      "Training loss: 10526332453300.906 961 2.526317048606637\n",
      "Training loss: 10526330663731.2 962 2.526245896429687\n",
      "Training loss: 10526329097857.707 963 2.526174802349902\n",
      "Training loss: 10526328203072.854 964 2.526103934447787\n",
      "Training loss: 10526327084591.787 965 2.526032990210515\n",
      "Training loss: 10526325742414.506 966 2.5259620183060068\n",
      "Training loss: 10526323505452.373 967 2.525891165825183\n",
      "Training loss: 10526322386971.307 968 2.5258204065191716\n",
      "Training loss: 10526321044794.027 969 2.5257496292790864\n",
      "Training loss: 10526319031528.107 970 2.525678889698848\n",
      "Training loss: 10526318136743.254 971 2.5256083074455313\n",
      "Training loss: 10526316794565.973 972 2.525537652849737\n",
      "Training loss: 10526315228692.48 973 2.5254670081870465\n",
      "Training loss: 10526314557603.84 974 2.525396490329454\n",
      "Training loss: 10526312320641.707 975 2.5253260940092375\n",
      "Training loss: 10526310978464.426 976 2.525255621885548\n",
      "Training loss: 10526309412590.934 977 2.5251851984393334\n",
      "Training loss: 10526307399325.014 978 2.525114929715149\n",
      "Training loss: 10526306951932.586 979 2.525044564712424\n",
      "Training loss: 10526305609755.307 980 2.5249743218049483\n",
      "Training loss: 10526303596489.387 981 2.524904209181406\n",
      "Training loss: 10526302701704.533 982 2.5248339743600305\n",
      "Training loss: 10526300688438.613 983 2.5247638379576767\n",
      "Training loss: 10526299122565.12 984 2.524693843024262\n",
      "Training loss: 10526297780387.84 985 2.5246238835699604\n",
      "Training loss: 10526296885602.986 986 2.5245537924095545\n",
      "Training loss: 10526295096033.28 987 2.5244839047594025\n",
      "Training loss: 10526293977552.213 988 2.5244139811629096\n",
      "Training loss: 10526292859071.146 989 2.5243441137424107\n",
      "Training loss: 10526291069501.44 990 2.524274531688246\n",
      "Training loss: 10526289727324.16 991 2.524204726661021\n",
      "Training loss: 10526287937754.453 992 2.5241349086026923\n",
      "Training loss: 10526287042969.6 993 2.524065248584417\n",
      "Training loss: 10526285477096.107 994 2.523995687269767\n",
      "Training loss: 10526283463830.188 995 2.523926079543297\n",
      "Training loss: 10526283240133.973 996 2.5238566986792317\n",
      "Training loss: 10526280555779.414 997 2.5237872496583553\n",
      "Training loss: 10526279884690.773 998 2.5237177310058136\n",
      "Training loss: 10526278542513.494 999 2.5236482701030574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1000\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10526276529247.574 0 2.5235789832403137\n",
      "Training loss: 10526275634462.72 1 2.5235097157910773\n",
      "Training loss: 10526273844893.014 2 2.523440577605568\n",
      "Training loss: 10526272279019.52 3 2.5233713101336757\n",
      "Training loss: 10526272502715.732 4 2.5233020192330238\n",
      "Training loss: 10526269818361.174 5 2.5232330050939304\n",
      "Training loss: 10526268476183.893 6 2.523163996089291\n",
      "Training loss: 10526266686614.188 7 2.5230948745339568\n",
      "Training loss: 10526265120740.693 8 2.523025899322123\n",
      "Training loss: 10526265344436.906 9 2.5229570857500287\n",
      "Training loss: 10526262660082.346 10 2.5228881885248833\n",
      "Training loss: 10526261541601.28 11 2.5228192265894496\n",
      "Training loss: 10526259752031.574 12 2.522750468868988\n",
      "Training loss: 10526258633550.506 13 2.5226818020755077\n",
      "Training loss: 10526257291373.227 14 2.5226129955438523\n",
      "Training loss: 10526255725499.732 15 2.522544325276475\n",
      "Training loss: 10526253935930.027 16 2.522475874203917\n",
      "Training loss: 10526252593752.746 17 2.5224072516458564\n",
      "Training loss: 10526251475271.68 18 2.5223385630226276\n",
      "Training loss: 10526250133094.4 19 2.522270115199315\n",
      "Training loss: 10526248567220.906 20 2.5222018520258875\n",
      "Training loss: 10526247672436.053 21 2.5221333941619792\n",
      "Training loss: 10526245659170.133 22 2.5220649477074337\n",
      "Training loss: 10526244316992.854 23 2.5219967615179275\n",
      "Training loss: 10526243198511.787 24 2.521928337539121\n",
      "Training loss: 10526241856334.506 25 2.5218600594850415\n",
      "Training loss: 10526240290461.014 26 2.521792204289234\n",
      "Training loss: 10526238948283.732 27 2.5217239418874917\n",
      "Training loss: 10526236935017.812 28 2.521655625291386\n",
      "Training loss: 10526236040232.96 29 2.521587831778252\n",
      "Training loss: 10526234250663.254 30 2.5215197948051373\n",
      "Training loss: 10526233355878.4 31 2.5214516765114845\n",
      "Training loss: 10526232013701.12 32 2.5213839601124834\n",
      "Training loss: 10526230224131.414 33 2.521316182666416\n",
      "Training loss: 10526228881954.133 34 2.5212480311344376\n",
      "Training loss: 10526227987169.28 35 2.5211802931055027\n",
      "Training loss: 10526225526510.934 36 2.521112752592827\n",
      "Training loss: 10526224184333.654 37 2.5210449952325584\n",
      "Training loss: 10526223065852.586 38 2.520977294975976\n",
      "Training loss: 10526222394763.947 39 2.5209097520061916\n",
      "Training loss: 10526220828890.453 40 2.520842221231233\n",
      "Training loss: 10526218815624.533 41 2.5207745930751937\n",
      "Training loss: 10526217920839.68 42 2.5207071139139163\n",
      "Training loss: 10526215907573.76 43 2.5206397325312304\n",
      "Training loss: 10526215236485.12 44 2.520572347792175\n",
      "Training loss: 10526212328434.346 45 2.5205049651950486\n",
      "Training loss: 10526211657345.707 46 2.520437671765587\n",
      "Training loss: 10526210315168.426 47 2.5203703094032064\n",
      "Training loss: 10526208972991.146 48 2.5203031014964727\n",
      "Training loss: 10526207183421.44 49 2.520236116537379\n",
      "Training loss: 10526206736029.014 50 2.5201689423144193\n",
      "Training loss: 10526204722763.094 51 2.52010159637718\n",
      "Training loss: 10526203604282.027 52 2.52003480012514\n",
      "Training loss: 10526201591016.107 53 2.5199679188661337\n",
      "Training loss: 10526200696231.254 54 2.5199005472700713\n",
      "Training loss: 10526199354053.973 55 2.5198338984047695\n",
      "Training loss: 10526198235572.906 56 2.5197672829097413\n",
      "Training loss: 10526196446003.2 57 2.519700070259823\n",
      "Training loss: 10526195103825.92 58 2.519633107452361\n",
      "Training loss: 10526193537952.426 59 2.5195667600264096\n",
      "Training loss: 10526191972078.934 60 2.5194999684321546\n",
      "Training loss: 10526191077294.08 61 2.519433178061641\n",
      "Training loss: 10526189511420.586 62 2.5193667834037616\n",
      "Training loss: 10526188616635.732 63 2.519299988518105\n",
      "Training loss: 10526185932281.174 64 2.5192332852257238\n",
      "Training loss: 10526184590103.893 65 2.519167183161714\n",
      "Training loss: 10526183471622.826 66 2.5191007137034434\n",
      "Training loss: 10526182353141.76 67 2.5190340481285873\n",
      "Training loss: 10526181010964.48 68 2.5189677886637205\n",
      "Training loss: 10526179668787.2 69 2.518901491966127\n",
      "Training loss: 10526178102913.707 70 2.518835206741374\n",
      "Training loss: 10526176760736.426 71 2.5187689754031264\n",
      "Training loss: 10526175418559.146 72 2.5187027878613573\n",
      "Training loss: 10526173405293.227 73 2.5186366081848424\n",
      "Training loss: 10526172063115.947 74 2.518570489211659\n",
      "Training loss: 10526170944634.88 75 2.5185044484817434\n",
      "Training loss: 10526169602457.6 76 2.5184384126251333\n",
      "Training loss: 10526168483976.533 77 2.5183723991193605\n",
      "Training loss: 10526167365495.467 78 2.518306492391461\n",
      "Training loss: 10526166023318.188 79 2.5182405567394763\n",
      "Training loss: 10526163562659.84 80 2.5181746951801163\n",
      "Training loss: 10526163338963.627 81 2.518108923777002\n",
      "Training loss: 10526161325697.707 82 2.5180431435502366\n",
      "Training loss: 10526159759824.213 83 2.51797734227808\n",
      "Training loss: 10526157970254.506 84 2.5179115947735413\n",
      "Training loss: 10526156851773.44 85 2.5178459455818865\n",
      "Training loss: 10526155062203.732 86 2.517780472173305\n",
      "Training loss: 10526154391115.094 87 2.517714985838396\n",
      "Training loss: 10526153048937.812 88 2.517649287398759\n",
      "Training loss: 10526150811975.68 89 2.517583677408035\n",
      "Training loss: 10526148351317.334 90 2.517518356201655\n",
      "Training loss: 10526147903924.906 91 2.5174530580368546\n",
      "Training loss: 10526146785443.84 92 2.517387608640089\n",
      "Training loss: 10526145890658.986 93 2.5173222687829173\n",
      "Training loss: 10526144101089.28 94 2.5172571268622628\n",
      "Training loss: 10526143206304.426 95 2.5171917373686017\n",
      "Training loss: 10526141640430.934 96 2.517126430854112\n",
      "Training loss: 10526139627165.014 97 2.5170614542371506\n",
      "Training loss: 10526138956076.373 98 2.5169964150106834\n",
      "Training loss: 10526136495418.027 99 2.5169311675170403\n",
      "Training loss: 10526135600633.174 100 2.5168661725313526\n",
      "Training loss: 10526133587367.254 101 2.5168012511088276\n",
      "Training loss: 10526132468886.188 102 2.5167362159214717\n",
      "Training loss: 10526131350405.12 103 2.5166712793790813\n",
      "Training loss: 10526130008227.84 104 2.516606548726586\n",
      "Training loss: 10526127994961.92 105 2.5165417025992722\n",
      "Training loss: 10526126876480.854 106 2.516476728306969\n",
      "Training loss: 10526124863214.934 107 2.5164120196224737\n",
      "Training loss: 10526124639518.72 108 2.5163475008169893\n",
      "Training loss: 10526122402556.586 109 2.516282830542353\n",
      "Training loss: 10526121507771.732 110 2.5162180799335627\n",
      "Training loss: 10526119941898.24 111 2.5161533492953083\n",
      "Training loss: 10526117928632.32 112 2.5160888739099025\n",
      "Training loss: 10526117481239.893 113 2.516024639628775\n",
      "Training loss: 10526115244277.76 114 2.5159600426730413\n",
      "Training loss: 10526113678404.268 115 2.515895490926257\n",
      "Training loss: 10526113231011.84 116 2.5158312021395335\n",
      "Training loss: 10526110322961.066 117 2.5157668465500365\n",
      "Training loss: 10526109651872.426 118 2.5157024995318724\n",
      "Training loss: 10526108533391.36 119 2.5156382664800625\n",
      "Training loss: 10526107638606.506 120 2.515574102082673\n",
      "Training loss: 10526105401644.373 121 2.515509906966495\n",
      "Training loss: 10526103835770.88 122 2.515445749982022\n",
      "Training loss: 10526102046201.174 123 2.5153815477339894\n",
      "Training loss: 10526101151416.32 124 2.5153175007517663\n",
      "Training loss: 10526099585542.826 125 2.515253626427058\n",
      "Training loss: 10526098467061.76 126 2.5151896482746117\n",
      "Training loss: 10526097124884.48 127 2.515125615323761\n",
      "Training loss: 10526095335314.773 128 2.5150616301552167\n",
      "Training loss: 10526095111618.56 129 2.5149977890849398\n",
      "Training loss: 10526093322048.854 130 2.514934082660566\n",
      "Training loss: 10526091085086.72 131 2.5148702593464503\n",
      "Training loss: 10526089295517.014 132 2.5148064231950467\n",
      "Training loss: 10526088400732.16 133 2.5147428103956275\n",
      "Training loss: 10526087058554.88 134 2.514679142297415\n",
      "Training loss: 10526086163770.027 135 2.514615393041761\n",
      "Training loss: 10526084597896.533 136 2.5145518512047786\n",
      "Training loss: 10526082360934.4 137 2.514488384489674\n",
      "Training loss: 10526080795060.906 138 2.5144249998086425\n",
      "Training loss: 10526079900276.053 139 2.5143614470264066\n",
      "Training loss: 10526078334402.56 140 2.5142978110494836\n",
      "Training loss: 10526077887010.133 141 2.5142344071614597\n",
      "Training loss: 10526075873744.213 142 2.514171337169178\n",
      "Training loss: 10526073636782.08 143 2.5141080266103115\n",
      "Training loss: 10526072965693.44 144 2.5140446577270525\n",
      "Training loss: 10526071623516.16 145 2.513981432238089\n",
      "Training loss: 10526069833946.453 146 2.5139181045881753\n",
      "Training loss: 10526068491769.174 147 2.51385487333153\n",
      "Training loss: 10526066478503.254 148 2.513792045988936\n",
      "Training loss: 10526065583718.4 149 2.513729133695075\n",
      "Training loss: 10526064017844.906 150 2.513665889950638\n",
      "Training loss: 10526063346756.268 151 2.5136026565226914\n",
      "Training loss: 10526061109794.133 152 2.513539770173088\n",
      "Training loss: 10526059991313.066 153 2.5134770947200273\n",
      "Training loss: 10526059096528.213 154 2.5134140948450154\n",
      "Training loss: 10526057083262.293 155 2.5133513136712278\n",
      "Training loss: 10526056188477.44 156 2.513288578763323\n",
      "Training loss: 10526053280426.666 157 2.5132255479680308\n",
      "Training loss: 10526051938249.387 158 2.5131628574060807\n",
      "Training loss: 10526051938249.387 159 2.5131003813239374\n",
      "Training loss: 10526049477591.04 160 2.5130377247111344\n",
      "Training loss: 10526048582806.188 161 2.512974912457556\n",
      "Training loss: 10526046793236.48 162 2.512912350731645\n",
      "Training loss: 10526045674755.414 163 2.5128500247135017\n",
      "Training loss: 10526044332578.133 164 2.512787575094163\n",
      "Training loss: 10526042319312.213 165 2.5127249368667166\n",
      "Training loss: 10526040977134.934 166 2.51266235173746\n",
      "Training loss: 10526039858653.867 167 2.512600139396732\n",
      "Training loss: 10526038292780.373 168 2.5125380141750218\n",
      "Training loss: 10526037397995.52 169 2.512475490552094\n",
      "Training loss: 10526035384729.6 170 2.5124132866572886\n",
      "Training loss: 10526034489944.746 171 2.5123511483233663\n",
      "Training loss: 10526033371463.68 172 2.51228878531412\n",
      "Training loss: 10526031134501.547 173 2.5122266737224312\n",
      "Training loss: 10526030239716.693 174 2.512164702584378\n",
      "Training loss: 10526028450146.986 175 2.512102587626094\n",
      "Training loss: 10526027331665.92 176 2.5120404281121327\n",
      "Training loss: 10526025765792.426 177 2.511978476398036\n",
      "Training loss: 10526023976222.72 178 2.511916628649126\n",
      "Training loss: 10526022634045.44 179 2.5118546293530137\n",
      "Training loss: 10526021739260.586 180 2.5117927777287625\n",
      "Training loss: 10526020397083.307 181 2.5117308278441723\n",
      "Training loss: 10526019054906.027 182 2.5116691422191058\n",
      "Training loss: 10526018160121.174 183 2.5116073630154605\n",
      "Training loss: 10526015699462.826 184 2.511545716819228\n",
      "Training loss: 10526013909893.12 185 2.511483758384336\n",
      "Training loss: 10526013462500.693 186 2.511422259132108\n",
      "Training loss: 10526011896627.2 187 2.5113606897920033\n",
      "Training loss: 10526010330753.707 188 2.5112989871433955\n",
      "Training loss: 10526008541184.0 189 2.511237427404018\n",
      "Training loss: 10526007199006.72 190 2.5111761152149334\n",
      "Training loss: 10526006080525.654 191 2.5111145706397617\n",
      "Training loss: 10526005185740.8 192 2.5110530616303923\n",
      "Training loss: 10526002725082.453 193 2.510991738161758\n",
      "Training loss: 10526001830297.6 194 2.5109303269751155\n",
      "Training loss: 10525999817031.68 195 2.510868926736752\n",
      "Training loss: 10525999369639.254 196 2.5108078939513465\n",
      "Training loss: 10525997356373.334 197 2.510746580209922\n",
      "Training loss: 10525996908980.906 198 2.5106852224394447\n",
      "Training loss: 10525994224626.346 199 2.510624066023837\n",
      "Training loss: 10525992435056.64 200 2.5105630695705803\n",
      "Training loss: 10525992211360.426 201 2.5105019910320236\n",
      "Training loss: 10525990198094.506 202 2.5104409017508047\n",
      "Training loss: 10525989079613.44 203 2.510379779196055\n",
      "Training loss: 10525987737436.16 204 2.5103186387354426\n",
      "Training loss: 10525986395258.88 205 2.5102578845891785\n",
      "Training loss: 10525984158296.746 206 2.5101974113353958\n",
      "Training loss: 10525983039815.68 207 2.510136080837653\n",
      "Training loss: 10525981697638.4 208 2.510074998064542\n",
      "Training loss: 10525980131764.906 209 2.5100146461462236\n",
      "Training loss: 10525978565891.414 210 2.509953856845432\n",
      "Training loss: 10525976776321.707 211 2.509892764194511\n",
      "Training loss: 10525975881536.854 212 2.5098323579307737\n",
      "Training loss: 10525974315663.36 213 2.5097719459883385\n",
      "Training loss: 10525972973486.08 214 2.5097110302160166\n",
      "Training loss: 10525970736523.947 215 2.509650354949907\n",
      "Training loss: 10525969618042.88 216 2.509589889618477\n",
      "Training loss: 10525968499561.812 217 2.5095295185360618\n",
      "Training loss: 10525967157384.533 218 2.5094689647614117\n",
      "Training loss: 10525966262599.68 219 2.5094085433815443\n",
      "Training loss: 10525964249333.76 220 2.509348214878789\n",
      "Training loss: 10525962012371.627 221 2.5092876950468566\n",
      "Training loss: 10525961788675.414 222 2.5092273391749034\n",
      "Training loss: 10525959999105.707 223 2.5091672230741295\n",
      "Training loss: 10525958656928.426 224 2.5091070717260555\n",
      "Training loss: 10525957314751.146 225 2.5090467619027383\n",
      "Training loss: 10525955972573.867 226 2.508986453622251\n",
      "Training loss: 10525954630396.586 227 2.5089263735026424\n",
      "Training loss: 10525952840826.88 228 2.508866358510199\n",
      "Training loss: 10525951498649.6 229 2.5088061855668475\n",
      "Training loss: 10525950156472.32 230 2.508746256753639\n",
      "Training loss: 10525948814295.04 231 2.5086862970670687\n",
      "Training loss: 10525947919510.188 232 2.5086262958203105\n",
      "Training loss: 10525946353636.693 233 2.508566243785206\n",
      "Training loss: 10525944564066.986 234 2.508506370199097\n",
      "Training loss: 10525943445585.92 235 2.508446643739135\n",
      "Training loss: 10525941432320.0 236 2.5083869581753873\n",
      "Training loss: 10525940537535.146 237 2.5083270025610047\n",
      "Training loss: 10525938076876.8 238 2.5082671361613142\n",
      "Training loss: 10525936958395.732 239 2.508207583015758\n",
      "Training loss: 10525935616218.453 240 2.5081479902070822\n",
      "Training loss: 10525934497737.387 241 2.5080882514473473\n",
      "Training loss: 10525933379256.32 242 2.508028654317293\n",
      "Training loss: 10525932037079.04 243 2.507969096466374\n",
      "Training loss: 10525930247509.334 244 2.50790959922633\n",
      "Training loss: 10525929129028.268 245 2.5078501280541547\n",
      "Training loss: 10525926892066.133 246 2.50779062125558\n",
      "Training loss: 10525925549888.854 247 2.507731190046995\n",
      "Training loss: 10525924431407.787 248 2.5076718735890324\n",
      "Training loss: 10525923312926.72 249 2.5076125603540786\n",
      "Training loss: 10525921970749.44 250 2.5075530997582236\n",
      "Training loss: 10525920181179.732 251 2.5074938317401685\n",
      "Training loss: 10525918615306.24 252 2.5074347041093112\n",
      "Training loss: 10525917273128.96 253 2.5073755650540144\n",
      "Training loss: 10525915259863.04 254 2.507316292748855\n",
      "Training loss: 10525914365078.188 255 2.5072570889807446\n",
      "Training loss: 10525913246597.12 256 2.5071980561380176\n",
      "Training loss: 10525911457027.414 257 2.5071389807016824\n",
      "Training loss: 10525910114850.133 258 2.5070799434493534\n",
      "Training loss: 10525908548976.64 259 2.5070211274810372\n",
      "Training loss: 10525905864622.08 260 2.5069629528484896\n",
      "Training loss: 10525906759406.934 261 2.506903619782837\n",
      "Training loss: 10525906088318.293 262 2.5068444577129467\n",
      "Training loss: 10525904298748.586 263 2.506785779166957\n",
      "Training loss: 10525902061786.453 264 2.506726804129105\n",
      "Training loss: 10525900048520.533 265 2.5066675500083124\n",
      "Training loss: 10525898930039.467 266 2.506608863611406\n",
      "Training loss: 10525896916773.547 267 2.5065504278478685\n",
      "Training loss: 10525895798292.48 268 2.5064917875516786\n",
      "Training loss: 10525894679811.414 269 2.5064330128958283\n",
      "Training loss: 10525892666545.494 270 2.506374410229596\n",
      "Training loss: 10525891771760.64 271 2.5063158671081482\n",
      "Training loss: 10525890205887.146 272 2.5062575265529023\n",
      "Training loss: 10525889758494.72 273 2.506198980887671\n",
      "Training loss: 10525887297836.373 274 2.5061402975944342\n",
      "Training loss: 10525885731962.88 275 2.5060817959672708\n",
      "Training loss: 10525884389785.6 276 2.506023469462957\n",
      "Training loss: 10525882376519.68 277 2.505965238408267\n",
      "Training loss: 10525881481734.826 278 2.505906894692626\n",
      "Training loss: 10525880139557.547 279 2.5058483903162507\n",
      "Training loss: 10525879021076.48 280 2.5057901607163564\n",
      "Training loss: 10525877678899.2 281 2.5057320986170373\n",
      "Training loss: 10525875889329.494 282 2.5056738404980186\n",
      "Training loss: 10525874323456.0 283 2.5056156959453797\n",
      "Training loss: 10525872757582.506 284 2.5055575412519637\n",
      "Training loss: 10525871415405.227 285 2.505499476220436\n",
      "Training loss: 10525870296924.16 286 2.505441546562094\n",
      "Training loss: 10525868283658.24 287 2.5053834789309715\n",
      "Training loss: 10525866941480.96 288 2.5053252099820926\n",
      "Training loss: 10525865599303.68 289 2.505267323388054\n",
      "Training loss: 10525864257126.4 290 2.50520987358122\n",
      "Training loss: 10525863362341.547 291 2.505151711552151\n",
      "Training loss: 10525861125379.414 292 2.505093690585594\n",
      "Training loss: 10525859112113.494 293 2.5050361303491706\n",
      "Training loss: 10525858441024.854 294 2.504978253946791\n",
      "Training loss: 10525857098847.574 295 2.5049203037070127\n",
      "Training loss: 10525855309277.867 296 2.5048629088126564\n",
      "Training loss: 10525854190796.8 297 2.504805257408351\n",
      "Training loss: 10525853072315.732 298 2.504747324343285\n",
      "Training loss: 10525850611657.387 299 2.5046898598272707\n",
      "Training loss: 10525849493176.32 300 2.504632532078402\n",
      "Training loss: 10525848598391.467 301 2.504574521547589\n",
      "Training loss: 10525846585125.547 302 2.5045171377529303\n",
      "Training loss: 10525845242948.268 303 2.5044600519374467\n",
      "Training loss: 10525844124467.2 304 2.504402360570991\n",
      "Training loss: 10525841887505.066 305 2.504344749525069\n",
      "Training loss: 10525840545327.787 306 2.504287678459325\n",
      "Training loss: 10525839874239.146 307 2.504230248184418\n",
      "Training loss: 10525838532061.867 308 2.504172710109232\n",
      "Training loss: 10525836071403.52 309 2.504115775206358\n",
      "Training loss: 10525834729226.24 310 2.504058507253846\n",
      "Training loss: 10525833387048.96 311 2.5040011324440505\n",
      "Training loss: 10525832268567.893 312 2.50394409823567\n",
      "Training loss: 10525831373783.04 313 2.503886892408853\n",
      "Training loss: 10525829807909.547 314 2.5038296574418903\n",
      "Training loss: 10525828242036.053 315 2.5037728044832552\n",
      "Training loss: 10525826676162.56 316 2.5037159887677154\n",
      "Training loss: 10525825781377.707 317 2.503658702206517\n",
      "Training loss: 10525823544415.574 318 2.503601505192057\n",
      "Training loss: 10525822202238.293 319 2.503544695896459\n",
      "Training loss: 10525821083757.227 320 2.5034880932966517\n",
      "Training loss: 10525819965276.16 321 2.5034312549712503\n",
      "Training loss: 10525818623098.88 322 2.503374134165611\n",
      "Training loss: 10525816833529.174 323 2.503317256005641\n",
      "Training loss: 10525815043959.467 324 2.503260598915494\n",
      "Training loss: 10525813925478.4 325 2.5032040103354354\n",
      "Training loss: 10525812583301.12 326 2.503147127285169\n",
      "Training loss: 10525811464820.053 327 2.5030905079935\n",
      "Training loss: 10525810122642.773 328 2.5030340920175385\n",
      "Training loss: 10525807885680.64 329 2.5029772951109783\n",
      "Training loss: 10525806990895.787 330 2.502920631399613\n",
      "Training loss: 10525804530237.44 331 2.5028642356091053\n",
      "Training loss: 10525803411756.373 332 2.502807719960812\n",
      "Training loss: 10525801622186.666 333 2.5027512482799694\n",
      "Training loss: 10525800951098.027 334 2.502694810500989\n",
      "Training loss: 10525798266743.467 335 2.5026383435144064\n",
      "Training loss: 10525797371958.613 336 2.502582054392484\n",
      "Training loss: 10525796700869.973 337 2.5025256957746116\n",
      "Training loss: 10525795582388.906 338 2.5024693993963405\n",
      "Training loss: 10525794240211.627 339 2.5024131710511637\n",
      "Training loss: 10525792003249.494 340 2.502356755294661\n",
      "Training loss: 10525791332160.854 341 2.5023005761378863\n",
      "Training loss: 10525789989983.574 342 2.5022446284120687\n",
      "Training loss: 10525787753021.44 343 2.5021885098982195\n",
      "Training loss: 10525786410844.16 344 2.5021321627817583\n",
      "Training loss: 10525785516059.307 345 2.502075961991419\n",
      "Training loss: 10525784173882.027 346 2.502020001933945\n",
      "Training loss: 10525782608008.533 347 2.50196428430071\n",
      "Training loss: 10525780594742.613 348 2.5019083581284005\n",
      "Training loss: 10525779028869.12 349 2.5018521564614162\n",
      "Training loss: 10525777462995.627 350 2.5017960107251094\n",
      "Training loss: 10525776120818.346 351 2.50174030548095\n",
      "Training loss: 10525775226033.494 352 2.5016846840868414\n",
      "Training loss: 10525773212767.574 353 2.50162902850207\n",
      "Training loss: 10525771870590.293 354 2.501572835405584\n",
      "Training loss: 10525771199501.654 355 2.501516966682236\n",
      "Training loss: 10525769409931.947 356 2.501461518448308\n",
      "Training loss: 10525767620362.24 357 2.501405867213936\n",
      "Training loss: 10525766278184.96 358 2.501350114880588\n",
      "Training loss: 10525765159703.893 359 2.5012944034326865\n",
      "Training loss: 10525763593830.4 360 2.501238881284825\n",
      "Training loss: 10525761356868.268 361 2.501183560480559\n",
      "Training loss: 10525761133172.053 362 2.5011277428422343\n",
      "Training loss: 10525758225121.28 363 2.501072004266018\n",
      "Training loss: 10525757777728.854 364 2.501016888780404\n",
      "Training loss: 10525755988159.146 365 2.5009618537230587\n",
      "Training loss: 10525753974893.227 366 2.500905957995129\n",
      "Training loss: 10525753303804.586 367 2.5008503473475416\n",
      "Training loss: 10525751514234.88 368 2.5007952586922486\n",
      "Training loss: 10525750395753.812 369 2.5007400589973914\n",
      "Training loss: 10525748829880.32 370 2.500684826467841\n",
      "Training loss: 10525746816614.4 371 2.5006296164010307\n",
      "Training loss: 10525745921829.547 372 2.5005741595560718\n",
      "Training loss: 10525744355956.053 373 2.500518932218216\n",
      "Training loss: 10525742790082.56 374 2.500464147447029\n",
      "Training loss: 10525742118993.92 375 2.500408875619801\n",
      "Training loss: 10525739658335.574 376 2.500353631057655\n",
      "Training loss: 10525739210943.146 377 2.5002987721287786\n",
      "Training loss: 10525737645069.654 378 2.5002438368345117\n",
      "Training loss: 10525735184411.307 379 2.5001886369944795\n",
      "Training loss: 10525734513322.666 380 2.5001337487012516\n",
      "Training loss: 10525732723752.96 381 2.500078937805778\n",
      "Training loss: 10525731605271.893 382 2.5000239732352156\n",
      "Training loss: 10525730263094.613 383 2.499969101287145\n",
      "Training loss: 10525728249828.693 384 2.4999142995759045\n",
      "Training loss: 10525727355043.84 385 2.4998594497519506\n",
      "Training loss: 10525725789170.346 386 2.499804690821139\n",
      "Training loss: 10525724223296.854 387 2.499750012862212\n",
      "Training loss: 10525723104815.787 388 2.499695246844028\n",
      "Training loss: 10525721538942.293 389 2.4996405731502525\n",
      "Training loss: 10525721315246.08 390 2.499585974583528\n",
      "Training loss: 10525718183499.094 391 2.4995313764860794\n",
      "Training loss: 10525716841321.812 392 2.4994768266568275\n",
      "Training loss: 10525715275448.32 393 2.499422273842121\n",
      "Training loss: 10525713933271.04 394 2.499367696841418\n",
      "Training loss: 10525711920005.12 395 2.4993131625368763\n",
      "Training loss: 10525711248916.48 396 2.4992588592180214\n",
      "Training loss: 10525709459346.773 397 2.4992043445888883\n",
      "Training loss: 10525708564561.92 398 2.499149949013908\n",
      "Training loss: 10525707669777.066 399 2.499095708195818\n",
      "Training loss: 10525706551296.0 400 2.499041370142782\n",
      "Training loss: 10525703643245.227 401 2.4989869251554193\n",
      "Training loss: 10525702301067.947 402 2.498932623731304\n",
      "Training loss: 10525700958890.666 403 2.4988785109531784\n",
      "Training loss: 10525699840409.6 404 2.4988244771216395\n",
      "Training loss: 10525697827143.68 405 2.49877017508447\n",
      "Training loss: 10525696261270.188 406 2.498715954633702\n",
      "Training loss: 10525696037573.973 407 2.4986618431207104\n",
      "Training loss: 10525693800611.84 408 2.4986077852963677\n",
      "Training loss: 10525692458434.56 409 2.498553681516758\n",
      "Training loss: 10525691339953.494 410 2.498499923206958\n",
      "Training loss: 10525689550383.787 411 2.498445930789319\n",
      "Training loss: 10525688431902.72 412 2.4983917220885083\n",
      "Training loss: 10525686642333.014 413 2.498337780694112\n",
      "Training loss: 10525685300155.732 414 2.498283979419274\n",
      "Training loss: 10525683957978.453 415 2.4982302911449192\n",
      "Training loss: 10525682392104.96 416 2.4981763207065457\n",
      "Training loss: 10525679931446.613 417 2.4981223354948425\n",
      "Training loss: 10525679484054.188 418 2.498068739253246\n",
      "Training loss: 10525677694484.48 419 2.498015263273471\n",
      "Training loss: 10525676128610.986 420 2.4979612515683014\n",
      "Training loss: 10525674786433.707 421 2.4979074005731783\n",
      "Training loss: 10525673667952.64 422 2.497853952593672\n",
      "Training loss: 10525671654686.72 423 2.497800490228678\n",
      "Training loss: 10525671207294.293 424 2.4977467731914302\n",
      "Training loss: 10525669194028.373 425 2.497693170560809\n",
      "Training loss: 10525666957066.24 426 2.497639714871649\n",
      "Training loss: 10525665838585.174 427 2.4975860307027986\n",
      "Training loss: 10525665167496.533 428 2.4975326215675775\n",
      "Training loss: 10525663154230.613 429 2.497479593388716\n",
      "Training loss: 10525662483141.973 430 2.497425944524393\n",
      "Training loss: 10525660246179.84 431 2.4973722236310283\n",
      "Training loss: 10525658456610.133 432 2.49731899540202\n",
      "Training loss: 10525657338129.066 433 2.49726613692943\n",
      "Training loss: 10525656443344.213 434 2.4972125782401933\n",
      "Training loss: 10525654877470.72 435 2.4971591916804026\n",
      "Training loss: 10525652640508.586 436 2.4971060411821666\n",
      "Training loss: 10525651745723.732 437 2.497052942341126\n",
      "Training loss: 10525649956154.027 438 2.4969996984058787\n",
      "Training loss: 10525648837672.96 439 2.496946548791898\n",
      "Training loss: 10525647495495.68 440 2.4968934726465886\n",
      "Training loss: 10525646377014.613 441 2.4968404529897965\n",
      "Training loss: 10525644587444.906 442 2.496787268549718\n",
      "Training loss: 10525642797875.2 443 2.4967342537049793\n",
      "Training loss: 10525641455697.92 444 2.4966814367608983\n",
      "Training loss: 10525640113520.64 445 2.4966285123104948\n",
      "Training loss: 10525638323950.934 446 2.496575414633679\n",
      "Training loss: 10525637429166.08 447 2.4965224875370544\n",
      "Training loss: 10525635639596.373 448 2.4964697667479876\n",
      "Training loss: 10525634297419.094 449 2.496416955563939\n",
      "Training loss: 10525632731545.6 450 2.4963641196341073\n",
      "Training loss: 10525631165672.107 451 2.4963112055898105\n",
      "Training loss: 10525629599798.613 452 2.4962583593020513\n",
      "Training loss: 10525628033925.12 453 2.4962058558398708\n",
      "Training loss: 10525627139140.268 454 2.496153295348023\n",
      "Training loss: 10525625796962.986 455 2.4961006046042757\n",
      "Training loss: 10525624902178.133 456 2.4960477908536456\n",
      "Training loss: 10525621994127.36 457 2.4958473111679873\n",
      "Training loss: 10526635561669.973 458 2.4956563409737638\n",
      "Training loss: 10525998698550.613 459 2.495598701889595\n",
      "Training loss: 10525724894385.494 460 2.495590907251841\n",
      "Training loss: 10525612598886.4 461 2.495594622590277\n",
      "Training loss: 10525618191291.732 462 2.4955866716961186\n",
      "Training loss: 10525687760814.08 463 2.495555631791289\n",
      "Training loss: 10525772765375.146 464 2.495500546413814\n",
      "Training loss: 10525838084669.44 465 2.4954274255714783\n",
      "Training loss: 10525867612569.6 466 2.495345902079186\n",
      "Training loss: 10525863362341.547 467 2.495265250104438\n",
      "Training loss: 10525833834441.387 468 2.4951917728096946\n",
      "Training loss: 10525790661072.213 469 2.495128531063293\n",
      "Training loss: 10525745921829.547 470 2.4950749724470347\n",
      "Training loss: 10525708788258.133 471 2.495028660877945\n",
      "Training loss: 10525685747548.16 472 2.4949858664461173\n",
      "Training loss: 10525676352307.2 473 2.494942869009769\n",
      "Training loss: 10525677694484.48 474 2.494897415582625\n",
      "Training loss: 10525687313421.654 475 2.494848298681741\n",
      "Training loss: 10525696932358.826 476 2.4947959257311556\n",
      "Training loss: 10525703866941.44 477 2.4947406259557425\n",
      "Training loss: 10525704314333.867 478 2.494684139552815\n",
      "Training loss: 10525702748460.373 479 2.4946282889271894\n",
      "Training loss: 10525695813877.76 480 2.4945731912500433\n",
      "Training loss: 10525687089725.44 481 2.4945191828794866\n",
      "Training loss: 10525676799699.627 482 2.4944668964863443\n",
      "Training loss: 10525669194028.373 483 2.4944158590992718\n",
      "Training loss: 10525663825319.254 484 2.4943653674633417\n",
      "Training loss: 10525659575091.2 485 2.494314486314852\n",
      "Training loss: 10525657561825.28 486 2.494263283778112\n",
      "Training loss: 10525655995951.787 487 2.4941351817030974\n",
      "Training loss: 10526714973825.707 488 2.493931051940214\n",
      "Training loss: 10526084150504.107 489 2.4938613388005426\n",
      "Training loss: 10525811464820.053 490 2.493844149145476\n",
      "Training loss: 10525692458434.56 491 2.493841978038908\n",
      "Training loss: 10525682168408.746 492 2.493831015088799\n",
      "Training loss: 10525735184411.307 493 2.493799564386177\n",
      "Training loss: 10525806096110.934 494 2.4937457826802913\n",
      "Training loss: 10525862020164.268 495 2.493675259500419\n",
      "Training loss: 10525887297836.373 496 2.4935968989024815\n",
      "Training loss: 10525881705431.04 497 2.4935192339524015\n",
      "Training loss: 10525852624923.307 498 2.493448232391273\n",
      "Training loss: 10525811241123.84 499 2.4933869503567703\n",
      "Training loss: 10525768738843.307 500 2.4933348010009095\n",
      "Training loss: 10525734065930.24 501 2.4932245985419876\n",
      "Training loss: 10526770674182.826 502 2.493031385631915\n",
      "Training loss: 10526129784531.627 503 2.4929741625585238\n",
      "Training loss: 10525862020164.268 504 2.4929666036575853\n",
      "Training loss: 10525757106640.213 505 2.4929684690768448\n",
      "Training loss: 10525764712311.467 506 2.4929563059451634\n",
      "Training loss: 10525830255301.973 507 2.492919610719108\n",
      "Training loss: 10525903851356.16 508 2.4928589282527303\n",
      "Training loss: 10525952617130.666 509 2.4927822232321\n",
      "Training loss: 10525965591511.04 510 2.492700237565208\n",
      "Training loss: 10525947695813.973 511 2.4926219818379014\n",
      "Training loss: 10525906535710.72 512 2.492553406640834\n",
      "Training loss: 10525858217328.64 513 2.4924964485850016\n",
      "Training loss: 10525812359604.906 514 2.4924484985134234\n",
      "Training loss: 10525779699957.76 515 2.4924054859977804\n",
      "Training loss: 10525761804260.693 516 2.4923642575674374\n",
      "Training loss: 10525758225121.28 517 2.4923214434653636\n",
      "Training loss: 10525763146437.973 518 2.4922744522747644\n",
      "Training loss: 10525770752109.227 519 2.4922230539019377\n",
      "Training loss: 10525776568210.773 520 2.4921687095470526\n",
      "Training loss: 10525777686691.84 521 2.4921129957659023\n",
      "Training loss: 10525772765375.146 522 2.4920577265102963\n",
      "Training loss: 10525763593830.4 523 2.4920030899200403\n",
      "Training loss: 10525752632715.947 524 2.4919499095435973\n",
      "Training loss: 10525741224209.066 525 2.491898815681294\n",
      "Training loss: 10525730710487.04 526 2.4918489864094155\n",
      "Training loss: 10525722433727.146 527 2.491799451883768\n",
      "Training loss: 10525717288714.24 528 2.4917498258304107\n",
      "Training loss: 10525713933271.04 529 2.4916998055706694\n",
      "Training loss: 10525712143701.334 530 2.4916491419846496\n",
      "Training loss: 10525709011954.346 531 2.4915981581460698\n",
      "Training loss: 10525705880207.36 532 2.491546619068273\n",
      "Training loss: 10525702748460.373 533 2.491494815629795\n",
      "Training loss: 10525698498232.32 534 2.491443308004992\n",
      "Training loss: 10525692905826.986 535 2.491392139918148\n",
      "Training loss: 10525688431902.72 536 2.491341036314509\n",
      "Training loss: 10525683286889.812 537 2.491290280581334\n",
      "Training loss: 10525678589269.334 538 2.4912397258365684\n",
      "Training loss: 10525674115345.066 539 2.4911892412795096\n",
      "Training loss: 10525671430990.506 540 2.4911387408453654\n",
      "Training loss: 10525667404458.666 541 2.49108818067908\n",
      "Training loss: 10525663377926.826 542 2.4910375806118967\n",
      "Training loss: 10525660917268.48 543 2.490987116484538\n",
      "Training loss: 10525657114432.854 544 2.4909364904850113\n",
      "Training loss: 10525653311597.227 545 2.490885744140987\n",
      "Training loss: 10525650179850.24 546 2.4908352463039707\n",
      "Training loss: 10525646153318.4 547 2.490784935999008\n",
      "Training loss: 10525643468963.84 548 2.4907344610308426\n",
      "Training loss: 10525639442432.0 549 2.4906840274378497\n",
      "Training loss: 10525635863292.586 550 2.490633735982698\n",
      "Training loss: 10525633178938.027 551 2.490583608669675\n",
      "Training loss: 10525630718279.68 552 2.490533376217312\n",
      "Training loss: 10525626468051.627 553 2.4904829552288166\n",
      "Training loss: 10525624454785.707 554 2.4904327201572865\n",
      "Training loss: 10525620428253.867 555 2.4903825980169403\n",
      "Training loss: 10525617072810.666 556 2.490332359957125\n",
      "Training loss: 10525613941063.68 557 2.4902821621738336\n",
      "Training loss: 10525611480405.334 558 2.490232004393456\n",
      "Training loss: 10525608572354.56 559 2.4901820275865725\n",
      "Training loss: 10525605440607.574 560 2.4901318290999996\n",
      "Training loss: 10525602756253.014 561 2.490081654799401\n",
      "Training loss: 10525600295594.666 562 2.490031777282896\n",
      "Training loss: 10525597387543.893 563 2.4899820446273786\n",
      "Training loss: 10525594479493.12 564 2.489931855224212\n",
      "Training loss: 10525590676657.494 565 2.4898817404539946\n",
      "Training loss: 10525588887087.787 566 2.489832029789176\n",
      "Training loss: 10525585531644.586 567 2.4897822527165765\n",
      "Training loss: 10525582176201.387 568 2.4897322339164054\n",
      "Training loss: 10525579939239.254 569 2.4896824031807987\n",
      "Training loss: 10525578373365.76 570 2.489632666639744\n",
      "Training loss: 10525575017922.56 571 2.489582975420126\n",
      "Training loss: 10525572557264.213 572 2.4895332587433847\n",
      "Training loss: 10525570320302.08 573 2.4894833606611253\n",
      "Training loss: 10525566293770.24 574 2.489434042494862\n",
      "Training loss: 10525568978124.8 575 2.4893842729146005\n",
      "Training loss: 10525561596149.76 576 2.4893345962756084\n",
      "Training loss: 10525559806580.053 577 2.48928473583253\n",
      "Training loss: 10525556898529.28 578 2.4892352153481285\n",
      "Training loss: 10525553766782.293 579 2.489185894838971\n",
      "Training loss: 10525551529820.16 580 2.4891365727301307\n",
      "Training loss: 10525549069161.812 581 2.48908688151889\n",
      "Training loss: 10525546161111.04 582 2.489037330732688\n",
      "Training loss: 10525543700452.693 583 2.488988076221936\n",
      "Training loss: 10525541687186.773 584 2.488938826907043\n",
      "Training loss: 10525539002832.213 585 2.4888892967848237\n",
      "Training loss: 10525536542173.867 586 2.4888399197248354\n",
      "Training loss: 10525534305211.732 587 2.488790711938732\n",
      "Training loss: 10525531844553.387 588 2.4887414030643713\n",
      "Training loss: 10525529383895.04 589 2.4886921753308893\n",
      "Training loss: 10525526475844.268 590 2.4886429134134156\n",
      "Training loss: 10525524686274.56 591 2.488593720415763\n",
      "Training loss: 10525522001920.0 592 2.488544655386934\n",
      "Training loss: 10525520212350.293 593 2.4884954772881165\n",
      "Training loss: 10525518199084.373 594 2.4884463693531282\n",
      "Training loss: 10525515738426.027 595 2.4883972487203647\n",
      "Training loss: 10525513501463.893 596 2.4883482005442494\n",
      "Training loss: 10525510146020.693 597 2.4882992413793623\n",
      "Training loss: 10525507909058.56 598 2.4882503670749023\n",
      "Training loss: 10525505448400.213 599 2.48820109647918\n",
      "Training loss: 10525503882526.72 600 2.488152193460613\n",
      "Training loss: 10525500527083.52 601 2.4881037533450385\n",
      "Training loss: 10525498513817.6 602 2.4880547248590634\n",
      "Training loss: 10525496053159.254 603 2.4880053643188407\n",
      "Training loss: 10525494039893.334 604 2.4879565929431493\n",
      "Training loss: 10525492250323.627 605 2.4879083795792885\n",
      "Training loss: 10525490237057.707 606 2.487859427463759\n",
      "Training loss: 10525488000095.574 607 2.487810346807721\n",
      "Training loss: 10525484644652.373 608 2.487761834556801\n",
      "Training loss: 10525482855082.666 609 2.4877131205534355\n",
      "Training loss: 10525481065512.96 610 2.487664498221339\n",
      "Training loss: 10525478381158.4 611 2.487615996756193\n",
      "Training loss: 10525475249411.414 612 2.4875672242889197\n",
      "Training loss: 10525475025715.2 613 2.487518389425081\n",
      "Training loss: 10525470999183.36 614 2.4874702627193614\n",
      "Training loss: 10525469209613.654 615 2.4874217339732354\n",
      "Training loss: 10525466972651.52 616 2.4873729213936335\n",
      "Training loss: 10525465630474.24 617 2.4873245341736125\n",
      "Training loss: 10525462722423.467 618 2.487276270817902\n",
      "Training loss: 10525461603942.4 619 2.4872276699255047\n",
      "Training loss: 10525458472195.414 620 2.4871792431556754\n",
      "Training loss: 10525456906321.92 621 2.4871310990543325\n",
      "Training loss: 10525455116752.213 622 2.4870826596663913\n",
      "Training loss: 10525452208701.44 623 2.487034261523521\n",
      "Training loss: 10525450866524.16 624 2.4869860269139563\n",
      "Training loss: 10525447958473.387 625 2.486937872251494\n",
      "Training loss: 10525445497815.04 626 2.486889456400388\n",
      "Training loss: 10525443931941.547 627 2.48684127632648\n",
      "Training loss: 10525441694979.414 628 2.486793308759244\n",
      "Training loss: 10525439905409.707 629 2.486745138939273\n",
      "Training loss: 10525437221055.146 630 2.48669682854119\n",
      "Training loss: 10525436102574.08 631 2.486648696434125\n",
      "Training loss: 10525432299738.453 632 2.486600739823072\n",
      "Training loss: 10525432076042.24 633 2.4865525710693688\n",
      "Training loss: 10525429167991.467 634 2.486504593996049\n",
      "Training loss: 10525427154725.547 635 2.48645701531779\n",
      "Training loss: 10525425365155.84 636 2.4864087809291986\n",
      "Training loss: 10525422680801.28 637 2.4863603881061738\n",
      "Training loss: 10525420443839.146 638 2.486312701494125\n",
      "Training loss: 10525418430573.227 639 2.4862652902779128\n",
      "Training loss: 10525415746218.666 640 2.486217155238329\n",
      "Training loss: 10525414851433.812 641 2.4861691439075124\n",
      "Training loss: 10525412390775.467 642 2.4861214095451714\n",
      "Training loss: 10525410153813.334 643 2.4860736767687315\n",
      "Training loss: 10525409035332.268 644 2.486026044230761\n",
      "Training loss: 10525406127281.494 645 2.485978233326837\n",
      "Training loss: 10525404561408.0 646 2.485930403888956\n",
      "Training loss: 10525402771838.293 647 2.485882779575033\n",
      "Training loss: 10525400534876.16 648 2.485835226655879\n",
      "Training loss: 10525398745306.453 649 2.485787532101657\n",
      "Training loss: 10525395837255.68 650 2.4857398649574725\n",
      "Training loss: 10525394047685.973 651 2.4856924697000378\n",
      "Training loss: 10525572109871.787 652 2.4856460379414527\n",
      "Training loss: 10525385099837.44 653 2.4855994487712763\n",
      "Training loss: 10525386442014.72 654 2.4855523277030214\n",
      "Training loss: 10525387336799.574 655 2.4855045435272674\n",
      "Training loss: 10525387113103.36 656 2.485456030912876\n",
      "Training loss: 10525386442014.72 657 2.4854075832001095\n",
      "Training loss: 10525384205052.586 658 2.4853592211272924\n",
      "Training loss: 10525380178520.746 659 2.4853115573931674\n",
      "Training loss: 10525377270469.973 660 2.485264497852458\n",
      "Training loss: 10525373691330.56 661 2.4852174141816734\n",
      "Training loss: 10525371678064.64 662 2.4851699494441184\n",
      "Training loss: 10525368993710.08 663 2.485123055948135\n",
      "Training loss: 10525367427836.586 664 2.485076536964365\n",
      "Training loss: 10525365638266.88 665 2.485029844946361\n",
      "Training loss: 10525364743482.027 666 2.4849823207170245\n",
      "Training loss: 10525363177608.533 667 2.4849346048909737\n",
      "Training loss: 10525360940646.4 668 2.4848874576548337\n",
      "Training loss: 10525359374772.906 669 2.4848410548603503\n",
      "Training loss: 10525357137810.773 670 2.4847933824209676\n",
      "Training loss: 10525354229760.0 671 2.4847458588045357\n",
      "Training loss: 10525352663886.506 672 2.4846991894678334\n",
      "Training loss: 10525351545405.44 673 2.4846522978784424\n",
      "Training loss: 10525348861050.88 674 2.4846050316830337\n",
      "Training loss: 10525346847784.96 675 2.484558388792743\n",
      "Training loss: 10525344834519.04 676 2.4845115868244996\n",
      "Training loss: 10525343268645.547 677 2.484464488262664\n",
      "Training loss: 10525340584290.986 678 2.4844176711250445\n",
      "Training loss: 10525339018417.494 679 2.484370899807591\n",
      "Training loss: 10525336781455.36 680 2.4843238133908776\n",
      "Training loss: 10525335439278.08 681 2.4842770906747087\n",
      "Training loss: 10525332978619.732 682 2.4842303985559093\n",
      "Training loss: 10525331412746.24 683 2.4841834915703807\n",
      "Training loss: 10525329175784.107 684 2.4841368033600677\n",
      "Training loss: 10525327386214.4 685 2.4840901684198733\n",
      "Training loss: 10525325372948.48 686 2.4840433124187142\n",
      "Training loss: 10525324030771.2 687 2.483984060837597\n",
      "Training loss: 10525541687186.773 688 2.483904261716549\n",
      "Training loss: 10525410601205.76 689 2.483851601921841\n",
      "Training loss: 10525354677152.426 690 2.4838105911247896\n",
      "Training loss: 10525328057303.04 691 2.483774083881737\n",
      "Training loss: 10525322017505.28 692 2.4837365520857637\n",
      "Training loss: 10525327386214.4 693 2.4836953033501707\n",
      "Training loss: 10525339018417.494 694 2.483649782196594\n",
      "Training loss: 10525349755835.732 695 2.4836006103567065\n",
      "Training loss: 10525356019329.707 696 2.4835492443345024\n",
      "Training loss: 10525354677152.426 697 2.4834979398015515\n",
      "Training loss: 10525353111278.934 698 2.483446306619407\n",
      "Training loss: 10525342821253.12 699 2.483396973397921\n",
      "Training loss: 10525331412746.24 700 2.48334970738844\n",
      "Training loss: 10525322241201.494 701 2.4833040683976497\n",
      "Training loss: 10525315306618.88 702 2.483259501488482\n",
      "Training loss: 10525311056390.826 703 2.483215095145273\n",
      "Training loss: 10525309266821.12 704 2.4831703847363458\n",
      "Training loss: 10525309043124.906 705 2.483125055510477\n",
      "Training loss: 10525308819428.693 706 2.483078937884039\n",
      "Training loss: 10525308595732.48 707 2.483032115736305\n",
      "Training loss: 10525308372036.268 708 2.482985236257544\n",
      "Training loss: 10525305016593.066 709 2.482938357633585\n",
      "Training loss: 10525302779630.934 710 2.482891572226715\n",
      "Training loss: 10525299200491.52 711 2.4828449286092082\n",
      "Training loss: 10525295845048.32 712 2.482798518140496\n",
      "Training loss: 10525292265908.906 713 2.4827526446159043\n",
      "Training loss: 10525290028946.773 714 2.4827072389223472\n",
      "Training loss: 10525287568288.426 715 2.4826614716785853\n",
      "Training loss: 10525284660237.654 716 2.482615183300276\n",
      "Training loss: 10525283989149.014 717 2.482569477401065\n",
      "Training loss: 10525281528490.666 718 2.4825237220750664\n",
      "Training loss: 10525279515224.746 719 2.4824777530383138\n",
      "Training loss: 10525277725655.04 720 2.482431464661052\n",
      "Training loss: 10525276607173.973 721 2.4823853498411084\n",
      "Training loss: 10525273699123.2 722 2.482339698066531\n",
      "Training loss: 10525271685857.28 723 2.4822937912584844\n",
      "Training loss: 10525268106717.867 724 2.482247818113289\n",
      "Training loss: 10525265869755.732 725 2.482202096266728\n",
      "Training loss: 10525265198667.094 726 2.4821563630398873\n",
      "Training loss: 10525263185401.174 727 2.4821106852579473\n",
      "Training loss: 10525260277350.4 728 2.482064989169623\n",
      "Training loss: 10525257592995.84 729 2.4820193871776848\n",
      "Training loss: 10525256250818.56 730 2.4819737038246927\n",
      "Training loss: 10525254237552.64 731 2.4819279253337325\n",
      "Training loss: 10525252447982.934 732 2.4818822588083402\n",
      "Training loss: 10525250882109.44 733 2.4818368079116806\n",
      "Training loss: 10525247526666.24 734 2.4817912418093604\n",
      "Training loss: 10525246184488.96 735 2.481745518194036\n",
      "Training loss: 10525243723830.613 736 2.481700094269964\n",
      "Training loss: 10525241934260.906 737 2.48165474996912\n",
      "Training loss: 10525239473602.56 738 2.48160916208573\n",
      "Training loss: 10525237460336.64 739 2.48156361891847\n",
      "Training loss: 10525235670766.934 740 2.481518291170445\n",
      "Training loss: 10525234328589.654 741 2.4814730638373934\n",
      "Training loss: 10525232091627.52 742 2.4814275004048714\n",
      "Training loss: 10525230302057.812 743 2.4813821110434175\n",
      "Training loss: 10525228736184.32 744 2.4813369386864004\n",
      "Training loss: 10525226499222.188 745 2.4812916767813045\n",
      "Training loss: 10525223814867.627 746 2.481246234562249\n",
      "Training loss: 10525222025297.92 747 2.4812009744394334\n",
      "Training loss: 10525220683120.64 748 2.4811557367958708\n",
      "Training loss: 10525218669854.72 749 2.4811106940214906\n",
      "Training loss: 10525216432892.586 750 2.481065553132269\n",
      "Training loss: 10525214643322.88 751 2.481020127385195\n",
      "Training loss: 10525212406360.746 752 2.4809748655110058\n",
      "Training loss: 10525211064183.467 753 2.480930049570894\n",
      "Training loss: 10525208379828.906 754 2.480885005679228\n",
      "Training loss: 10525205471778.133 755 2.4808397349433027\n",
      "Training loss: 10525204129600.854 756 2.4807948207297366\n",
      "Training loss: 10525201892638.72 757 2.4807498437324305\n",
      "Training loss: 10525200550461.44 758 2.4807045320057983\n",
      "Training loss: 10525198760891.732 759 2.4806596535582264\n",
      "Training loss: 10525196076537.174 760 2.4806151050582237\n",
      "Training loss: 10525195629144.746 761 2.480569842194285\n",
      "Training loss: 10525192944790.188 762 2.480524674334387\n",
      "Training loss: 10525191602612.906 763 2.4804800888458423\n",
      "Training loss: 10525189813043.2 764 2.480435350495205\n",
      "Training loss: 10525187128688.64 765 2.4803902183913684\n",
      "Training loss: 10525185115422.72 766 2.4803454412330495\n",
      "Training loss: 10525182878460.586 767 2.480300736131336\n",
      "Training loss: 10525180865194.666 768 2.4802558998037387\n",
      "Training loss: 10525178851928.746 769 2.4802112507796696\n",
      "Training loss: 10525177733447.68 770 2.4801665101381243\n",
      "Training loss: 10525176167574.188 771 2.480121687001576\n",
      "Training loss: 10525174378004.48 772 2.4800771100573873\n",
      "Training loss: 10525171246257.494 773 2.4800325443898896\n",
      "Training loss: 10525170575168.854 774 2.4799877691256804\n",
      "Training loss: 10525167667118.08 775 2.4799430694505515\n",
      "Training loss: 10525166324940.8 776 2.4798985442211183\n",
      "Training loss: 10525164759067.307 777 2.4798540806364473\n",
      "Training loss: 10525162522105.174 778 2.4798094970645868\n",
      "Training loss: 10525160285143.04 779 2.4797648736613014\n",
      "Training loss: 10525158495573.334 780 2.4797202423983338\n",
      "Training loss: 10525155811218.773 781 2.4796758979125935\n",
      "Training loss: 10525155363826.346 782 2.479631687541456\n",
      "Training loss: 10525153126864.213 783 2.479587062239929\n",
      "Training loss: 10525150889902.08 784 2.4795425127879915\n",
      "Training loss: 10525148876636.16 785 2.479498226321128\n",
      "Training loss: 10525146863370.24 786 2.4794538913207096\n",
      "Training loss: 10525145744889.174 787 2.4794095386000627\n",
      "Training loss: 10525143955319.467 788 2.479365199546263\n",
      "Training loss: 10525141494661.12 789 2.47932085306124\n",
      "Training loss: 10525139705091.414 790 2.4792765737461706\n",
      "Training loss: 10525137915521.707 791 2.479232327759744\n",
      "Training loss: 10525136349648.213 792 2.47918809651571\n",
      "Training loss: 10525134336382.293 793 2.479143836116153\n",
      "Training loss: 10525132323116.373 794 2.479099688264621\n",
      "Training loss: 10525130086154.24 795 2.4790558738198545\n",
      "Training loss: 10525128743976.96 796 2.4790114904968994\n",
      "Training loss: 10525127625495.893 797 2.4789671403854503\n",
      "Training loss: 10525125164837.547 798 2.478923376614099\n",
      "Training loss: 10525122927875.414 799 2.478879387775844\n",
      "Training loss: 10525121362001.92 800 2.4788348967166702\n",
      "Training loss: 10525119796128.426 801 2.478790961779118\n",
      "Training loss: 10525117559166.293 802 2.478747134598154\n",
      "Training loss: 10525116216989.014 803 2.478703074344649\n",
      "Training loss: 10525113532634.453 804 2.4786592032555057\n",
      "Training loss: 10525113085242.027 805 2.47861514654603\n",
      "Training loss: 10525110177191.254 806 2.4785710331982433\n",
      "Training loss: 10525108835013.973 807 2.4785274443792034\n",
      "Training loss: 10525107045444.268 808 2.478483840948925\n",
      "Training loss: 10525104808482.133 809 2.4784393841575736\n",
      "Training loss: 10525102795216.213 810 2.4783954354893525\n",
      "Training loss: 10525101676735.146 811 2.4783520462659787\n",
      "Training loss: 10525098992380.586 812 2.4783083503168215\n",
      "Training loss: 10525097202810.88 813 2.478264414452097\n",
      "Training loss: 10525095636937.387 814 2.4782205067026113\n",
      "Training loss: 10525093847367.68 815 2.4781768544193645\n",
      "Training loss: 10525092281494.188 816 2.478133142997491\n",
      "Training loss: 10525090939316.906 817 2.4780895296705525\n",
      "Training loss: 10525088702354.773 818 2.4780458812049235\n",
      "Training loss: 10525086018000.213 819 2.4780021506087984\n",
      "Training loss: 10525084675822.934 820 2.4779584601576006\n",
      "Training loss: 10525082886253.227 821 2.477914940875787\n",
      "Training loss: 10525081096683.52 822 2.4778714014353698\n",
      "Training loss: 10525078859721.387 823 2.477827857639196\n",
      "Training loss: 10525077964936.533 824 2.4777842901498306\n",
      "Training loss: 10525075504278.188 825 2.477740648080685\n",
      "Training loss: 10525073938404.693 826 2.477697170062571\n",
      "Training loss: 10525071701442.56 827 2.4776538626345617\n",
      "Training loss: 10525070359265.28 828 2.477610463160414\n",
      "Training loss: 10525067451214.506 829 2.4775668273484883\n",
      "Training loss: 10525065661644.8 830 2.477523335645783\n",
      "Training loss: 10525064990556.16 831 2.477480136869761\n",
      "Training loss: 10525062977290.24 832 2.4774367981346748\n",
      "Training loss: 10525060964024.32 833 2.477393243997272\n",
      "Training loss: 10525059398150.826 834 2.4773499840614206\n",
      "Training loss: 10525057832277.334 835 2.4773068187325706\n",
      "Training loss: 10525056042707.627 836 2.477263478094989\n",
      "Training loss: 10525053582049.28 837 2.477220099312114\n",
      "Training loss: 10525051792479.574 838 2.4771769111506057\n",
      "Training loss: 10525050002909.867 839 2.4771336654527754\n",
      "Training loss: 10525047989643.947 840 2.4770904943130274\n",
      "Training loss: 10525045976378.027 841 2.4770473629183067\n",
      "Training loss: 10525044186808.32 842 2.477004183680977\n",
      "Training loss: 10525042397238.613 843 2.4769609269637427\n",
      "Training loss: 10525041055061.334 844 2.476917819428626\n",
      "Training loss: 10525039265491.627 845 2.4768748071810216\n",
      "Training loss: 10525037252225.707 846 2.476831826996955\n",
      "Training loss: 10525035015263.574 847 2.4767886737380667\n",
      "Training loss: 10525033896782.506 848 2.4767454876598105\n",
      "Training loss: 10525030765035.52 849 2.476702407718637\n",
      "Training loss: 10525029422858.24 850 2.4766596643159366\n",
      "Training loss: 10525028080680.96 851 2.4766167362001577\n",
      "Training loss: 10525026291111.254 852 2.4765736478983618\n",
      "Training loss: 10525024948933.973 853 2.4765306530119218\n",
      "Training loss: 10525023159364.268 854 2.4764876981350707\n",
      "Training loss: 10525021146098.346 855 2.476444833575709\n",
      "Training loss: 10525019356528.64 856 2.4764021332317427\n",
      "Training loss: 10525017566958.934 857 2.4763592751524652\n",
      "Training loss: 10525015329996.8 858 2.476316245242561\n",
      "Training loss: 10525013987819.52 859 2.4762733358050166\n",
      "Training loss: 10525011527161.174 860 2.476230658050869\n",
      "Training loss: 10525009737591.467 861 2.4761879331668264\n",
      "Training loss: 10525007724325.547 862 2.476145347807207\n",
      "Training loss: 10525006829540.693 863 2.4761025123096863\n",
      "Training loss: 10525004592578.56 864 2.476059592880612\n",
      "Training loss: 10525002803008.854 865 2.4760169508829613\n",
      "Training loss: 10525000789742.934 866 2.475974466085005\n",
      "Training loss: 10524999000173.227 867 2.475931843675164\n",
      "Training loss: 10524997881692.16 868 2.47588903629935\n",
      "Training loss: 10524994973641.387 869 2.475846229630664\n",
      "Training loss: 10524993407767.893 870 2.475803673460442\n",
      "Training loss: 10524992065590.613 871 2.475761516115194\n",
      "Training loss: 10524989828628.48 872 2.475718913539723\n",
      "Training loss: 10524989157539.84 873 2.4756759655165195\n",
      "Training loss: 10524986249489.066 874 2.475633640631152\n",
      "Training loss: 10524985131008.0 875 2.4755913924435715\n",
      "Training loss: 10524981999261.014 876 2.4755486623666396\n",
      "Training loss: 10524981551868.586 877 2.475506141113174\n",
      "Training loss: 10524979314906.453 878 2.4754638658336123\n",
      "Training loss: 10524977525336.746 879 2.475421418572374\n",
      "Training loss: 10524976183159.467 880 2.4753790390546686\n",
      "Training loss: 10524974393589.76 881 2.4753367010089615\n",
      "Training loss: 10524972380323.84 882 2.475294123943119\n",
      "Training loss: 10524970367057.92 883 2.4752517991430927\n",
      "Training loss: 10524968577488.213 884 2.475209758812448\n",
      "Training loss: 10524966787918.506 885 2.475167316921191\n",
      "Training loss: 10524964998348.8 886 2.4751248805764656\n",
      "Training loss: 10524962985082.88 887 2.475082742535173\n",
      "Training loss: 10524961419209.387 888 2.4750406434967775\n",
      "Training loss: 10524959629639.68 889 2.474998307315776\n",
      "Training loss: 10524958063766.188 890 2.4749560245919304\n",
      "Training loss: 10524955826804.053 891 2.4749138661925376\n",
      "Training loss: 10524953589841.92 892 2.4748717321545577\n",
      "Training loss: 10524952471360.854 893 2.4748296389348354\n",
      "Training loss: 10524950458094.934 894 2.4747875332842657\n",
      "Training loss: 10524948668525.227 895 2.47474536697095\n",
      "Training loss: 10524946878955.52 896 2.4747032315475392\n",
      "Training loss: 10524944865689.6 897 2.4746610764484442\n",
      "Training loss: 10524943747208.533 898 2.474619124289972\n",
      "Training loss: 10524941510246.4 899 2.4745772609834993\n",
      "Training loss: 10524939720676.693 900 2.4745352794350333\n",
      "Training loss: 10524938154803.2 901 2.4744931048677867\n",
      "Training loss: 10524935917841.066 902 2.474451074542321\n",
      "Training loss: 10524934575663.787 903 2.4744091604562453\n",
      "Training loss: 10524932338701.654 904 2.474367470774006\n",
      "Training loss: 10524930996524.373 905 2.4743253778630176\n",
      "Training loss: 10524928759562.24 906 2.4742833021887094\n",
      "Training loss: 10524926969992.533 907 2.4742416382433547\n",
      "Training loss: 10524962313994.24 908 2.4742006324322317\n",
      "Training loss: 10524913771915.947 909 2.4741610417534936\n",
      "Training loss: 10524917574751.574 910 2.4741207990143823\n",
      "Training loss: 10524920706498.56 911 2.474077594605834\n",
      "Training loss: 10524922943460.693 912 2.474034643856038\n",
      "Training loss: 10524921824979.627 913 2.47399153872858\n",
      "Training loss: 10524920035409.92 914 2.4739480063654344\n",
      "Training loss: 10524917127359.146 915 2.473905308657191\n",
      "Training loss: 10524913771915.947 916 2.4738634884901973\n",
      "Training loss: 10524909969080.32 917 2.4738219471267127\n",
      "Training loss: 10524906389940.906 918 2.4737807849352236\n",
      "Training loss: 10524903929282.56 919 2.4737399136528486\n",
      "Training loss: 10524902587105.28 920 2.473698857965738\n",
      "Training loss: 10524900797535.574 921 2.473657603007223\n",
      "Training loss: 10524899455358.293 922 2.473616315560726\n",
      "Training loss: 10524898560573.44 923 2.4735745694574596\n",
      "Training loss: 10524897442092.373 924 2.473532768809452\n",
      "Training loss: 10524895876218.88 925 2.473491235544927\n",
      "Training loss: 10524894757737.812 926 2.473449692747326\n",
      "Training loss: 10524892520775.68 927 2.4734077405570476\n",
      "Training loss: 10524890283813.547 928 2.473366058220607\n",
      "Training loss: 10524888046851.414 929 2.4733250574354924\n",
      "Training loss: 10524886257281.707 930 2.4732838870016085\n",
      "Training loss: 10524884020319.574 931 2.4732423098969933\n",
      "Training loss: 10524882230749.867 932 2.4732009906085293\n",
      "Training loss: 10524879993787.732 933 2.473159750079344\n",
      "Training loss: 10524878875306.666 934 2.4731183644157824\n",
      "Training loss: 10524877309433.174 935 2.4730772222366935\n",
      "Training loss: 10524876414648.32 936 2.473035918269763\n",
      "Training loss: 10524873730293.76 937 2.472994498400613\n",
      "Training loss: 10524871717027.84 938 2.4729532225350743\n",
      "Training loss: 10524869927458.133 939 2.4729119373145965\n",
      "Training loss: 10524868361584.64 940 2.4728705633177963\n",
      "Training loss: 10524866348318.72 941 2.4728296072031566\n",
      "Training loss: 10524916679966.72 942 2.4727896855144964\n",
      "Training loss: 10524849571102.72 943 2.472751734164507\n",
      "Training loss: 10524855387204.268 944 2.4727124134867244\n",
      "Training loss: 10524860084824.746 945 2.4726697044494514\n",
      "Training loss: 10524862992875.52 946 2.4726267946906595\n",
      "Training loss: 10524863440267.947 947 2.4725839271716152\n",
      "Training loss: 10524861427002.027 948 2.472540514048164\n",
      "Training loss: 10524857847862.613 949 2.4724979013766197\n",
      "Training loss: 10524853150242.133 950 2.4724565246093397\n",
      "Training loss: 10524848900014.08 951 2.472415543682143\n",
      "Training loss: 10524845097178.453 952 2.47237551743895\n",
      "Training loss: 10524841965431.467 953 2.4723357038643\n",
      "Training loss: 10524841070646.613 954 2.4722952685564783\n",
      "Training loss: 10524839504773.12 955 2.472254713547719\n",
      "Training loss: 10524839952165.547 956 2.4722141428544493\n",
      "Training loss: 10524838386292.053 957 2.4721731205328155\n",
      "Training loss: 10524836820418.56 958 2.4721319364849665\n",
      "Training loss: 10524835925633.707 959 2.472090821587936\n",
      "Training loss: 10524834136064.0 960 2.472049558531881\n",
      "Training loss: 10524831899101.867 961 2.47200844013796\n",
      "Training loss: 10524828991051.094 962 2.471967637391674\n",
      "Training loss: 10524826977785.174 963 2.4719270644178875\n",
      "Training loss: 10524825411911.68 964 2.4718863475334296\n",
      "Training loss: 10524824740823.04 965 2.4718453553324347\n",
      "Training loss: 10524820714291.2 966 2.4718047832654473\n",
      "Training loss: 10524819819506.346 967 2.471764515477089\n",
      "Training loss: 10524818701025.28 968 2.471723840435261\n",
      "Training loss: 10524817135151.787 969 2.4716827051214394\n",
      "Training loss: 10524814674493.44 970 2.471642038681645\n",
      "Training loss: 10524813779708.586 971 2.47160165386923\n",
      "Training loss: 10524811766442.666 972 2.471560949294364\n",
      "Training loss: 10524809753176.746 973 2.471519965754222\n",
      "Training loss: 10524808187303.254 974 2.4714793358569955\n",
      "Training loss: 10524805279252.48 975 2.4714390622144493\n",
      "Training loss: 10524803713378.986 976 2.47139838133103\n",
      "Training loss: 10524802147505.494 977 2.4713576741032326\n",
      "Training loss: 10524800805328.213 978 2.4713172246794155\n",
      "Training loss: 10524798568366.08 979 2.47127669828685\n",
      "Training loss: 10524797226188.8 980 2.4712362629899967\n",
      "Training loss: 10524795660315.307 981 2.471195745168826\n",
      "Training loss: 10524793423353.174 982 2.4711551420113875\n",
      "Training loss: 10524791186391.04 983 2.4711147483443985\n",
      "Training loss: 10524790067909.973 984 2.471074349938714\n",
      "Training loss: 10524788502036.48 985 2.4710338432845824\n",
      "Training loss: 10524785593985.707 986 2.4709934073340283\n",
      "Training loss: 10524784699200.854 987 2.4709531313191677\n",
      "Training loss: 10524782462238.72 988 2.470912752250949\n",
      "Training loss: 10524780896365.227 989 2.4708723437273123\n",
      "Training loss: 10524778659403.094 990 2.4708319790109385\n",
      "Training loss: 10524777764618.24 991 2.4707916470958686\n",
      "Training loss: 10524775975048.533 992 2.470751380127681\n",
      "Training loss: 10524774185478.826 993 2.470711091904318\n",
      "Training loss: 10524771724820.48 994 2.470670728775756\n",
      "Training loss: 10524771053731.84 995 2.470630524725746\n",
      "Training loss: 10524768369377.28 996 2.470590368570415\n",
      "Training loss: 10524767250896.213 997 2.470550036676625\n",
      "Training loss: 10524765461326.506 998 2.4705098090202626\n",
      "Training loss: 10524763224364.373 999 2.470469688188403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1000\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10524761211098.453 0 2.4704294441150134\n",
      "Training loss: 10524760092617.387 1 2.4703893521733344\n",
      "Training loss: 10524757408262.826 2 2.470349358173749\n",
      "Training loss: 10524756289781.76 3 2.4703090188032837\n",
      "Training loss: 10524754947604.48 4 2.4702690109999645\n",
      "Training loss: 10524752039553.707 5 2.4702293329838794\n",
      "Training loss: 10524751368465.066 6 2.470189332891876\n",
      "Training loss: 10524749131502.934 7 2.4701490810927416\n",
      "Training loss: 10524747565629.44 8 2.470108938688637\n",
      "Training loss: 10524745776059.732 9 2.4700689826889373\n",
      "Training loss: 10524744433882.453 10 2.47002887255016\n",
      "Training loss: 10524741749527.893 11 2.4699889624169127\n",
      "Training loss: 10524740183654.4 12 2.4699491277192127\n",
      "Training loss: 10524738841477.12 13 2.4699088790166472\n",
      "Training loss: 10524737275603.627 14 2.469868999296713\n",
      "Training loss: 10524735486033.92 15 2.4698294172743003\n",
      "Training loss: 10524733025375.574 16 2.4697893524252397\n",
      "Training loss: 10524731459502.08 17 2.469749396371865\n",
      "Training loss: 10524729893628.586 18 2.4697098053093827\n",
      "Training loss: 10524727880362.666 19 2.469669884960006\n",
      "Training loss: 10524726090792.96 20 2.469629853670848\n",
      "Training loss: 10524724301223.254 21 2.469590115324921\n",
      "Training loss: 10524722735349.76 22 2.4694606921663445\n",
      "Training loss: 10526003172474.88 23 2.469238448436093\n",
      "Training loss: 10525237012944.213 24 2.4691880873853504\n",
      "Training loss: 10524908850599.254 25 2.4692041677356964\n",
      "Training loss: 10524764566541.654 26 2.469237664185424\n",
      "Training loss: 10524758974136.32 27 2.469256047748798\n",
      "Training loss: 10524832346494.293 28 2.469243130181809\n",
      "Training loss: 10524922048675.84 29 2.4691981646599257\n",
      "Training loss: 10524988710147.414 30 2.469130319878547\n",
      "Training loss: 10525016448477.867 31 2.4690526989893438\n",
      "Training loss: 10525005934755.84 32 2.4689778303554557\n",
      "Training loss: 10524967682703.36 33 2.4689135408307843\n",
      "Training loss: 10524916008878.08 34 2.468862753809428\n",
      "Training loss: 10524865229837.654 35 2.4688249284444663\n",
      "Training loss: 10524826083000.32 36 2.468795651454833\n",
      "Training loss: 10524803713378.986 37 2.4687690885217273\n",
      "Training loss: 10524798344669.867 38 2.4687406884786047\n",
      "Training loss: 10524803937075.2 39 2.4687080948703897\n",
      "Training loss: 10524815569278.293 40 2.468670161620133\n",
      "Training loss: 10524824517126.826 41 2.4686275923745153\n",
      "Training loss: 10524830333228.373 42 2.468582599800909\n",
      "Training loss: 10524828319962.453 43 2.468537148600362\n",
      "Training loss: 10524821385379.84 44 2.4684927353324713\n",
      "Training loss: 10524810200569.174 45 2.468450098371916\n",
      "Training loss: 10524798344669.867 46 2.468409587124866\n",
      "Training loss: 10524788725732.693 47 2.4683709866910846\n",
      "Training loss: 10524779777884.16 48 2.4683332835234446\n",
      "Training loss: 10524775527656.107 49 2.468295518859873\n",
      "Training loss: 10524772619605.334 50 2.468257576639068\n",
      "Training loss: 10524769487858.346 51 2.4682188019292255\n",
      "Training loss: 10524769264162.133 52 2.4681789597143102\n",
      "Training loss: 10524767474592.426 53 2.4681386801694827\n",
      "Training loss: 10524764566541.654 54 2.468098309833049\n",
      "Training loss: 10524761658490.88 55 2.4680579195177086\n",
      "Training loss: 10524758750440.107 56 2.4680177519962414\n",
      "Training loss: 10524752710642.346 57 2.4679778883816526\n",
      "Training loss: 10524748460414.293 58 2.4679382119376365\n",
      "Training loss: 10524745104971.094 59 2.467709321083939\n",
      "Training loss: 10526014133589.334 60 2.467506955859879\n",
      "Training loss: 10525186010207.574 61 2.467490707758323\n",
      "Training loss: 10524830556924.586 62 2.467541339756127\n",
      "Training loss: 10524704839652.693 63 2.4675982064099586\n",
      "Training loss: 10524744881274.88 64 2.4676248557432845\n",
      "Training loss: 10524861650698.24 65 2.4676070025959658\n",
      "Training loss: 10524977972729.174 66 2.4675483498050714\n",
      "Training loss: 10525049331821.227 67 2.4674649400288056\n",
      "Training loss: 10525065437948.586 68 2.4673755249475007\n",
      "Training loss: 10525035238959.787 69 2.4672953087780916\n",
      "Training loss: 10524976406855.68 70 2.4672318992164493\n",
      "Training loss: 10524907508421.973 71 2.467187417259092\n",
      "Training loss: 10524847781533.014 72 2.4671579788424296\n",
      "Training loss: 10524807516214.613 73 2.467136056495035\n",
      "Training loss: 10524791410087.254 74 2.4671141505887695\n",
      "Training loss: 10524795436619.094 75 2.467087313051402\n",
      "Training loss: 10524809529480.533 76 2.467053382467029\n",
      "Training loss: 10524825635607.893 77 2.4670126555156253\n",
      "Training loss: 10524836149329.92 78 2.466967208285942\n",
      "Training loss: 10524837267810.986 79 2.4669197272163377\n",
      "Training loss: 10524831228013.227 80 2.466873212592682\n",
      "Training loss: 10524818924721.494 81 2.466829383274993\n",
      "Training loss: 10524803489682.773 82 2.4667885573178503\n",
      "Training loss: 10524789396821.334 83 2.466750094590368\n",
      "Training loss: 10524778435706.88 84 2.4667132183011624\n",
      "Training loss: 10524771501124.268 85 2.466544953270844\n",
      "Training loss: 10526070728731.307 86 2.4663337590581613\n",
      "Training loss: 10525249763628.373 87 2.466307141237348\n",
      "Training loss: 10524900350143.146 88 2.466348550132632\n",
      "Training loss: 10524772843301.547 89 2.4663989196164837\n",
      "Training loss: 10524803042290.346 90 2.4664209865982802\n",
      "Training loss: 10524905942548.48 91 2.466400084135334\n",
      "Training loss: 10525010632376.32 92 2.4663408246603344\n",
      "Training loss: 10525074162100.906 93 2.466258694340771\n",
      "Training loss: 10525083781038.08 94 2.4661715752522126\n",
      "Training loss: 10525050897694.72 95 2.4660943795342534\n",
      "Training loss: 10524990052324.693 96 2.466034962115184\n",
      "Training loss: 10524922943460.693 97 2.4659933452454372\n",
      "Training loss: 10524865677230.08 98 2.465965190827701\n",
      "Training loss: 10524829438443.52 99 2.465943421993946\n",
      "Training loss: 10524817582544.213 100 2.4659205210564563\n",
      "Training loss: 10524822280164.693 101 2.4658918309510236\n",
      "Training loss: 10524835925633.707 102 2.4658559592650415\n",
      "Training loss: 10524849123710.293 103 2.465813830581434\n",
      "Training loss: 10524855163508.053 104 2.4657675360626143\n",
      "Training loss: 10524853597634.56 105 2.4657206039432675\n",
      "Training loss: 10524843978697.387 106 2.465675372743712\n",
      "Training loss: 10524829214747.307 107 2.4656327355909498\n",
      "Training loss: 10524813332316.16 108 2.465592884445499\n",
      "Training loss: 10524799239454.72 109 2.4655555678493344\n",
      "Training loss: 10524789173125.12 110 2.465519829847838\n",
      "Training loss: 10524782685934.934 111 2.465483931340906\n",
      "Training loss: 10524779330491.732 112 2.4654466802858015\n",
      "Training loss: 10524777317225.812 113 2.4654066356037982\n",
      "Training loss: 10524775527656.107 114 2.465368629382664\n",
      "Training loss: 10524773514390.188 115 2.4653293391436244\n",
      "Training loss: 10524768593073.494 116 2.4652879798576492\n",
      "Training loss: 10524763895453.014 117 2.4652485938644473\n",
      "Training loss: 10524758079351.467 118 2.465208901363107\n",
      "Training loss: 10524751144768.854 119 2.4651699568865895\n",
      "Training loss: 10524744657578.666 120 2.465132121736488\n",
      "Training loss: 10524739512565.76 121 2.4650928480578074\n",
      "Training loss: 10524735038641.494 122 2.4650546602455057\n",
      "Training loss: 10524731235805.867 123 2.4650169412928324\n",
      "Training loss: 10524726985577.812 124 2.464977873927935\n",
      "Training loss: 10524722735349.76 125 2.464939192470881\n",
      "Training loss: 10524719827298.986 126 2.4649003355387067\n",
      "Training loss: 10524716248159.574 127 2.464861426970163\n",
      "Training loss: 10524710879450.453 128 2.4648229245209086\n",
      "Training loss: 10524707300311.04 129 2.4647838924263024\n",
      "Training loss: 10524703273779.2 130 2.4647451397065594\n",
      "Training loss: 10524699470943.574 131 2.4647069388864704\n",
      "Training loss: 10524695220715.52 132 2.4646683972568626\n",
      "Training loss: 10524691417879.893 133 2.464630025270062\n",
      "Training loss: 10524686943955.627 134 2.464591689934343\n",
      "Training loss: 10524683364816.213 135 2.4645532079818\n",
      "Training loss: 10524680009373.014 136 2.4645148495823714\n",
      "Training loss: 10524676877626.027 137 2.464476543072283\n",
      "Training loss: 10524672851094.188 138 2.4644381546260408\n",
      "Training loss: 10524669495650.986 139 2.4643995643017864\n",
      "Training loss: 10524666363904.0 140 2.4643612251231004\n",
      "Training loss: 10524661889979.732 141 2.464323149092581\n",
      "Training loss: 10524658310840.32 142 2.4642847060064565\n",
      "Training loss: 10524654955397.12 143 2.4642462797139997\n",
      "Training loss: 10524650928865.28 144 2.4642081485966796\n",
      "Training loss: 10524648691903.146 145 2.464169955837529\n",
      "Training loss: 10524644665371.307 146 2.464131640178496\n",
      "Training loss: 10524641757320.533 147 2.4640934221438173\n",
      "Training loss: 10524638401877.334 148 2.4640552668155724\n",
      "Training loss: 10524634599041.707 149 2.4640170788816014\n",
      "Training loss: 10524631914687.146 150 2.4639788620764125\n",
      "Training loss: 10524628782940.16 151 2.4639406791127234\n",
      "Training loss: 10524625651193.174 152 2.4639025640322116\n",
      "Training loss: 10524621848357.547 153 2.4638644573020927\n",
      "Training loss: 10524618940306.773 154 2.4638263987413658\n",
      "Training loss: 10524616479648.426 155 2.4637883030843613\n",
      "Training loss: 10524613124205.227 156 2.463750237939398\n",
      "Training loss: 10524609545065.812 157 2.4637122420687585\n",
      "Training loss: 10524606189622.613 158 2.4636743552069373\n",
      "Training loss: 10524603728964.268 159 2.4636364410428184\n",
      "Training loss: 10524599926128.64 160 2.463598318955394\n",
      "Training loss: 10524596794381.654 161 2.4635601590466027\n",
      "Training loss: 10524594781115.732 162 2.4635222105296757\n",
      "Training loss: 10524591425672.533 163 2.4634843353527214\n",
      "Training loss: 10524588293925.547 164 2.4634464896076858\n",
      "Training loss: 10524585385874.773 165 2.4634086032846563\n",
      "Training loss: 10524582030431.574 166 2.463370738796412\n",
      "Training loss: 10524579569773.227 167 2.4633328051051318\n",
      "Training loss: 10524575766937.6 168 2.4632948287947936\n",
      "Training loss: 10524573753671.68 169 2.4632571398999117\n",
      "Training loss: 10524569950836.053 170 2.4632195696138712\n",
      "Training loss: 10524568608658.773 171 2.4631816234589703\n",
      "Training loss: 10524564805823.146 172 2.463143574117813\n",
      "Training loss: 10524561674076.16 173 2.4631059316090798\n",
      "Training loss: 10524559213417.812 174 2.4630683442564383\n",
      "Training loss: 10524556976455.68 175 2.46303053706933\n",
      "Training loss: 10524553173620.053 176 2.4629927439719594\n",
      "Training loss: 10524550712961.707 177 2.462955198318318\n",
      "Training loss: 10524548475999.574 178 2.4629175112272916\n",
      "Training loss: 10524544673163.947 179 2.4628797611986823\n",
      "Training loss: 10524541317720.746 180 2.462842169661464\n",
      "Training loss: 10524539304454.826 181 2.462804554333053\n",
      "Training loss: 10524536396404.053 182 2.4627669216124466\n",
      "Training loss: 10524534383138.133 183 2.462729376057267\n",
      "Training loss: 10524531251391.146 184 2.46269171316136\n",
      "Training loss: 10524528567036.586 185 2.462654209793043\n",
      "Training loss: 10524525658985.812 186 2.4626167077683556\n",
      "Training loss: 10524522527238.826 187 2.462579236333244\n",
      "Training loss: 10524520961365.334 188 2.4625416442976116\n",
      "Training loss: 10524517158529.707 189 2.462504107331576\n",
      "Training loss: 10524514697871.36 190 2.462466633494543\n",
      "Training loss: 10524512013516.8 191 2.462429320359159\n",
      "Training loss: 10524508881769.812 192 2.4623918821725352\n",
      "Training loss: 10524506868503.893 193 2.4623543104883705\n",
      "Training loss: 10524504184149.334 194 2.4623169426949607\n",
      "Training loss: 10524501052402.346 195 2.462279661830858\n",
      "Training loss: 10524499039136.426 196 2.4622421883821906\n",
      "Training loss: 10524496354781.867 197 2.4622048245386265\n",
      "Training loss: 10524493446731.094 198 2.4621675493010398\n",
      "Training loss: 10524490986072.746 199 2.4621301983896022\n",
      "Training loss: 10524488078021.973 200 2.4620928436726346\n",
      "Training loss: 10524486064756.053 201 2.462055569964494\n",
      "Training loss: 10524483604097.707 202 2.462018272885541\n",
      "Training loss: 10524480472350.72 203 2.461981038023992\n",
      "Training loss: 10524477787996.16 204 2.461943941497946\n",
      "Training loss: 10524475103641.6 205 2.461906565474389\n",
      "Training loss: 10524472195590.826 206 2.4618691419543564\n",
      "Training loss: 10524470182324.906 207 2.4618321839232555\n",
      "Training loss: 10524467274274.133 208 2.461795123849203\n",
      "Training loss: 10524465484704.426 209 2.46175779329119\n",
      "Training loss: 10524462576653.654 210 2.461720654522325\n",
      "Training loss: 10524460115995.307 211 2.461683635711198\n",
      "Training loss: 10524456984248.32 212 2.4616463692397605\n",
      "Training loss: 10524454523589.973 213 2.4616094055584674\n",
      "Training loss: 10524451615539.2 214 2.461572417548794\n",
      "Training loss: 10524449602273.28 215 2.461535113706076\n",
      "Training loss: 10524446246830.08 216 2.461498052296077\n",
      "Training loss: 10524444233564.16 217 2.461460943004698\n",
      "Training loss: 10524441549209.6 218 2.4614242559070254\n",
      "Training loss: 10524439983336.107 219 2.4613873176819157\n",
      "Training loss: 10524436851589.12 220 2.4613498181293205\n",
      "Training loss: 10524434167234.56 221 2.461312869547991\n",
      "Training loss: 10524431259183.787 222 2.4612764326630865\n",
      "Training loss: 10524429245917.867 223 2.461239357750845\n",
      "Training loss: 10524427008955.732 224 2.46120197240405\n",
      "Training loss: 10524424324601.174 225 2.4611652750097117\n",
      "Training loss: 10524422311335.254 226 2.4611286512120496\n",
      "Training loss: 10524419626980.693 227 2.4610915157172957\n",
      "Training loss: 10524416942626.133 228 2.4610546196660303\n",
      "Training loss: 10524414929360.213 229 2.4610179305030795\n",
      "Training loss: 10524412692398.08 230 2.4609809767550246\n",
      "Training loss: 10524408889562.453 231 2.460944201536422\n",
      "Training loss: 10524406205207.893 232 2.460907640801322\n",
      "Training loss: 10524404863030.613 233 2.4608706289308966\n",
      "Training loss: 10524402849764.693 234 2.4608336491798544\n",
      "Training loss: 10524399270625.28 235 2.4607971167317313\n",
      "Training loss: 10524397033663.146 236 2.460760410663723\n",
      "Training loss: 10524395467789.654 237 2.460723514309357\n",
      "Training loss: 10524392112346.453 238 2.460686820901317\n",
      "Training loss: 10524390099080.533 239 2.4606507046250043\n",
      "Training loss: 10524386743637.334 240 2.4606144229202043\n",
      "Training loss: 10524385401460.053 241 2.4605779892450816\n",
      "Training loss: 10524383835586.56 242 2.4605413824299616\n",
      "Training loss: 10524380480143.36 243 2.460504529331773\n",
      "Training loss: 10524379361662.293 244 2.46046751002345\n",
      "Training loss: 10524376229915.307 245 2.4604307565886185\n",
      "Training loss: 10524373545560.746 246 2.460394178118093\n",
      "Training loss: 10524370413813.76 247 2.4603577672345374\n",
      "Training loss: 10524368400547.84 248 2.460321322848726\n",
      "Training loss: 10524366163585.707 249 2.4602847881201084\n",
      "Training loss: 10524362808142.506 250 2.460248439674267\n",
      "Training loss: 10524360347484.16 251 2.460212261522197\n",
      "Training loss: 10524358557914.453 252 2.460176012226758\n",
      "Training loss: 10524356768344.746 253 2.4601396299001586\n",
      "Training loss: 10524354307686.4 254 2.460102886602622\n",
      "Training loss: 10524352741812.906 255 2.4600662445735755\n",
      "Training loss: 10524350057458.346 256 2.460029723167502\n",
      "Training loss: 10524347596800.0 257 2.459993427679686\n",
      "Training loss: 10524344465053.014 258 2.4599570631137397\n",
      "Training loss: 10524341780698.453 259 2.4599205267921285\n",
      "Training loss: 10524339096343.893 260 2.4598842586710514\n",
      "Training loss: 10524337306774.188 261 2.459848181829176\n",
      "Training loss: 10524334175027.2 262 2.4598119598494264\n",
      "Training loss: 10524332832849.92 263 2.459775533599522\n",
      "Training loss: 10524330372191.574 264 2.459739174995703\n",
      "Training loss: 10524328135229.44 265 2.4597028522892788\n",
      "Training loss: 10524326121963.52 266 2.4596666883228324\n",
      "Training loss: 10524322319127.893 267 2.4596307039180108\n",
      "Training loss: 10524320305861.973 268 2.4595943286473387\n",
      "Training loss: 10524318516292.268 269 2.4595576147889164\n",
      "Training loss: 10524316055633.92 270 2.4595214980019597\n",
      "Training loss: 10524313147583.146 271 2.4594859630971015\n",
      "Training loss: 10524311358013.44 272 2.459449717080286\n",
      "Training loss: 10524309121051.307 273 2.459413228370857\n",
      "Training loss: 10524306660392.96 274 2.4593772589247767\n",
      "Training loss: 10524304199734.613 275 2.4593412488111372\n",
      "Training loss: 10524301962772.48 276 2.459305009845871\n",
      "Training loss: 10524298607329.28 277 2.459269037597991\n",
      "Training loss: 10524297265152.0 278 2.4592330293424496\n",
      "Training loss: 10524294804493.654 279 2.459196874739745\n",
      "Training loss: 10524292567531.52 280 2.459160916349852\n",
      "Training loss: 10524291001658.027 281 2.459124965483081\n",
      "Training loss: 10524287646214.826 282 2.459088918782409\n",
      "Training loss: 10524286527733.76 283 2.4590528951283557\n",
      "Training loss: 10524283172290.56 284 2.459017024418895\n",
      "Training loss: 10524280711632.213 285 2.4589810780164654\n",
      "Training loss: 10524278474670.08 286 2.458945137080073\n",
      "Training loss: 10524277132492.8 287 2.458909187238494\n",
      "Training loss: 10524274224442.027 288 2.4588732782060325\n",
      "Training loss: 10524271540087.467 289 2.4588373916504875\n",
      "Training loss: 10524269750517.76 290 2.4588015746589935\n",
      "Training loss: 10524267737251.84 291 2.4587657167440757\n",
      "Training loss: 10524265276593.494 292 2.4587298870644085\n",
      "Training loss: 10524262368542.72 293 2.4586940644129833\n",
      "Training loss: 10524260355276.8 294 2.458658254515282\n",
      "Training loss: 10524258118314.666 295 2.458622438786215\n",
      "Training loss: 10524254986567.68 296 2.4585866913993932\n",
      "Training loss: 10524253196997.973 297 2.458551015983337\n",
      "Training loss: 10524251407428.268 298 2.4585153002576234\n",
      "Training loss: 10524248946769.92 299 2.4584794485404324\n",
      "Training loss: 10524247604592.64 300 2.458443673650769\n",
      "Training loss: 10524244696541.867 301 2.458408056947523\n",
      "Training loss: 10524242459579.732 302 2.4583724973650636\n",
      "Training loss: 10524239998921.387 303 2.4583368308188174\n",
      "Training loss: 10524238209351.68 304 2.458301015903702\n",
      "Training loss: 10524235301300.906 305 2.458265364311296\n",
      "Training loss: 10524232840642.56 306 2.4582298110697995\n",
      "Training loss: 10524231274769.066 307 2.458194235416566\n",
      "Training loss: 10524228590414.506 308 2.458158641613475\n",
      "Training loss: 10524225458667.52 309 2.4581230450500935\n",
      "Training loss: 10524223445401.6 310 2.4580874196154237\n",
      "Training loss: 10524221432135.68 311 2.4580519598831136\n",
      "Training loss: 10524218971477.334 312 2.4580164577869406\n",
      "Training loss: 10524216958211.414 313 2.4579808977688877\n",
      "Training loss: 10524214944945.494 314 2.457945342458373\n",
      "Training loss: 10524212484287.146 315 2.4579098056133852\n",
      "Training loss: 10524210023628.8 316 2.457874494088058\n",
      "Training loss: 10524207562970.453 317 2.4578391122762238\n",
      "Training loss: 10524205102312.107 318 2.4578034854540536\n",
      "Training loss: 10524203312742.4 319 2.457767973640795\n",
      "Training loss: 10524200852084.053 320 2.4577326131125563\n",
      "Training loss: 10524198838818.133 321 2.4576972743572205\n",
      "Training loss: 10524196601856.0 322 2.4576619630563137\n",
      "Training loss: 10524194364893.867 323 2.457626566168984\n",
      "Training loss: 10524192351627.947 324 2.4575910403381886\n",
      "Training loss: 10524190114665.812 325 2.457555744262784\n",
      "Training loss: 10524187654007.467 326 2.4575205732300653\n",
      "Training loss: 10524185417045.334 327 2.457485143504597\n",
      "Training loss: 10524182061602.133 328 2.4574498308473838\n",
      "Training loss: 10524180719424.854 329 2.457414666930733\n",
      "Training loss: 10524178929855.146 330 2.4573793010835003\n",
      "Training loss: 10524176021804.373 331 2.4573439198773444\n",
      "Training loss: 10524173784842.24 332 2.457308744555561\n",
      "Training loss: 10524171771576.32 333 2.457273619116815\n",
      "Training loss: 10524169758310.4 334 2.4572384389373774\n",
      "Training loss: 10524167297652.053 335 2.457203114770614\n",
      "Training loss: 10524165284386.133 336 2.4571677533813867\n",
      "Training loss: 10524163047424.0 337 2.4571327481261016\n",
      "Training loss: 10524160586765.654 338 2.457097658311736\n",
      "Training loss: 10524157455018.666 339 2.4570624358247843\n",
      "Training loss: 10524156783930.027 340 2.4570274258803915\n",
      "Training loss: 10524154323271.68 341 2.4569923298538012\n",
      "Training loss: 10524152310005.76 342 2.4569569562442632\n",
      "Training loss: 10524148954562.56 343 2.456921849557411\n",
      "Training loss: 10524146270208.0 344 2.4568869929117136\n",
      "Training loss: 10524144480638.293 345 2.4568518560507644\n",
      "Training loss: 10524142914764.8 346 2.4568168067361182\n",
      "Training loss: 10524140230410.24 347 2.4567819291034763\n",
      "Training loss: 10524138888232.96 348 2.4567467216919274\n",
      "Training loss: 10524135980182.188 349 2.4567114923876727\n",
      "Training loss: 10524133743220.053 350 2.4566766315804407\n",
      "Training loss: 10524132177346.56 351 2.4566420425940554\n",
      "Training loss: 10524128821903.36 352 2.4566068260341334\n",
      "Training loss: 10524127032333.654 353 2.4565715475979064\n",
      "Training loss: 10524124795371.52 354 2.456536819404389\n",
      "Training loss: 10524122558409.387 355 2.456502184152813\n",
      "Training loss: 10524120545143.467 356 2.4564669646358284\n",
      "Training loss: 10524118531877.547 357 2.456431888941684\n",
      "Training loss: 10524115847522.986 358 2.456397384510265\n",
      "Training loss: 10524113386864.64 359 2.45636253803524\n",
      "Training loss: 10524111149902.506 360 2.4563272904901834\n",
      "Training loss: 10524109584029.014 361 2.4562926002980627\n",
      "Training loss: 10524106452282.027 362 2.456258043540628\n",
      "Training loss: 10524104886408.533 363 2.4562230728513694\n",
      "Training loss: 10524102202053.973 364 2.456188175574408\n",
      "Training loss: 10524100412484.268 365 2.456153636101038\n",
      "Training loss: 10524098399218.346 366 2.4561187502611093\n",
      "Training loss: 10524096162256.213 367 2.4560837375787274\n",
      "Training loss: 10524093925294.08 368 2.45604908598918\n",
      "Training loss: 10524091912028.16 369 2.4560145103411175\n",
      "Training loss: 10524089675066.027 370 2.455979633551341\n",
      "Training loss: 10524086990711.467 371 2.4559448734885043\n",
      "Training loss: 10524084753749.334 372 2.4559102791738012\n",
      "Training loss: 10524081845698.56 373 2.455875700644724\n",
      "Training loss: 10524080279825.066 374 2.4558409301447166\n",
      "Training loss: 10524078042862.934 375 2.4558061548926178\n",
      "Training loss: 10524075805900.8 376 2.4557713893330453\n",
      "Training loss: 10524074016331.094 377 2.4557370442415087\n",
      "Training loss: 10524071555672.746 378 2.455702579588797\n",
      "Training loss: 10524069542406.826 379 2.4556675814313182\n",
      "Training loss: 10524067081748.48 380 2.4556331817943304\n",
      "Training loss: 10524064173697.707 381 2.455599163038958\n",
      "Training loss: 10524062160431.787 382 2.4555645837372855\n",
      "Training loss: 10524059923469.654 383 2.455529848399889\n",
      "Training loss: 10524058581292.373 384 2.4554953220620765\n",
      "Training loss: 10524056344330.24 385 2.455460810893562\n",
      "Training loss: 10524054107368.107 386 2.4554261298095503\n",
      "Training loss: 10524052094102.188 387 2.4553914638927488\n",
      "Training loss: 10524049409747.627 388 2.4553569801087964\n",
      "Training loss: 10524047620177.92 389 2.4553226577482286\n",
      "Training loss: 10524045159519.574 390 2.455288172975609\n",
      "Training loss: 10524042922557.44 391 2.4552537724940557\n",
      "Training loss: 10524040461899.094 392 2.4552194598553894\n",
      "Training loss: 10524038672329.387 393 2.4551850227659893\n",
      "Training loss: 10524036211671.04 394 2.455150523970036\n",
      "Training loss: 10524034422101.334 395 2.4551162182205672\n",
      "Training loss: 10524032185139.2 396 2.4550817782227248\n",
      "Training loss: 10524029724480.854 397 2.455047474283422\n",
      "Training loss: 10524027263822.506 398 2.455013003527139\n",
      "Training loss: 10524025026860.373 399 2.454978617097397\n",
      "Training loss: 10524023908379.307 400 2.454944392035515\n",
      "Training loss: 10524021000328.533 401 2.4549101201136856\n",
      "Training loss: 10524018315973.973 402 2.4548756639232594\n",
      "Training loss: 10524016750100.48 403 2.4548411935955645\n",
      "Training loss: 10524013842049.707 404 2.4548071030638825\n",
      "Training loss: 10524011381391.36 405 2.4547729429606937\n",
      "Training loss: 10524009368125.44 406 2.454738659988584\n",
      "Training loss: 10524006907467.094 407 2.454704448118441\n",
      "Training loss: 10524005341593.6 408 2.4546703042093143\n",
      "Training loss: 10524002880935.254 409 2.4546353777542076\n",
      "Training loss: 10523998630707.2 410 2.4545987309898947\n",
      "Training loss: 10523990801339.732 411 2.4545636137298668\n",
      "Training loss: 10523989459162.453 412 2.45452912624463\n",
      "Training loss: 10523987893288.96 413 2.454494977324628\n",
      "Training loss: 10523984761541.973 414 2.454460941820627\n",
      "Training loss: 10523981629794.986 415 2.454427056385685\n",
      "Training loss: 10523979616529.066 416 2.4543936192060274\n",
      "Training loss: 10523976484782.08 417 2.4543601336951792\n",
      "Training loss: 10523976932174.506 418 2.4543252469177754\n",
      "Training loss: 10523974024123.732 419 2.4542895565757257\n",
      "Training loss: 10523970892376.746 420 2.4542543132848875\n",
      "Training loss: 10523969102807.04 421 2.454219663921345\n",
      "Training loss: 10523966194756.268 422 2.454185458419055\n",
      "Training loss: 10523962615616.854 423 2.4541519863377084\n",
      "Training loss: 10523960378654.72 424 2.4541184598959633\n",
      "Training loss: 10523959036477.44 425 2.454084008710225\n",
      "Training loss: 10523957246907.732 426 2.4540489582750284\n",
      "Training loss: 10523955681034.24 427 2.4540143159583026\n",
      "Training loss: 10523952996679.68 428 2.453979718112065\n",
      "Training loss: 10523950759717.547 429 2.453945288014163\n",
      "Training loss: 10523948299059.2 430 2.4539109361783487\n",
      "Training loss: 10523945838400.854 431 2.4538765273974303\n",
      "Training loss: 10523943154046.293 432 2.4538422837447156\n",
      "Training loss: 10523941140780.373 433 2.4538081797612357\n",
      "Training loss: 10523938903818.24 434 2.453773915275051\n",
      "Training loss: 10523936666856.107 435 2.453739593169784\n",
      "Training loss: 10523934429893.973 436 2.4537057357846783\n",
      "Training loss: 10523931745539.414 437 2.453671824125201\n",
      "Training loss: 10523930179665.92 438 2.4536375092716427\n",
      "Training loss: 10523928166400.0 439 2.453602566267764\n",
      "Training loss: 10523925705741.654 440 2.4535678733360275\n",
      "Training loss: 10523924810956.8 441 2.453533520827139\n",
      "Training loss: 10523920784424.96 442 2.4534997450233726\n",
      "Training loss: 10523918771159.04 443 2.4534657760992924\n",
      "Training loss: 10523916981589.334 444 2.453432047640232\n",
      "Training loss: 10523915192019.627 445 2.453398214213962\n",
      "Training loss: 10523912731361.28 446 2.453363744504949\n",
      "Training loss: 10523910270702.934 447 2.4533290571195985\n",
      "Training loss: 10523908481133.227 448 2.453295109393314\n",
      "Training loss: 10523905349386.24 449 2.4532613643147356\n",
      "Training loss: 10523902888727.893 450 2.453227030478336\n",
      "Training loss: 10523901099158.188 451 2.4531926413022203\n",
      "Training loss: 10523898638499.84 452 2.4531586548380964\n",
      "Training loss: 10523896625233.92 453 2.4531248916953508\n",
      "Training loss: 10523893940879.36 454 2.4530907197060143\n",
      "Training loss: 10523893046094.506 455 2.453056548624211\n",
      "Training loss: 10523889690651.307 456 2.4530226535282504\n",
      "Training loss: 10523888348474.027 457 2.45298860016332\n",
      "Training loss: 10523885216727.04 458 2.4529544867231903\n",
      "Training loss: 10523883203461.12 459 2.4529206421152545\n",
      "Training loss: 10523881413891.414 460 2.4528865098551784\n",
      "Training loss: 10523878729536.854 461 2.452852517510803\n",
      "Training loss: 10523877163663.36 462 2.452818616702197\n",
      "Training loss: 10523874479308.8 463 2.4527845602466445\n",
      "Training loss: 10523872242346.666 464 2.4527510831780823\n",
      "Training loss: 10523869110599.68 465 2.4527174894833403\n",
      "Training loss: 10523867768422.4 466 2.4526836416080027\n",
      "Training loss: 10523865755156.48 467 2.4526498069880254\n",
      "Training loss: 10523864189282.986 468 2.4526156695770482\n",
      "Training loss: 10523861728624.64 469 2.4525815453078557\n",
      "Training loss: 10523859044270.08 470 2.452547662478185\n",
      "Training loss: 10523857254700.373 471 2.452513751025893\n",
      "Training loss: 10523854346649.6 472 2.4524799707287017\n",
      "Training loss: 10523852333383.68 473 2.4524461674841977\n",
      "Training loss: 10523850320117.76 474 2.4524123049459634\n",
      "Training loss: 10523847412066.986 475 2.452378499584632\n",
      "Training loss: 10523845846193.494 476 2.4523447511457492\n",
      "Training loss: 10523843161838.934 477 2.452311285381363\n",
      "Training loss: 10523841148573.014 478 2.452277430154301\n",
      "Training loss: 10523838911610.88 479 2.4522437066060787\n",
      "Training loss: 10523836674648.746 480 2.452209995027208\n",
      "Training loss: 10523834885079.04 481 2.4521761142575333\n",
      "Training loss: 10523832200724.48 482 2.4521423541568\n",
      "Training loss: 10523829963762.346 483 2.4521089498011355\n",
      "Training loss: 10523827055711.574 484 2.4520751447570834\n",
      "Training loss: 10523825266141.867 485 2.452041149996885\n",
      "Training loss: 10523823476572.16 486 2.4520074449306506\n",
      "Training loss: 10523821687002.453 487 2.4519740113243875\n",
      "Training loss: 10523819002647.893 488 2.451940384407007\n",
      "Training loss: 10523816541989.547 489 2.451906657584315\n",
      "Training loss: 10523814528723.627 490 2.4518730826409523\n",
      "Training loss: 10523812291761.494 491 2.4518394775010686\n",
      "Training loss: 10523809831103.146 492 2.451805860394436\n",
      "Training loss: 10523807370444.8 493 2.4517722083357953\n",
      "Training loss: 10523806475659.947 494 2.451738722109979\n",
      "Training loss: 10523803120216.746 495 2.4517052968380213\n",
      "Training loss: 10523800883254.613 496 2.451671549392622\n",
      "Training loss: 10523799764773.547 497 2.4516378693089127\n",
      "Training loss: 10523796185634.133 498 2.4516043805687286\n",
      "Training loss: 10523794619760.64 499 2.4515709987956726\n",
      "Training loss: 10523792382798.506 500 2.451537490211277\n",
      "Training loss: 10523790145836.373 501 2.4515040487783994\n",
      "Training loss: 10523787461481.812 502 2.451470442803925\n",
      "Training loss: 10523785895608.32 503 2.4514369081461025\n",
      "Training loss: 10523783658646.188 504 2.4514034104961366\n",
      "Training loss: 10523781197987.84 505 2.4513706622011684\n",
      "Training loss: 10523777171456.0 506 2.4513376982112036\n",
      "Training loss: 10523776724063.574 507 2.4513043147452134\n",
      "Training loss: 10523775158190.08 508 2.4512708647490205\n",
      "Training loss: 10523772921227.947 509 2.4512372639831614\n",
      "Training loss: 10523770460569.6 510 2.4512036106229416\n",
      "Training loss: 10523767999911.254 511 2.451170688795231\n",
      "Training loss: 10523767999911.254 512 2.4511368874714043\n",
      "Training loss: 10523764197075.627 513 2.451103167571759\n",
      "Training loss: 10523760170543.787 514 2.4510699055496765\n",
      "Training loss: 10523758380974.08 515 2.451036751436261\n",
      "Training loss: 10523756367708.16 516 2.451003467297378\n",
      "Training loss: 10523754130746.027 517 2.450970193868564\n",
      "Training loss: 10523751670087.68 518 2.4509372223884585\n",
      "Training loss: 10523750551606.613 519 2.4509039757159763\n",
      "Training loss: 10523747419859.627 520 2.450870509470673\n",
      "Training loss: 10523746077682.346 521 2.450837196346502\n",
      "Training loss: 10523743617024.0 522 2.4508040053594042\n",
      "Training loss: 10523740932669.44 523 2.450770628176873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1000\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, optimizer, model, criterion, X_train, y_train):\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train).cuda().float()\n",
    "        label = torch.from_numpy(y_train).reshape(-1, 1).cuda().float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_cuda_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 9891075249056.426 0 10.7251509121513\n",
      "Training loss: 9888763124995.414 1 9.513110968898115\n",
      "Training loss: 9886638905753.6 2 9.08994541346975\n",
      "Training loss: 9884793859386.027 3 8.814381308961623\n",
      "Training loss: 9882536317201.066 4 8.60472078740365\n",
      "Training loss: 9880423730162.346 5 8.435887519064742\n",
      "Training loss: 9877931754345.812 6 8.292235898539763\n",
      "Training loss: 9876116235878.4 7 8.17029190653509\n",
      "Training loss: 9873733423813.973 8 8.060406268737918\n",
      "Training loss: 9871624415914.666 9 7.958772197344482\n",
      "Training loss: 9869264868256.426 10 7.859187669851263\n",
      "Training loss: 9867055644453.547 11 7.764718278834937\n",
      "Training loss: 9864363236829.867 12 7.6751744298854225\n",
      "Training loss: 9861752254627.84 13 7.591147302621273\n",
      "Training loss: 9858782463699.627 14 7.510490638237467\n",
      "Training loss: 9856116004836.693 15 7.4324440258276265\n",
      "Training loss: 9852982468280.32 16 7.357828707437794\n",
      "Training loss: 9849639552068.268 17 7.285972121426317\n",
      "Training loss: 9846169576407.04 18 7.216968998911158\n",
      "Training loss: 9842501853293.227 19 7.149960433901611\n",
      "Training loss: 9838990717528.746 20 7.084264727590245\n",
      "Training loss: 9835130615671.467 21 7.019761753800214\n",
      "Training loss: 9830575265983.146 22 6.95731764482152\n",
      "Training loss: 9826574682903.893 23 6.896122664215079\n",
      "Training loss: 9822379931511.467 24 6.835846084804937\n",
      "Training loss: 9817314554456.746 25 6.776672815780798\n",
      "Training loss: 9812568615594.666 26 6.718697799654888\n",
      "Training loss: 9807496975045.973 27 6.661702614174397\n",
      "Training loss: 9801309537785.174 28 6.605753593021045\n",
      "Training loss: 9795616916548.268 29 6.550844961715386\n",
      "Training loss: 9789800815001.6 30 6.496685955771438\n",
      "Training loss: 9783107824298.666 31 6.443510441849817\n",
      "Training loss: 9776948125368.32 32 6.391072371508843\n",
      "Training loss: 9770031438452.053 33 6.339151923125787\n",
      "Training loss: 9762035641002.666 34 6.287753734663497\n",
      "Training loss: 9754541817856.0 35 6.236856962041708\n",
      "Training loss: 9745964410251.947 36 6.186557429638664\n",
      "Training loss: 9737860343835.307 37 6.1368583462361\n",
      "Training loss: 9728312989450.24 38 6.0876929934398145\n",
      "Training loss: 9719647892930.56 39 6.039061769922016\n",
      "Training loss: 9709458977805.654 40 5.9909099715799154\n",
      "Training loss: 9699819460580.693 41 5.943202143039085\n",
      "Training loss: 9687933138589.014 42 5.89580961344402\n",
      "Training loss: 9676697325185.707 43 5.848791821367443\n",
      "Training loss: 9664502302419.627 44 5.802205747975126\n",
      "Training loss: 9652539923715.414 45 5.756069233534321\n",
      "Training loss: 9638051567370.24 46 5.7104162139913965\n",
      "Training loss: 9624630689355.094 47 5.665181018577394\n",
      "Training loss: 9609467665230.506 48 5.620375213674698\n",
      "Training loss: 9594885356475.732 49 5.575998690395794\n",
      "Training loss: 9578169880630.613 50 5.531945871235186\n",
      "Training loss: 9562360821841.92 51 5.488333723596006\n",
      "Training loss: 9542762349199.36 52 5.445237725449587\n",
      "Training loss: 9524367362184.533 53 5.402518584183344\n",
      "Training loss: 9506860001744.213 54 5.360311903591262\n",
      "Training loss: 9486060727828.48 55 5.31854399564849\n",
      "Training loss: 9460471670592.854 56 5.277180841069672\n",
      "Training loss: 9440634290394.453 57 5.236253129726597\n",
      "Training loss: 9416141344604.16 58 5.195738771957768\n",
      "Training loss: 9391089158280.533 59 5.155654367673779\n",
      "Training loss: 9366620371681.28 60 5.115909475490111\n",
      "Training loss: 9337909410092.373 61 5.076641502821786\n",
      "Training loss: 9309377405474.133 62 5.037831700050759\n",
      "Training loss: 9283255056465.92 63 4.999450290647562\n",
      "Training loss: 9251813211504.64 64 4.961515062224078\n",
      "Training loss: 9222678121895.254 65 4.924063369729544\n",
      "Training loss: 9184822459105.28 66 4.887084987304064\n",
      "Training loss: 9152645100994.56 67 4.850580274771747\n",
      "Training loss: 9117112299683.84 68 4.814554753242303\n",
      "Training loss: 9072933192335.36 69 4.7790059300430405\n",
      "Training loss: 9037477342522.027 70 4.743924383029482\n",
      "Training loss: 8997825846531.414 71 4.709309616576264\n",
      "Training loss: 8963224516253.014 72 4.675144947204645\n",
      "Training loss: 8918999774877.014 73 4.641422679900158\n",
      "Training loss: 8876977993809.92 74 4.608126936632795\n",
      "Training loss: 8818553911596.373 75 4.575294395954962\n",
      "Training loss: 8769832876332.373 76 4.542923434706994\n",
      "Training loss: 8719832298728.106 77 4.510986486701407\n",
      "Training loss: 8669943569230.507 78 4.4794974240203045\n",
      "Training loss: 8622171900695.894 79 4.4484766103467015\n",
      "Training loss: 8559251524594.347 80 4.417966755415979\n",
      "Training loss: 8505788129607.68 81 4.387935527271344\n",
      "Training loss: 8448215882574.507 82 4.3583722701643755\n",
      "Training loss: 8385381405818.88 83 4.329285092389\n",
      "Training loss: 8316825674711.04 84 4.300683072036482\n",
      "Training loss: 8254060991173.974 85 4.272537572812726\n",
      "Training loss: 8189746540270.934 86 4.244867129777524\n",
      "Training loss: 8122365661675.52 87 4.217683163532178\n",
      "Training loss: 8045857977576.106 88 4.190975758553104\n",
      "Training loss: 7979310143679.146 89 4.164750802411827\n",
      "Training loss: 7909741516117.333 90 4.13905151469713\n",
      "Training loss: 7824677899250.347 91 4.1138093426935205\n",
      "Training loss: 7726117347655.68 92 4.0890172920868455\n",
      "Training loss: 7665966330675.2 93 4.064678694954628\n",
      "Training loss: 7577424685083.307 94 4.040816121545537\n",
      "Training loss: 7498394602482.347 95 4.017455507395008\n",
      "Training loss: 7414443650580.48 96 3.994566858894477\n",
      "Training loss: 7323528588165.12 97 3.9721365966801447\n",
      "Training loss: 7234042049768.106 98 3.9501938547905806\n",
      "Training loss: 7134820252166.827 99 3.928717331247159\n",
      "Training loss: 7042582697738.24 100 3.9077139082833394\n",
      "Training loss: 6945460960187.733 101 3.8871813664435457\n",
      "Training loss: 6858800152357.547 102 3.8671345041233893\n",
      "Training loss: 6752582926772.906 103 3.847554791691204\n",
      "Training loss: 6634078962974.72 104 3.8284210979042483\n",
      "Training loss: 6544663559973.547 105 3.8097416256771193\n",
      "Training loss: 6451993824460.8 106 3.791515708798813\n",
      "Training loss: 6340781462432.427 107 3.7737505175380357\n",
      "Training loss: 6249521907848.533 108 3.7564240644438636\n",
      "Training loss: 6140066903272.106 109 3.7395374888047765\n",
      "Training loss: 6025250347854.507 110 3.7230959148212595\n",
      "Training loss: 5927812303858.347 111 3.7071083327013348\n",
      "Training loss: 5820884171707.733 112 3.691570555488887\n",
      "Training loss: 5722550448073.387 113 3.676471318789337\n",
      "Training loss: 5619413071953.92 114 3.661812724716858\n",
      "Training loss: 5514229770267.307 115 3.6475880590124468\n",
      "Training loss: 5406004200079.36 116 3.6337971800292515\n",
      "Training loss: 5281841616868.693 117 3.620429612376633\n",
      "Training loss: 5185131927743.146 118 3.6074805959870715\n",
      "Training loss: 5082466103241.387 119 3.5949400811071635\n",
      "Training loss: 4998532152251.733 120 3.582810970855548\n",
      "Training loss: 4909995875368.96 121 3.5710946289458647\n",
      "Training loss: 4789208420625.066 122 3.5597762761175558\n",
      "Training loss: 4702358813190.827 123 3.5488634596186404\n",
      "Training loss: 4613563496093.014 124 3.5383522999648926\n",
      "Training loss: 4505288712738.134 125 3.5282326200070115\n",
      "Training loss: 4420663093056.854 126 3.5184943341610158\n",
      "Training loss: 4345603170850.1333 127 3.5091417091249415\n",
      "Training loss: 4261154271177.3867 128 3.50016712913919\n",
      "Training loss: 4185051477224.1064 129 3.491568153289583\n",
      "Training loss: 4114497244146.3467 130 3.4833371840982164\n",
      "Training loss: 4051782668561.067 131 3.4754689869178024\n",
      "Training loss: 3983339232952.32 132 3.4679546257747513\n",
      "Training loss: 3937724443306.6665 133 3.460789975017597\n",
      "Training loss: 3870665687258.453 134 3.45397986400898\n",
      "Training loss: 3828022478110.72 135 3.447507515707252\n",
      "Training loss: 3772924758589.44 136 3.4413667238772474\n",
      "Training loss: 3744301709612.3735 137 3.435552198726252\n",
      "Training loss: 3695106662072.32 138 3.430058577074884\n",
      "Training loss: 3660403326320.64 139 3.424881942899038\n",
      "Training loss: 3620059266853.547 140 3.420008399049834\n",
      "Training loss: 3614882041692.16 141 3.415435718055033\n",
      "Training loss: 3574882027001.1733 142 3.411149485274546\n",
      "Training loss: 3580034869275.3066 143 3.407143790613419\n",
      "Training loss: 3571649840414.72 144 3.403412863060975\n",
      "Training loss: 3522277849169.92 145 3.399949006562145\n",
      "Training loss: 3528563489068.3735 146 3.39674563735591\n",
      "Training loss: 3519206276464.64 147 3.393792515686822\n",
      "Training loss: 3518090703448.7466 148 3.391083320217236\n",
      "Training loss: 3511439767633.92 149 3.388617395557159\n",
      "Training loss: 3513897965322.24 150 3.38638526783603\n",
      "Training loss: 3513883201372.16 151 3.3843807061036717\n",
      "Training loss: 3503923798562.1333 152 3.3825968458642586\n",
      "Training loss: 3517858506779.3066 153 3.381020494079531\n",
      "Training loss: 3521718161244.16 154 3.379647057582674\n",
      "Training loss: 3511166858253.6533 155 3.378464568511728\n",
      "Training loss: 3530658627802.453 156 3.377451611530358\n",
      "Training loss: 3520675960586.24 157 3.3766061601039703\n",
      "Training loss: 3528671086946.987 158 3.375915125978654\n",
      "Training loss: 3556136060627.6265 159 3.3753674582210635\n",
      "Training loss: 3519596850053.12 160 3.3749407011209738\n",
      "Training loss: 3514051420924.5864 161 3.3746190256966906\n",
      "Training loss: 3516612742567.2534 162 3.3743956540273192\n",
      "Training loss: 3512041063055.36 163 3.3742637025536935\n",
      "Training loss: 3536063799405.2266 164 3.374221389091339\n",
      "Training loss: 3528658559959.04 165 3.3742509776833463\n",
      "Training loss: 3535216661845.3335 166 3.374337970504317\n",
      "Training loss: 3521859537250.987 167 3.374472859026563\n",
      "Training loss: 3507129812691.6265 168 3.374645808377232\n",
      "Training loss: 3514398150055.2534 169 3.37484240999682\n",
      "Training loss: 3505619863251.6265 170 3.3750582334253862\n",
      "Training loss: 3527489523548.16 171 3.375290382938675\n",
      "Training loss: 3508529927290.88 172 3.375531787492675\n",
      "Training loss: 3511240006915.4136 173 3.375768134297583\n",
      "Training loss: 3526826040579.4136 174 3.3759973720562684\n",
      "Training loss: 3518859994726.4 175 3.3762150760178593\n",
      "Training loss: 3507923263160.32 176 3.376412569319158\n",
      "Training loss: 3530034067974.8267 177 3.3765909464688426\n",
      "Training loss: 3521104338834.7734 178 3.3767434420232636\n",
      "Training loss: 3529158968388.2666 179 3.3768680973375163\n",
      "Training loss: 3515035684263.2534 180 3.3769633771756222\n",
      "Training loss: 3511688965215.573 181 3.3770319067509096\n",
      "Training loss: 3536395540889.6 182 3.37706943438845\n",
      "Training loss: 3514157229233.493 183 3.3770720956150497\n",
      "Training loss: 3518771634722.1333 184 3.3770400601245067\n",
      "Training loss: 3516251920575.1465 185 3.3769743046296665\n",
      "Training loss: 3505652075506.3467 186 3.376867994852021\n",
      "Training loss: 3510936227457.7065 187 3.3767209818683117\n",
      "Training loss: 3514538631277.2266 188 3.376542141568048\n",
      "Training loss: 3501503181837.6533 189 3.3763277496074457\n",
      "Training loss: 3505102230213.973 190 3.3760805784417722\n",
      "Training loss: 3516692602115.4136 191 3.375799923743081\n",
      "Training loss: 3501531143864.32 192 3.375488666565248\n",
      "Training loss: 3496977807441.92 193 3.375151535268242\n",
      "Training loss: 3501035209359.36 194 3.3747998314956544\n",
      "Training loss: 3517338636779.52 195 3.3744270468939015\n",
      "Training loss: 3497818681507.84 196 3.374029508294646\n",
      "Training loss: 3497754256998.4 197 3.373615758543461\n",
      "Training loss: 3508874643155.6265 198 3.373183673584279\n",
      "Training loss: 3524605631965.8667 199 3.3727376650407286\n",
      "Training loss: 3519376509282.987 200 3.372281434516713\n",
      "Training loss: 3539144767351.467 201 3.371815586906589\n",
      "Training loss: 3505213407232.0 202 3.3713369772183874\n",
      "Training loss: 3514390768080.2134 203 3.370856679808846\n",
      "Training loss: 3507144800337.92 204 3.370371975341156\n",
      "Training loss: 3501113950426.453 205 3.369879055293238\n",
      "Training loss: 3528436429619.2 206 3.3693829549768255\n",
      "Training loss: 3509381538775.04 207 3.3688885846879186\n",
      "Training loss: 3494874839340.3735 208 3.368401575092682\n",
      "Training loss: 3504909404078.08 209 3.3679121928552203\n",
      "Training loss: 3527289986525.8667 210 3.367431199103359\n",
      "Training loss: 3501473430241.28 211 3.3669541180602884\n",
      "Training loss: 3523431674238.2935 212 3.366486690240335\n",
      "Training loss: 3527076804034.56 213 3.3660198977799345\n",
      "Training loss: 3510500467234.1333 214 3.365560713250636\n",
      "Training loss: 3509981715715.4136 215 3.365108296825585\n",
      "Training loss: 3515369662709.76 216 3.3646626599839498\n",
      "Training loss: 3503156744246.6133 217 3.3642194769482106\n",
      "Training loss: 3520747767070.72 218 3.3637802201309195\n",
      "Training loss: 3524969362008.7466 219 3.363345705257986\n",
      "Training loss: 3514256774048.427 220 3.3629180035734287\n",
      "Training loss: 3509703885018.453 221 3.362497048017334\n",
      "Training loss: 3504082175481.1733 222 3.3620782926185138\n",
      "Training loss: 3505386995493.547 223 3.36166396979516\n",
      "Training loss: 3519157286993.92 224 3.361254759098803\n",
      "Training loss: 3512754653975.8936 225 3.360852414320449\n",
      "Training loss: 3519104718383.7866 226 3.360451537154575\n",
      "Training loss: 3502131768197.12 227 3.360049167436307\n",
      "Training loss: 3503253381010.7734 228 3.3596511312671313\n",
      "Training loss: 3519123732561.92 229 3.359256133004669\n",
      "Training loss: 3508814245178.027 230 3.3588690836057156\n",
      "Training loss: 3500290077272.7466 231 3.358485552453784\n",
      "Training loss: 3526954218509.6533 232 3.3581103461317547\n",
      "Training loss: 3514483602008.7466 233 3.357734531271916\n",
      "Training loss: 3499863935986.3467 234 3.3573592141492847\n",
      "Training loss: 3501071224449.7065 235 3.356976077589802\n",
      "Training loss: 3504907614508.3735 236 3.356588117942936\n",
      "Training loss: 3514144702245.547 237 3.3562026571646246\n",
      "Training loss: 3523154514629.973 238 3.3558186659439437\n",
      "Training loss: 3494550703527.2534 239 3.355433516094802\n",
      "Training loss: 3510000729893.547 240 3.3550480063792216\n",
      "Training loss: 3510057996124.16 241 3.354655126071669\n",
      "Training loss: 3526771011310.933 242 3.3542703306788937\n",
      "Training loss: 3477205522841.6 243 3.3538779150730065\n",
      "Training loss: 3516604689503.573 244 3.353486851825249\n",
      "Training loss: 3501465824570.027 245 3.353094981067937\n",
      "Training loss: 3527540302588.5864 246 3.352702187329569\n",
      "Training loss: 3498506323667.6265 247 3.3523065459067936\n",
      "Training loss: 3502840214104.7466 248 3.3519148987010983\n",
      "Training loss: 3508538651443.2 249 3.3515223072329285\n",
      "Training loss: 3520176670638.08 250 3.35112795813088\n",
      "Training loss: 3528783382446.08 251 3.350740592342089\n",
      "Training loss: 3526178887434.24 252 3.350357480592733\n",
      "Training loss: 3513279221596.16 253 3.349965695687242\n",
      "Training loss: 3539097343754.24 254 3.3495690611926254\n",
      "Training loss: 3503770342959.7866 255 3.3491645817338145\n",
      "Training loss: 3522762151471.7866 256 3.3487491133774667\n",
      "Training loss: 3511650936859.3066 257 3.34833306906519\n",
      "Training loss: 3502961681148.5864 258 3.347919308867459\n",
      "Training loss: 3519124403650.56 259 3.347506620675836\n",
      "Training loss: 3516777382980.2666 260 3.3470872036480164\n",
      "Training loss: 3513964403097.6 261 3.3466686900492837\n",
      "Training loss: 3503855123824.64 262 3.346253003736029\n",
      "Training loss: 3517603940488.533 263 3.3458385489311535\n",
      "Training loss: 3518213288973.6533 264 3.345428032396399\n",
      "Training loss: 3491506198063.7866 265 3.345018258508466\n",
      "Training loss: 3532491147182.08 266 3.3446079233723522\n",
      "Training loss: 3531926090547.2 267 3.344207274003547\n",
      "Training loss: 3519011213366.6133 268 3.343807128257202\n",
      "Training loss: 3505100440644.2666 269 3.343403105468733\n",
      "Training loss: 3511847118438.4 270 3.34298858363981\n",
      "Training loss: 3513827724711.2534 271 3.342573156164512\n",
      "Training loss: 3491795437267.6265 272 3.3421533818346725\n",
      "Training loss: 3512917504819.2 273 3.3417299655136383\n",
      "Training loss: 3529556029166.933 274 3.341306582939147\n",
      "Training loss: 3526219823841.28 275 3.3408830446370343\n",
      "Training loss: 3521111944506.027 276 3.340465726189658\n",
      "Training loss: 3528639098388.48 277 3.340046869105302\n",
      "Training loss: 3505141153355.0933 278 3.339625097539685\n",
      "Training loss: 3493197341436.5864 279 3.339200376785632\n",
      "Training loss: 3500049380147.2 280 3.3387744835951634\n",
      "Training loss: 3494311795971.4136 281 3.33833964469498\n",
      "Training loss: 3502615846802.7734 282 3.337905568791806\n",
      "Training loss: 3493584559581.8667 283 3.3374657216074315\n",
      "Training loss: 3529374835234.1333 284 3.337029552735242\n",
      "Training loss: 3502000011127.467 285 3.3365899489576205\n",
      "Training loss: 3505866152782.507 286 3.3361569684941252\n",
      "Training loss: 3509703885018.453 287 3.335723122034943\n",
      "Training loss: 3521499610043.7334 288 3.335292069255139\n",
      "Training loss: 3513550565102.933 289 3.334857147643129\n",
      "Training loss: 3507981200479.573 290 3.3344228812950543\n",
      "Training loss: 3506867193337.1733 291 3.333993349590072\n",
      "Training loss: 3494339981694.2935 292 3.333568259345484\n",
      "Training loss: 3521255110082.56 293 3.3331428613260754\n",
      "Training loss: 3524657976879.7866 294 3.3327191916477124\n",
      "Training loss: 3498559339670.1865 295 3.332292989894315\n",
      "Training loss: 3528316975841.28 296 3.3318695997000427\n",
      "Training loss: 3506835204778.6665 297 3.3314570188199317\n",
      "Training loss: 3524080616953.1733 298 3.3310481556018665\n",
      "Training loss: 3518093387803.3066 299 3.3306434997595438\n",
      "Training loss: 3522629275921.067 300 3.33024406220368\n",
      "Training loss: 3531956736928.427 301 3.329848475146639\n",
      "Training loss: 3514949337524.9067 302 3.329459370097193\n",
      "Training loss: 3501325343348.053 303 3.3290659280438746\n",
      "Training loss: 3530902232978.7734 304 3.3286689625857235\n",
      "Training loss: 3507302506168.32 305 3.3282677928665607\n",
      "Training loss: 3503057423127.8936 306 3.3278683820404864\n",
      "Training loss: 3518600730815.1465 307 3.327466243734187\n",
      "Training loss: 3499312972212.9067 308 3.327061351545446\n",
      "Training loss: 3487303617303.8936 309 3.326646691979924\n",
      "Training loss: 3522914712289.28 310 3.3262275349683925\n",
      "Training loss: 3501621293438.2935 311 3.325806528139424\n",
      "Training loss: 3498010836555.0933 312 3.3253869942296768\n",
      "Training loss: 3503071963381.76 313 3.324967341458687\n",
      "Training loss: 3527272314525.013 314 3.324549189094205\n",
      "Training loss: 3496328417334.6133 315 3.324117132421471\n",
      "Training loss: 3498172345221.12 316 3.3236815220259848\n",
      "Training loss: 3497119854537.3867 317 3.3232384906111587\n",
      "Training loss: 3501587962702.507 318 3.32280029187683\n",
      "Training loss: 3524607868928.0 319 3.3223683644339457\n",
      "Training loss: 3515904520355.84 320 3.321932085618732\n",
      "Training loss: 3496592826258.7734 321 3.321497648810832\n",
      "Training loss: 3498840302114.1333 322 3.3210723353197915\n",
      "Training loss: 3501449047354.027 323 3.320641149962547\n",
      "Training loss: 3513863739801.6 324 3.3202145929495512\n",
      "Training loss: 3504369625115.3066 325 3.3197869414393204\n",
      "Training loss: 3499535326248.96 326 3.3193533218483555\n",
      "Training loss: 3512653095895.04 327 3.3189234433450454\n",
      "Training loss: 3528094621805.2266 328 3.318492341760364\n",
      "Training loss: 3514284288682.6665 329 3.3180602788332947\n",
      "Training loss: 3515168559813.973 330 3.31763010613873\n",
      "Training loss: 3487919452979.2 331 3.3171954717340557\n",
      "Training loss: 3508404657411.4136 332 3.3167601526045183\n",
      "Training loss: 3502923876488.533 333 3.316331723231519\n",
      "Training loss: 3494041570945.7065 334 3.3159073735979634\n",
      "Training loss: 3511500165611.52 335 3.315485645479525\n",
      "Training loss: 3529559160913.92 336 3.315071902395398\n",
      "Training loss: 3491015855964.16 337 3.3146595090384334\n",
      "Training loss: 3535758006681.6 338 3.3142477274308346\n",
      "Training loss: 3491652271691.0933 339 3.313835662953974\n",
      "Training loss: 3528671086946.987 340 3.3134278616734583\n",
      "Training loss: 3512745706127.36 341 3.313026196586869\n",
      "Training loss: 3510304061958.8267 342 3.3126272513706927\n",
      "Training loss: 3522751414053.547 343 3.3122263120166737\n",
      "Training loss: 3508780690746.027 344 3.3118222160525557\n",
      "Training loss: 3503231906174.2935 345 3.3114099773211785\n",
      "Training loss: 3508582943293.44 346 3.3109972183885743\n",
      "Training loss: 3498452412880.2134 347 3.3105871094612005\n",
      "Training loss: 3517136862795.0933 348 3.310177283653996\n",
      "Training loss: 3525139371130.88 349 3.3097575171528484\n",
      "Training loss: 3510394435229.013 350 3.3093367681292034\n",
      "Training loss: 3507819468117.3335 351 3.3089121247201416\n",
      "Training loss: 3498669845599.573 352 3.308492720405428\n",
      "Training loss: 3521724424738.1333 353 3.3080649667433155\n",
      "Training loss: 3525742232425.8135 354 3.307646380734799\n",
      "Training loss: 3517477552128.0 355 3.307221147082736\n",
      "Training loss: 3512695374479.36 356 3.3068027749390634\n",
      "Training loss: 3496815180294.8267 357 3.306365092937319\n",
      "Training loss: 3499000468602.88 358 3.3059299225859853\n",
      "Training loss: 3498895107686.4 359 3.3055025262615674\n",
      "Training loss: 3496849405815.467 360 3.3050774486656582\n",
      "Training loss: 3503782198859.0933 361 3.304648247742141\n",
      "Training loss: 3518474342454.6133 362 3.304220533259638\n",
      "Training loss: 3522959451531.947 363 3.303798108049734\n",
      "Training loss: 3522368446136.32 364 3.3033831062687047\n",
      "Training loss: 3508463042123.0933 365 3.3029627139704014\n",
      "Training loss: 3508382958878.72 366 3.3025456027865934\n",
      "Training loss: 3543808609703.2534 367 3.3021248194625104\n",
      "Training loss: 3513695743945.3867 368 3.301711970436852\n",
      "Training loss: 3507363127842.1333 369 3.3012961497527424\n",
      "Training loss: 3511369079630.507 370 3.3008766267895235\n",
      "Training loss: 3521243701575.68 371 3.300465360946316\n",
      "Training loss: 3513780301114.027 372 3.3000585509430778\n",
      "Training loss: 3503269039745.7065 373 3.299647092061604\n",
      "Training loss: 3502566186243.4136 374 3.2992271999261487\n",
      "Training loss: 3520312901632.0 375 3.2988099757989553\n",
      "Training loss: 3520305743353.1733 376 3.2983976595739812\n",
      "Training loss: 3524109026372.2666 377 3.2979843874866455\n",
      "Training loss: 3508899697131.52 378 3.2975752745358653\n",
      "Training loss: 3488439994067.6265 379 3.2971656519640598\n",
      "Training loss: 3505311833565.8667 380 3.2967551930722734\n",
      "Training loss: 3516438259520.8535 381 3.2963402709462315\n",
      "Training loss: 3542446299764.053 382 3.2959301319440986\n",
      "Training loss: 3508514939644.5864 383 3.295516160018033\n",
      "Training loss: 3520011806528.8535 384 3.2951043745539264\n",
      "Training loss: 3507214146164.053 385 3.2946871448470247\n",
      "Training loss: 3520538611111.2534 386 3.294267962935643\n",
      "Training loss: 3511278258967.8936 387 3.293843656252713\n",
      "Training loss: 3512797827345.067 388 3.2934082934461233\n",
      "Training loss: 3502300435141.973 389 3.2929781374242975\n",
      "Training loss: 3507448579795.6265 390 3.2925587337686193\n",
      "Training loss: 3521270097728.8535 391 3.292132110310527\n",
      "Training loss: 3503875703876.2666 392 3.2917000933149545\n",
      "Training loss: 3515573897352.533 393 3.2912692820192126\n",
      "Training loss: 3486318682876.5864 394 3.2908392700353803\n",
      "Training loss: 3515021144009.3867 395 3.2904225368651687\n",
      "Training loss: 3495740096293.547 396 3.290000401945214\n",
      "Training loss: 3499718980840.1064 397 3.2895900268714486\n",
      "Training loss: 3508190132742.8267 398 3.2891817863964787\n",
      "Training loss: 3498748362970.453 399 3.2887716813533396\n",
      "Training loss: 3495591785704.1064 400 3.288362174046126\n",
      "Training loss: 3511354539376.64 401 3.2879488967010477\n",
      "Training loss: 3509806785276.5864 402 3.2875298608293684\n",
      "Training loss: 3515782382223.36 403 3.2871097747185405\n",
      "Training loss: 3509061205797.547 404 3.286692812981033\n",
      "Training loss: 3501632030856.533 405 3.2862807687971376\n",
      "Training loss: 3514411795524.2666 406 3.285873710929415\n",
      "Training loss: 3525130646978.56 407 3.2854688197248634\n",
      "Training loss: 3515240589994.6665 408 3.2850645137642087\n",
      "Training loss: 3503477524616.533 409 3.284655204991389\n",
      "Training loss: 3524034535533.2266 410 3.284245964002111\n",
      "Training loss: 3505372678935.8936 411 3.283831691390935\n",
      "Training loss: 3532401445000.533 412 3.2834093613594146\n",
      "Training loss: 3513871345472.8535 413 3.282979786033361\n",
      "Training loss: 3524076590421.3335 414 3.282553001908241\n",
      "Training loss: 3538180860368.2134 415 3.282129461534941\n",
      "Training loss: 3507616351955.6265 416 3.2817132165267724\n",
      "Training loss: 3516355044529.493 417 3.2812965724474776\n",
      "Training loss: 3505557004615.68 418 3.2808776323378672\n",
      "Training loss: 3510154409192.1064 419 3.28046309957474\n",
      "Training loss: 3492022265227.947 420 3.2800505297710587\n",
      "Training loss: 3500969890065.067 421 3.2796362969914084\n",
      "Training loss: 3516907126784.0 422 3.2792268144848955\n",
      "Training loss: 3512206821949.44 423 3.2788177329068358\n",
      "Training loss: 3498352420672.8535 424 3.278403596861641\n",
      "Training loss: 3532571006730.24 425 3.277988998775335\n",
      "Training loss: 3529537014988.8 426 3.2775793914345384\n",
      "Training loss: 3514814672404.48 427 3.2771688464337028\n",
      "Training loss: 3534685159642.453 428 3.276756184212292\n",
      "Training loss: 3501810316738.56 429 3.276340129042883\n",
      "Training loss: 3513650109917.8667 430 3.2759181843071503\n",
      "Training loss: 3489466983383.04 431 3.275492421600239\n",
      "Training loss: 3539460179012.2666 432 3.2750721318612164\n",
      "Training loss: 3519129772359.68 433 3.274653737579731\n",
      "Training loss: 3509384223129.6 434 3.274237253363689\n",
      "Training loss: 3535727583996.5864 435 3.273829380199988\n",
      "Training loss: 3517478670609.067 436 3.2734246330253574\n",
      "Training loss: 3513070289332.9067 437 3.273023142770768\n",
      "Training loss: 3548543587450.88 438 3.2726222188031464\n",
      "Training loss: 3502093963537.067 439 3.272212614593914\n",
      "Training loss: 3498870501102.933 440 3.2718021966543542\n",
      "Training loss: 3508657657828.6934 441 3.2713981703027337\n",
      "Training loss: 3501029840650.24 442 3.270989857934165\n",
      "Training loss: 3511350065452.3735 443 3.270577915942745\n",
      "Training loss: 3511710440052.053 444 3.2701624246985213\n",
      "Training loss: 3518310596826.453 445 3.269737598804466\n",
      "Training loss: 3525160398574.933 446 3.2693090869351034\n",
      "Training loss: 3507843403612.16 447 3.2688796221115513\n",
      "Training loss: 3527051973754.88 448 3.268456623297708\n",
      "Training loss: 3498296049227.0933 449 3.268031976312345\n",
      "Training loss: 3499447861029.547 450 3.267607159877691\n",
      "Training loss: 3512948822289.067 451 3.2671858314054982\n",
      "Training loss: 3493284359263.573 452 3.2667674314678883\n",
      "Training loss: 3495457567976.1064 453 3.2663439566732895\n",
      "Training loss: 3500492074953.3867 454 3.2659177646289757\n",
      "Training loss: 3531082532126.72 455 3.2654973799644123\n",
      "Training loss: 3542557924174.507 456 3.2650818523462144\n",
      "Training loss: 3495866484654.08 457 3.2646713800903377\n",
      "Training loss: 3519807124493.6533 458 3.264263125503068\n",
      "Training loss: 3518252212114.7734 459 3.2638511279867064\n",
      "Training loss: 3527552829576.533 460 3.2634474586775117\n",
      "Training loss: 3498317076671.1465 461 3.263044449808061\n",
      "Training loss: 3508404210018.987 462 3.262638253870856\n",
      "Training loss: 3505503541220.6934 463 3.2622292270460838\n",
      "Training loss: 3510439845560.32 464 3.2618135446766185\n",
      "Training loss: 3532514187892.053 465 3.261395708917006\n",
      "Training loss: 3500527195258.88 466 3.2609729501226257\n",
      "Training loss: 3519460395362.987 467 3.2605545700394503\n",
      "Training loss: 3502340029371.7334 468 3.2601360711206953\n",
      "Training loss: 3504767356982.6133 469 3.2597231001925615\n",
      "Training loss: 3495075718539.947 470 3.259313402429308\n",
      "Training loss: 3505958539318.6133 471 3.258903921134948\n",
      "Training loss: 3505483632257.7065 472 3.258492634947854\n",
      "Training loss: 3516001604512.427 473 3.258081573993205\n",
      "Training loss: 3493795281414.8267 474 3.2576724345796912\n",
      "Training loss: 3505928787722.24 475 3.257267191212093\n",
      "Training loss: 3503341517318.8267 476 3.256863871016062\n",
      "Training loss: 3495811679081.8135 477 3.256459706995273\n",
      "Training loss: 3521210147143.68 478 3.2560518858897756\n",
      "Training loss: 3517796766624.427 479 3.255645386681535\n",
      "Training loss: 3511947110645.76 480 3.2552396678908133\n",
      "Training loss: 3501214390026.24 481 3.2548398398893292\n",
      "Training loss: 3508822745634.1333 482 3.2544280592688812\n",
      "Training loss: 3522741347723.947 483 3.2540221992529457\n",
      "Training loss: 3500094119389.8667 484 3.253609391224991\n",
      "Training loss: 3511316511020.3735 485 3.2531915925790815\n",
      "Training loss: 3526227429512.533 486 3.2527833880906667\n",
      "Training loss: 3477767224033.28 487 3.2523764302230327\n",
      "Training loss: 3527285288905.3867 488 3.251975231296259\n",
      "Training loss: 3499331315302.4 489 3.25157401260086\n",
      "Training loss: 3515222694297.6 490 3.2511760799547407\n",
      "Training loss: 3477411770750.2935 491 3.250773103242552\n",
      "Training loss: 3521712568838.8267 492 3.2503651407171956\n",
      "Training loss: 3483003281298.7734 493 3.24995619131004\n",
      "Training loss: 3500516457840.64 494 3.2495510676582304\n",
      "Training loss: 3507151734920.533 495 3.2491359040976517\n",
      "Training loss: 3537131948823.8936 496 3.2487240147150236\n",
      "Training loss: 3520633682001.92 497 3.248311779927365\n",
      "Training loss: 3514365266711.8936 498 3.2478892197743905\n",
      "Training loss: 3509662277522.7734 499 3.2474574997331596\n",
      "Training loss: 3498633159420.5864 500 3.2470245408617857\n",
      "Training loss: 3500502364979.2 501 3.246586277138563\n",
      "Training loss: 3505285437412.6934 502 3.2461510588521683\n",
      "Training loss: 3521235648512.0 503 3.2457201297892593\n",
      "Training loss: 3524903371625.8135 504 3.245297830062474\n",
      "Training loss: 3527828199615.1465 505 3.2448747245193603\n",
      "Training loss: 3521403644368.2134 506 3.244455769184546\n",
      "Training loss: 3507678986895.36 507 3.244036597911611\n",
      "Training loss: 3523807260180.48 508 3.2436305849770175\n",
      "Training loss: 3516621466719.573 509 3.2432237558874983\n",
      "Training loss: 3504727762752.8535 510 3.242818910228423\n",
      "Training loss: 3502725010554.88 511 3.2424098680893776\n",
      "Training loss: 3486677044210.3467 512 3.242001646060707\n",
      "Training loss: 3514959627550.72 513 3.2415872852051377\n",
      "Training loss: 3513042103610.027 514 3.241167186447754\n",
      "Training loss: 3505327044908.3735 515 3.240740499122942\n",
      "Training loss: 3512723336506.027 516 3.2403190751908677\n",
      "Training loss: 3530065832837.12 517 3.2399025288124155\n",
      "Training loss: 3525605554039.467 518 3.239483722188019\n",
      "Training loss: 3507788821736.1064 519 3.2390655509213464\n",
      "Training loss: 3514139333536.427 520 3.2386504632558792\n",
      "Training loss: 3531165970814.2935 521 3.238231434630324\n",
      "Training loss: 3504638507963.7334 522 3.237812061710755\n",
      "Training loss: 3489376162720.427 523 3.237389742760462\n",
      "Training loss: 3511084314350.933 524 3.2369618766141954\n",
      "Training loss: 3529798515862.1865 525 3.2365383915500554\n",
      "Training loss: 3519417669386.24 526 3.2361150623828965\n",
      "Training loss: 3513652123183.7866 527 3.2356853995100128\n",
      "Training loss: 3502533973988.6934 528 3.235250294695925\n",
      "Training loss: 3523989796290.56 529 3.234814305784269\n",
      "Training loss: 3479799056738.987 530 3.234370999110092\n",
      "Training loss: 3511187438305.28 531 3.2339288456883373\n",
      "Training loss: 3540576423116.8 532 3.2334931788513597\n",
      "Training loss: 3502154585210.88 533 3.2330715736023192\n",
      "Training loss: 3517495000432.64 534 3.2326527057159775\n",
      "Training loss: 3523030139535.36 535 3.232237396272281\n",
      "Training loss: 3522147434277.547 536 3.231824229079996\n",
      "Training loss: 3506188722722.1333 537 3.2314167062683286\n",
      "Training loss: 3498408568422.4 538 3.2310092756801283\n",
      "Training loss: 3520548901137.067 539 3.2306070396239157\n",
      "Training loss: 3513229784733.013 540 3.2302094549202125\n",
      "Training loss: 3511495020598.6133 541 3.2298119652526514\n",
      "Training loss: 3527689507962.88 542 3.229416535891857\n",
      "Training loss: 3504754829994.6665 543 3.2290252805984405\n",
      "Training loss: 3525411609422.507 544 3.2286296905094907\n",
      "Training loss: 3521007702070.6133 545 3.2282397516052557\n",
      "Training loss: 3496783415432.533 546 3.2278431890617556\n",
      "Training loss: 3514787605162.6665 547 3.227449238584662\n",
      "Training loss: 3514161703157.76 548 3.2270556872852407\n",
      "Training loss: 3516716313914.027 549 3.226649137237429\n",
      "Training loss: 3532858456364.3735 550 3.2262469110638814\n",
      "Training loss: 3531780240616.1064 551 3.2258430328524885\n",
      "Training loss: 3506896273844.9067 552 3.2254381397481793\n",
      "Training loss: 3511618500908.3735 553 3.225032241810439\n",
      "Training loss: 3500130358176.427 554 3.2246276252477446\n",
      "Training loss: 3511587407134.72 555 3.224222454063279\n",
      "Training loss: 3515998472765.44 556 3.2238169188906025\n",
      "Training loss: 3509473254222.507 557 3.223409186270546\n",
      "Training loss: 3491554963838.2935 558 3.2230062277917377\n",
      "Training loss: 3519614298357.76 559 3.2226077270985236\n",
      "Training loss: 3510809391704.7466 560 3.22220872166025\n",
      "Training loss: 3499889437354.6665 561 3.221814848433667\n",
      "Training loss: 3497441529692.16 562 3.2214159274436223\n",
      "Training loss: 3496108523956.9067 563 3.221016583751777\n",
      "Training loss: 3518238119253.3335 564 3.2206184097324777\n",
      "Training loss: 3512388463274.6665 565 3.220217033008876\n",
      "Training loss: 3509399881864.533 566 3.2198166929850593\n",
      "Training loss: 3510152843318.6133 567 3.219416729695006\n",
      "Training loss: 3495885946224.64 568 3.219018785068848\n",
      "Training loss: 3518829348345.1733 569 3.2186178037392397\n",
      "Training loss: 3517767238724.2666 570 3.2182174941332207\n",
      "Training loss: 3502469102086.8267 571 3.2178262731896825\n",
      "Training loss: 3497835235027.6265 572 3.2174363092612754\n",
      "Training loss: 3515207482955.0933 573 3.2170495047134846\n",
      "Training loss: 3528040039929.1733 574 3.2166561345478244\n",
      "Training loss: 3498628461800.1064 575 3.2162621388154746\n",
      "Training loss: 3504741408221.8667 576 3.2158643796894437\n",
      "Training loss: 3508501965264.2134 577 3.215469970051511\n",
      "Training loss: 3515694693307.7334 578 3.2150703041403164\n",
      "Training loss: 3492975211096.7466 579 3.214667895399438\n",
      "Training loss: 3504676536320.0 580 3.2142623817973983\n",
      "Training loss: 3509838102746.453 581 3.2138528632056023\n",
      "Training loss: 3514724075438.08 582 3.2134340028946777\n",
      "Training loss: 3493077887658.6665 583 3.2130171142651456\n",
      "Training loss: 3517475538862.08 584 3.2126073306744436\n",
      "Training loss: 3520186960663.8936 585 3.2122003135316373\n",
      "Training loss: 3495415513088.0 586 3.21178789023036\n",
      "Training loss: 3515019354439.68 587 3.211382072168092\n",
      "Training loss: 3517117401224.533 588 3.210974266567807\n",
      "Training loss: 3507366483285.3335 589 3.2105692412928257\n",
      "Training loss: 3512507469660.16 590 3.2101582894728553\n",
      "Training loss: 3502071370219.52 591 3.2097503574170188\n",
      "Training loss: 3513742943846.4 592 3.2093433313659046\n",
      "Training loss: 3537891844860.5864 593 3.208942480480298\n",
      "Training loss: 3520501253843.6265 594 3.208542364657731\n",
      "Training loss: 3505472894839.467 595 3.2081376565158104\n",
      "Training loss: 3525884279521.28 596 3.20773130874977\n",
      "Training loss: 3499501995513.1733 597 3.2073234582353054\n",
      "Training loss: 3534867248360.1064 598 3.206911476572014\n",
      "Training loss: 3506265450523.3066 599 3.206499610408929\n",
      "Training loss: 3526703902446.933 600 3.206091190161124\n",
      "Training loss: 3507605167144.96 601 3.205682458824194\n",
      "Training loss: 3515328055214.08 602 3.205278214695487\n",
      "Training loss: 3518299188319.573 603 3.204871746621346\n",
      "Training loss: 3511802379195.7334 604 3.2044691513722285\n",
      "Training loss: 3522854761704.1064 605 3.2040700899664256\n",
      "Training loss: 3519773570061.6533 606 3.2036737995395805\n",
      "Training loss: 3516304265489.067 607 3.203279182935061\n",
      "Training loss: 3494935908406.6133 608 3.202882704958001\n",
      "Training loss: 3511630580503.8936 609 3.202490017911201\n",
      "Training loss: 3493657708243.6265 610 3.2020915052790073\n",
      "Training loss: 3514715351285.76 611 3.201692205519723\n",
      "Training loss: 3516217247662.08 612 3.201288549134755\n",
      "Training loss: 3515946575243.947 613 3.200891370993415\n",
      "Training loss: 3520314914897.92 614 3.200501786118526\n",
      "Training loss: 3534751373721.6 615 3.2001156926120387\n",
      "Training loss: 3528211391228.5864 616 3.1997305375776253\n",
      "Training loss: 3526338606530.56 617 3.19935312275803\n",
      "Training loss: 3507029373091.84 618 3.19897237838227\n",
      "Training loss: 3532727146687.1465 619 3.198592868492897\n",
      "Training loss: 3526876819619.84 620 3.1982120277253787\n",
      "Training loss: 3498847684089.1733 621 3.1978180371385903\n",
      "Training loss: 3510476531739.3066 622 3.1974216727688782\n",
      "Training loss: 3514065961178.453 623 3.1970231860079377\n",
      "Training loss: 3530004316378.453 624 3.1966268128247526\n",
      "Training loss: 3513233811264.8535 625 3.196231298361429\n",
      "Training loss: 3502660138653.013 626 3.1958385056822403\n",
      "Training loss: 3504989711018.6665 627 3.1954458504346865\n",
      "Training loss: 3507507859292.16 628 3.195063636171784\n",
      "Training loss: 3498373895509.3335 629 3.1946846299887164\n",
      "Training loss: 3510290192793.6 630 3.1942980813875312\n",
      "Training loss: 3510145908736.0 631 3.1939135583443625\n",
      "Training loss: 3496303139662.507 632 3.1935349113404388\n",
      "Training loss: 3518264739102.72 633 3.193150280286552\n",
      "Training loss: 3501418624669.013 634 3.1927639117914213\n",
      "Training loss: 3522505795611.3066 635 3.19237876917048\n",
      "Training loss: 3527169190570.6665 636 3.1919937827361258\n",
      "Training loss: 3507352837816.32 637 3.1916017159715517\n",
      "Training loss: 3516916969417.3867 638 3.191203285159783\n",
      "Training loss: 3514075580115.6265 639 3.1908053722312366\n",
      "Training loss: 3489272367677.44 640 3.190396263737568\n",
      "Training loss: 3515074607404.3735 641 3.1899773188990275\n",
      "Training loss: 3502027749457.92 642 3.189555957122617\n",
      "Training loss: 3514481812439.04 643 3.1891368830778286\n",
      "Training loss: 3512340815981.2266 644 3.188715346457376\n",
      "Training loss: 3490748762685.44 645 3.1882962047153307\n",
      "Training loss: 3491155218705.067 646 3.1878758103033924\n",
      "Training loss: 3499243178994.3467 647 3.187456472206775\n",
      "Training loss: 3510538271894.1865 648 3.187034724909538\n",
      "Training loss: 3502351661574.8267 649 3.186615080932936\n",
      "Training loss: 3518371218500.2666 650 3.1861966453342414\n",
      "Training loss: 3489362517251.4136 651 3.185781232589927\n",
      "Training loss: 3492779253213.8667 652 3.185373056277668\n",
      "Training loss: 3511204215521.28 653 3.184972090252872\n",
      "Training loss: 3492895351548.5864 654 3.184575361088631\n",
      "Training loss: 3529662732260.6934 655 3.1841905007655047\n",
      "Training loss: 3504035199276.3735 656 3.1838090342182035\n",
      "Training loss: 3503151822929.92 657 3.183427078182462\n",
      "Training loss: 3533131142048.427 658 3.1830424746001893\n",
      "Training loss: 3507943395819.52 659 3.1826660856098843\n",
      "Training loss: 3523534798192.64 660 3.182295818025944\n",
      "Training loss: 3493026437529.6 661 3.181922063054519\n",
      "Training loss: 3507951448883.2 662 3.1815534556754677\n",
      "Training loss: 3499889437354.6665 663 3.1811819484584225\n",
      "Training loss: 3503072410774.1865 664 3.180811330778721\n",
      "Training loss: 3520385155508.9067 665 3.1804413683919384\n",
      "Training loss: 3519540254911.1465 666 3.180081595737798\n",
      "Training loss: 3502039605357.2266 667 3.179722689290563\n",
      "Training loss: 3529687338844.16 668 3.179347455192418\n",
      "Training loss: 3515000563957.76 669 3.1789738202554343\n",
      "Training loss: 3507769807557.973 670 3.1786028966533526\n",
      "Training loss: 3535435436741.973 671 3.178236134451718\n",
      "Training loss: 3531849810138.453 672 3.1778643323575957\n",
      "Training loss: 3507119298969.6 673 3.1775026313672994\n",
      "Training loss: 3492651746372.2666 674 3.177129269554192\n",
      "Training loss: 3526401912558.933 675 3.1767489048635875\n",
      "Training loss: 3487736916869.12 676 3.176362872255717\n",
      "Training loss: 3516183693230.08 677 3.1759761378086315\n",
      "Training loss: 3514714903893.3335 678 3.175599987843503\n",
      "Training loss: 3500080697617.067 679 3.1752228404143668\n",
      "Training loss: 3530614783344.64 680 3.174843893942914\n",
      "Training loss: 3515518196995.4136 681 3.1744660105170093\n",
      "Training loss: 3521495359815.68 682 3.174090317942836\n",
      "Training loss: 3492896917422.08 683 3.1737001325062546\n",
      "Training loss: 3514123898497.7065 684 3.173308115724864\n",
      "Training loss: 3494467264839.68 685 3.1729233980748393\n",
      "Training loss: 3521335417023.1465 686 3.172544180622452\n",
      "Training loss: 3513053959509.3335 687 3.172165156139884\n",
      "Training loss: 3512489350266.88 688 3.171792865954098\n",
      "Training loss: 3509901856167.2534 689 3.171413720965923\n",
      "Training loss: 3502720312934.4 690 3.1710308218144023\n",
      "Training loss: 3508004241189.547 691 3.1706466898969956\n",
      "Training loss: 3519004278784.0 692 3.170263007393041\n",
      "Training loss: 3490163349695.1465 693 3.1698752285913\n",
      "Training loss: 3506674143505.067 694 3.169493717686951\n",
      "Training loss: 3511998560774.8267 695 3.1691156279217325\n",
      "Training loss: 3513382569246.72 696 3.1687409246477927\n",
      "Training loss: 3505612704972.8 697 3.1683574873388705\n",
      "Training loss: 3508011846860.8 698 3.167971752766026\n",
      "Training loss: 3514504182060.3735 699 3.1675819491288615\n",
      "Training loss: 3543554267108.6934 700 3.167203496812835\n",
      "Training loss: 3514160360980.48 701 3.166820235454278\n",
      "Training loss: 3508865471610.88 702 3.166442886456951\n",
      "Training loss: 3501278143447.04 703 3.1660628477486346\n",
      "Training loss: 3501782802104.32 704 3.1656849414743062\n",
      "Training loss: 3497774837050.027 705 3.165300891062363\n",
      "Training loss: 3516994144610.987 706 3.1649227399305104\n",
      "Training loss: 3531495475336.533 707 3.164551118758521\n",
      "Training loss: 3505042055932.5864 708 3.164171288909748\n",
      "Training loss: 3509689792157.013 709 3.1637940226266696\n",
      "Training loss: 3515749498880.0 710 3.1634118315301283\n",
      "Training loss: 3517134625832.96 711 3.1630422261996283\n",
      "Training loss: 3524841184078.507 712 3.162676795134501\n",
      "Training loss: 3506647299959.467 713 3.1623166500036515\n",
      "Training loss: 3516887665213.44 714 3.1619468196778584\n",
      "Training loss: 3496736439227.7334 715 3.1615836013661656\n",
      "Training loss: 3519173393121.28 716 3.1612123658942437\n",
      "Training loss: 3516998394839.04 717 3.1608325320247928\n",
      "Training loss: 3514264827112.1064 718 3.160449921779596\n",
      "Training loss: 3495624669047.467 719 3.1600548224611646\n",
      "Training loss: 3502147650628.2666 720 3.1596611334833615\n",
      "Training loss: 3513502470417.067 721 3.159258210316673\n",
      "Training loss: 3504690629181.44 722 3.1588498603683832\n",
      "Training loss: 3518898023082.6665 723 3.158444489075289\n",
      "Training loss: 3528040039929.1733 724 3.158053439247479\n",
      "Training loss: 3503744394199.04 725 3.1576620457039697\n",
      "Training loss: 3493487251729.067 726 3.1572719032810777\n",
      "Training loss: 3527472298939.7334 727 3.1568890999282058\n",
      "Training loss: 3503198351742.2935 728 3.1565045806967147\n",
      "Training loss: 3514987142184.96 729 3.1561229912111055\n",
      "Training loss: 3506737896925.8667 730 3.155750324625294\n",
      "Training loss: 3511952031962.453 731 3.155379279488781\n",
      "Training loss: 3506929157188.2666 732 3.1550081222135153\n",
      "Training loss: 3512645042831.36 733 3.154633842323239\n",
      "Training loss: 3493614982266.88 734 3.154256297801409\n",
      "Training loss: 3515468536436.053 735 3.153876219569371\n",
      "Training loss: 3508849812875.947 736 3.1534948568291674\n",
      "Training loss: 3510178792079.36 737 3.1531191716601863\n",
      "Training loss: 3492752185972.053 738 3.152745131222894\n",
      "Training loss: 3500030365969.067 739 3.1523812482566638\n",
      "Training loss: 3504478117778.7734 740 3.152015497857274\n",
      "Training loss: 3497952675539.6265 741 3.151645963812783\n",
      "Training loss: 3521054678275.4136 742 3.1512815871241537\n",
      "Training loss: 3519339823104.0 743 3.1509213945733263\n",
      "Training loss: 3510167383572.48 744 3.1505583071388688\n",
      "Training loss: 3543319162388.48 745 3.1501874426591443\n",
      "Training loss: 3504399376711.68 746 3.149823150727559\n",
      "Training loss: 3507666236211.2 747 3.149458987022438\n",
      "Training loss: 3499274496464.2134 748 3.1490914005879933\n",
      "Training loss: 3511681806936.7466 749 3.148722234083159\n",
      "Training loss: 3521545020375.04 750 3.148353920594455\n",
      "Training loss: 3517117401224.533 751 3.1479752315998586\n",
      "Training loss: 3525952506866.3467 752 3.147594121467909\n",
      "Training loss: 3512641239995.7334 753 3.1472099737487076\n",
      "Training loss: 3518202551555.4136 754 3.146828917647552\n",
      "Training loss: 3496181448922.453 755 3.146443534878366\n",
      "Training loss: 3533649446174.72 756 3.1460683800320344\n",
      "Training loss: 3485794338952.533 757 3.145690717212277\n",
      "Training loss: 3534971267099.3066 758 3.1453140821271868\n",
      "Training loss: 3504796437490.3467 759 3.1449384878193514\n",
      "Training loss: 3529706129326.08 760 3.1445663898287064\n",
      "Training loss: 3512468993911.467 761 3.1441893720786527\n",
      "Training loss: 3521129616506.88 762 3.1438116199523414\n",
      "Training loss: 3515290026857.8135 763 3.1434315229533705\n",
      "Training loss: 3539563974055.2534 764 3.1430467125546673\n",
      "Training loss: 3516303147008.0 765 3.1426757550012936\n",
      "Training loss: 3513077223915.52 766 3.142306035744701\n",
      "Training loss: 3506214671482.88 767 3.1419304677907034\n",
      "Training loss: 3505890311973.547 768 3.1415538324587544\n",
      "Training loss: 3503120058067.6265 769 3.141179620905704\n",
      "Training loss: 3534635946475.52 770 3.1408076188372163\n",
      "Training loss: 3526120502722.56 771 3.1404414087933046\n",
      "Training loss: 3509973438955.52 772 3.1400752486240053\n",
      "Training loss: 3514409558562.1333 773 3.1397093901587634\n",
      "Training loss: 3501579685942.6133 774 3.1393428221220936\n",
      "Training loss: 3527136083531.0933 775 3.138974711226159\n",
      "Training loss: 3504643429280.427 776 3.13861094717789\n",
      "Training loss: 3510952109888.8535 777 3.1382575622558586\n",
      "Training loss: 3525504667047.2534 778 3.1379083560092447\n",
      "Training loss: 3505660128570.027 779 3.1375437388255993\n",
      "Training loss: 3506851087209.8135 780 3.13717322585988\n",
      "Training loss: 3494482923574.6133 781 3.1367997896338853\n",
      "Training loss: 3528575792360.1064 782 3.136421369495714\n",
      "Training loss: 3531849810138.453 783 3.1360453718063934\n",
      "Training loss: 3491647797766.8267 784 3.1356591468076753\n",
      "Training loss: 3520395892927.1465 785 3.135274025567826\n",
      "Training loss: 3525939532485.973 786 3.134889057632786\n",
      "Training loss: 3519577164786.3467 787 3.1345057568928745\n",
      "Training loss: 3511246270409.3867 788 3.1341295391584114\n",
      "Training loss: 3499629054962.3467 789 3.1337528405128006\n",
      "Training loss: 3525424583802.88 790 3.133379680001188\n",
      "Training loss: 3519365100776.1064 791 3.1330098877989134\n",
      "Training loss: 3513506273252.6934 792 3.1326458513882307\n",
      "Training loss: 3522899948339.2 793 3.1322795912374066\n",
      "Training loss: 3491416048489.8135 794 3.1319193713532822\n",
      "Training loss: 3510255072488.1064 795 3.1315575481128857\n",
      "Training loss: 3525733060881.067 796 3.1311954471882193\n",
      "Training loss: 3506441946835.6265 797 3.1308338746344306\n",
      "Training loss: 3528627913577.8135 798 3.130474606994014\n",
      "Training loss: 3526603462847.1465 799 3.130118622042157\n"
     ]
    }
   ],
   "source": [
    "model = RevenuePredictor(X_train.shape[-1]).cuda()\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "# use adamW instead of adam\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "epochs = 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full model\n",
    "torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3502276052254.72 0 2.4097179241738953\n",
      "Training loss: 3502152348248.7466 1 2.4098651490680423\n",
      "Training loss: 3513130239918.08 2 2.4101107414188405\n",
      "Training loss: 3473812498677.76 3 2.4104195690736194\n",
      "Training loss: 3513595304345.6 4 2.41064640199983\n",
      "Training loss: 3513284814001.493 5 2.410488445276835\n",
      "Training loss: 3514902808712.533 6 2.4101992450703014\n",
      "Training loss: 3503073081862.8267 7 2.4098319369060146\n",
      "Training loss: 3500732772078.933 8 2.4095701312111077\n",
      "Training loss: 3513320381699.4136 9 2.40940408995175\n",
      "Training loss: 3506637681022.2935 10 2.40936356703281\n",
      "Training loss: 3501745221140.48 11 2.409459899281883\n",
      "Training loss: 3521719056029.013 12 2.4096824737157823\n",
      "Training loss: 3487237850617.1733 13 2.409883247174646\n",
      "Training loss: 3524616593080.32 14 2.4099206288426682\n",
      "Training loss: 3502709799212.3735 15 2.4098807018300716\n",
      "Training loss: 3497800785810.7734 16 2.4098864021866415\n",
      "Training loss: 3533284597650.7734 17 2.409846074916649\n",
      "Training loss: 3496448542201.1733 18 2.4098164868856187\n",
      "Training loss: 3503318252912.64 19 2.4097638054210697\n",
      "Training loss: 3520352943254.1865 20 2.4096372040739453\n",
      "Training loss: 3522124169871.36 21 2.40957391392732\n",
      "Training loss: 3499891898013.013 22 2.4095743543325727\n",
      "Training loss: 3512091394703.36 23 2.4097575130113236\n",
      "Training loss: 3508747807402.6665 24 2.4099043282115686\n",
      "Training loss: 3511720282685.44 25 2.4101862145883124\n",
      "Training loss: 3506774359408.64 26 2.410383356495682\n",
      "Training loss: 3508457002325.3335 27 2.4104870794177065\n",
      "Training loss: 3508393472600.7466 28 2.410435227301813\n",
      "Training loss: 3531450512397.6533 29 2.4102323528068816\n",
      "Training loss: 3510969110801.067 30 2.4101085198432073\n",
      "Training loss: 3526018497249.28 31 2.4099298935468623\n",
      "Training loss: 3535385776182.6133 32 2.4096104407048933\n",
      "Training loss: 3498181516765.8667 33 2.409330464730978\n",
      "Training loss: 3504808740782.08 34 2.4094429143964113\n",
      "Training loss: 3520348469329.92 35 2.4096229689073736\n",
      "Training loss: 3512304353498.453 36 2.4097403378550295\n",
      "Training loss: 3494585600136.533 37 2.4099164142051452\n",
      "Training loss: 3500469929028.2666 38 2.410075047199039\n",
      "Training loss: 3486657135247.36 39 2.410333457581511\n",
      "Training loss: 3504267395945.8135 40 2.4105422936858942\n",
      "Training loss: 3515695364396.3735 41 2.410581257413693\n",
      "Training loss: 3484411896354.1333 42 2.4104209828643772\n",
      "Training loss: 3524967572439.04 43 2.4101503851914234\n",
      "Training loss: 3497607288586.24 44 2.410019526907768\n",
      "Training loss: 3499341605328.2134 45 2.409923731681872\n",
      "Training loss: 3503579753786.027 46 2.4099357748273005\n",
      "Training loss: 3523044903485.44 47 2.409778607607142\n",
      "Training loss: 3503409073575.2534 48 2.4097140541873587\n",
      "Training loss: 3491558095585.28 49 2.4097353411323743\n",
      "Training loss: 3503217365920.427 50 2.409929429310593\n",
      "Training loss: 3506614864008.533 51 2.410021806302212\n",
      "Training loss: 3509082233241.6 52 2.4101038696664556\n",
      "Training loss: 3524157121058.1333 53 2.4101000555776877\n",
      "Training loss: 3493523043123.2 54 2.4102834757770935\n",
      "Training loss: 3520182710435.84 55 2.410269793261956\n",
      "Training loss: 3509019598301.8667 56 2.410319564822436\n",
      "Training loss: 3475102107347.6265 57 2.4105570613529217\n",
      "Training loss: 3508398170221.2266 58 2.4108196218882365\n",
      "Training loss: 3513736009263.7866 59 2.410905231162133\n",
      "Training loss: 3501266287547.7334 60 2.4108302758754316\n",
      "Training loss: 3508086561396.053 61 2.410543678203125\n",
      "Training loss: 3514979089121.28 62 2.410208265464528\n",
      "Training loss: 3504833123669.3335 63 2.409935548449674\n",
      "Training loss: 3514220982654.2935 64 2.409707172988329\n",
      "Training loss: 3499469559562.24 65 2.409632674968255\n",
      "Training loss: 3515736524499.6265 66 2.409686438756238\n",
      "Training loss: 3506689802240.0 67 2.4099598292312945\n",
      "Training loss: 3501694889492.48 68 2.410344098575219\n",
      "Training loss: 3505918497696.427 69 2.410585072186824\n",
      "Training loss: 3511243809751.04 70 2.4106405383083556\n",
      "Training loss: 3497454951464.96 71 2.410571895134853\n",
      "Training loss: 3476208061426.3467 72 2.410568678663066\n",
      "Training loss: 3518509910152.533 73 2.410406451425932\n",
      "Training loss: 3504860638303.573 74 2.41019156915151\n",
      "Training loss: 3514013392568.32 75 2.40989861457831\n",
      "Training loss: 3512477718063.7866 76 2.4096496594474965\n",
      "Training loss: 3502631729233.92 77 2.409521354598612\n",
      "Training loss: 3494246476677.12 78 2.4095877065378204\n",
      "Training loss: 3516408507924.48 79 2.4099177900364332\n",
      "Training loss: 3507351719335.2534 80 2.410293293950039\n",
      "Training loss: 3516785659740.16 81 2.4106054847482263\n",
      "Training loss: 3518074597321.3867 82 2.4108054578414024\n",
      "Training loss: 3496617656538.453 83 2.4107779338053206\n",
      "Training loss: 3494626089151.1465 84 2.41076304729094\n",
      "Training loss: 3502741340378.453 85 2.4108139326608273\n",
      "Training loss: 3507171196491.0933 86 2.4107506713175804\n",
      "Training loss: 3507551256357.547 87 2.4104866645993197\n",
      "Training loss: 3500655820581.547 88 2.410163125315303\n",
      "Training loss: 3505793227816.96 89 2.4097398494955486\n",
      "Training loss: 3509632078533.973 90 2.409570446365237\n",
      "Training loss: 3513646083386.027 91 2.409583597289603\n",
      "Training loss: 3510297798464.8535 92 2.409835615378975\n",
      "Training loss: 3528968155518.2935 93 2.410151180670721\n",
      "Training loss: 3493198907310.08 94 2.4105505504285922\n",
      "Training loss: 3505407799241.3867 95 2.4109832878837865\n",
      "Training loss: 3524573867103.573 96 2.411122336176136\n",
      "Training loss: 3505443143243.0933 97 2.4111116020439916\n",
      "Training loss: 3496721227885.2266 98 2.4109862898111847\n",
      "Training loss: 3505820742451.2 99 2.410663021237568\n",
      "Training loss: 3510009901438.2935 100 2.4101611906334384\n",
      "Training loss: 3522015229815.467 101 2.409741404270577\n",
      "Training loss: 3501949232087.04 102 2.4096178768687344\n",
      "Training loss: 3520634129394.3467 103 2.409697953377696\n",
      "Training loss: 3500431229583.36 104 2.4101666764583376\n",
      "Training loss: 3500047143185.067 105 2.410555759417187\n",
      "Training loss: 3488909756115.6265 106 2.410908190672844\n",
      "Training loss: 3536268034048.0 107 2.411031756957987\n",
      "Training loss: 3508804402544.64 108 2.411017876097626\n",
      "Training loss: 3532265885095.2534 109 2.410621315121268\n",
      "Training loss: 3527000523625.8135 110 2.410245209448305\n",
      "Training loss: 3518977882630.8267 111 2.409890796023721\n",
      "Training loss: 3515640335127.8936 112 2.4098620963070427\n",
      "Training loss: 3527909177644.3735 113 2.40985184368607\n",
      "Training loss: 3505352993669.12 114 2.4098991660980813\n",
      "Training loss: 3491353860942.507 115 2.410086210102501\n",
      "Training loss: 3525971297348.2666 116 2.4103004260160894\n",
      "Training loss: 3539673137807.36 117 2.4105033681046955\n",
      "Training loss: 3489608359389.8667 118 2.410600127495789\n",
      "Training loss: 3506450223595.52 119 2.41057736201189\n",
      "Training loss: 3493503581552.64 120 2.4104786032384027\n",
      "Training loss: 3507957488680.96 121 2.410375054130401\n",
      "Training loss: 3510391079785.8135 122 2.4101877871884807\n",
      "Training loss: 3507668473173.3335 123 2.410098062913891\n",
      "Training loss: 3516233801181.8667 124 2.410040759233275\n",
      "Training loss: 3502595490447.36 125 2.410289529324446\n",
      "Training loss: 3522004492397.2266 126 2.4104369867199273\n",
      "Training loss: 3534156565490.3467 127 2.410584652527172\n",
      "Training loss: 3499467993688.7466 128 2.410748257915766\n",
      "Training loss: 3513544525305.1733 129 2.4107676798791515\n",
      "Training loss: 3507632234386.7734 130 2.4106796647063784\n",
      "Training loss: 3526505036513.28 131 2.410601559024035\n",
      "Training loss: 3491777094178.1333 132 2.410510789199252\n",
      "Training loss: 3508688304209.92 133 2.4105186012859305\n",
      "Training loss: 3526928717141.3335 134 2.4105014053773215\n",
      "Training loss: 3524472309022.72 135 2.4103167926803297\n",
      "Training loss: 3526262997210.453 136 2.4101540168089466\n",
      "Training loss: 3525362396255.573 137 2.410137008658839\n",
      "Training loss: 3517274883358.72 138 2.4099571351979168\n",
      "Training loss: 3529783751912.1064 139 2.409767997294556\n",
      "Training loss: 3503174639943.68 140 2.4098421374185417\n",
      "Training loss: 3500637701188.2666 141 2.4102805005906687\n",
      "Training loss: 3485748257532.5864 142 2.4105200647356515\n",
      "Training loss: 3512847264208.2134 143 2.4108704525446174\n",
      "Training loss: 3530668917828.2666 144 2.411118447344213\n",
      "Training loss: 3504658640622.933 145 2.411206710318708\n",
      "Training loss: 3505336216453.12 146 2.4111331377999794\n",
      "Training loss: 3519079888104.1064 147 2.410828092367079\n",
      "Training loss: 3518558004838.4 148 2.4104565515208694\n",
      "Training loss: 3505231750321.493 149 2.410205281570478\n",
      "Training loss: 3521685501597.013 150 2.4100093034934833\n",
      "Training loss: 3513577408648.533 151 2.4099304759296207\n",
      "Training loss: 3495547941246.2935 152 2.4099637431858367\n",
      "Training loss: 3498037232708.2666 153 2.410143143112608\n",
      "Training loss: 3494437065850.88 154 2.4102788062002243\n",
      "Training loss: 3505747146397.013 155 2.4104093631068713\n",
      "Training loss: 3506558716258.987 156 2.4105148998859973\n",
      "Training loss: 3511575551235.4136 157 2.41062699048282\n",
      "Training loss: 3524244138885.12 158 2.410626016625084\n",
      "Training loss: 3510804470388.053 159 2.4106853762310276\n",
      "Training loss: 3508727227351.04 160 2.4107394423041013\n",
      "Training loss: 3498838512544.427 161 2.4107666458654333\n",
      "Training loss: 3504510553729.7065 162 2.4107808357611042\n",
      "Training loss: 3520614891520.0 163 2.410694226838817\n",
      "Training loss: 3504169193308.16 164 2.4105831812836715\n",
      "Training loss: 3515877005721.6 165 2.410647754746974\n",
      "Training loss: 3518306793990.8267 166 2.4108066266647175\n",
      "Training loss: 3512868291652.2666 167 2.4109132610655597\n",
      "Training loss: 3531706197169.493 168 2.410908628725806\n",
      "Training loss: 3523985546062.507 169 2.4107791416656883\n",
      "Training loss: 3501940731630.933 170 2.410726204474488\n",
      "Training loss: 3494539742412.8 171 2.410491235141948\n",
      "Training loss: 3499605119467.52 172 2.4101222105375886\n",
      "Training loss: 3493964395752.1064 173 2.409907973043497\n",
      "Training loss: 3494149839912.96 174 2.409837054365884\n",
      "Training loss: 3493362652938.24 175 2.410053969179106\n",
      "Training loss: 3487121752282.453 176 2.4104870356212023\n",
      "Training loss: 3491909969728.8535 177 2.411055821233979\n",
      "Training loss: 3492622889560.7466 178 2.411515562492852\n",
      "Training loss: 3496096668057.6 179 2.4117373960906314\n",
      "Training loss: 3516923456607.573 180 2.411660310125936\n",
      "Training loss: 3488006023413.76 181 2.4113638310561156\n",
      "Training loss: 3532768754182.8267 182 2.410971033886543\n",
      "Training loss: 3521958858369.7065 183 2.4103085384346925\n",
      "Training loss: 3498520192832.8535 184 2.4097819009304797\n",
      "Training loss: 3503580201178.453 185 2.4094906924281663\n",
      "Training loss: 3498609447621.973 186 2.409420293023009\n",
      "Training loss: 3517321412171.0933 187 2.4096115494845303\n",
      "Training loss: 3508718055806.2935 188 2.410011713705306\n",
      "Training loss: 3497737927174.8267 189 2.410575671824138\n",
      "Training loss: 3510304733047.467 190 2.410919185420773\n",
      "Training loss: 3517487394761.3867 191 2.4112171142250847\n",
      "Training loss: 3506944815923.2 192 2.411424054704696\n",
      "Training loss: 3515053579960.32 193 2.411465612657055\n",
      "Training loss: 3524212821415.2534 194 2.4114163109820725\n",
      "Training loss: 3514839726380.3735 195 2.411114218646778\n",
      "Training loss: 3490387493300.9067 196 2.410847964956211\n",
      "Training loss: 3518791543685.12 197 2.4104796251416114\n",
      "Training loss: 3528225707786.24 198 2.4100960256236625\n",
      "Training loss: 3508175145096.533 199 2.4098830623183614\n",
      "Training loss: 3491915114741.76 200 2.409913937956051\n",
      "Training loss: 3511414266265.6 201 2.4100212325136443\n",
      "Training loss: 3508850260268.3735 202 2.410274300510072\n",
      "Training loss: 3513175650249.3867 203 2.4105962316732032\n",
      "Training loss: 3524030956393.8135 204 2.4107982100743994\n",
      "Training loss: 3496941568655.36 205 2.410954310409753\n",
      "Training loss: 3487365357458.7734 206 2.410912151056934\n",
      "Training loss: 3517136862795.0933 207 2.4110678516476143\n",
      "Training loss: 3508294375178.24 208 2.4110805429281537\n",
      "Training loss: 3504615467253.76 209 2.41125918292928\n",
      "Training loss: 3514907730029.2266 210 2.4111888415030203\n",
      "Training loss: 3506299899740.16 211 2.410834578236163\n",
      "Training loss: 3501234522685.44 212 2.410485766194642\n",
      "Training loss: 3498856855633.92 213 2.4103601394620076\n",
      "Training loss: 3512347303171.4136 214 2.410238824309832\n",
      "Training loss: 3480553807762.7734 215 2.4103308598166535\n",
      "Training loss: 3515647717102.933 216 2.4105193616469283\n",
      "Training loss: 3492917497473.7065 217 2.410964928468741\n",
      "Training loss: 3510245453550.933 218 2.411457213302611\n",
      "Training loss: 3516681417304.7466 219 2.4117139392244122\n",
      "Training loss: 3485534851345.067 220 2.411834396790687\n",
      "Training loss: 3488973509536.427 221 2.4117867380211213\n",
      "Training loss: 3509828707505.493 222 2.4115671143254716\n",
      "Training loss: 3518351756929.7065 223 2.4111265240013484\n",
      "Training loss: 3504339873518.933 224 2.410549782395156\n",
      "Training loss: 3505854296883.2 225 2.4100207553293824\n",
      "Training loss: 3493151483712.8535 226 2.4096846928583444\n",
      "Training loss: 3504825070605.6533 227 2.4098173489067065\n",
      "Training loss: 3484172317709.6533 228 2.4101580575901447\n",
      "Training loss: 3516886099339.947 229 2.4104258867152906\n",
      "Training loss: 3500985548800.0 230 2.4109876867759983\n",
      "Training loss: 3498188003956.053 231 2.411484750983674\n",
      "Training loss: 3487318381253.973 232 2.4119935357519564\n",
      "Training loss: 3518448841086.2935 233 2.412008737045608\n",
      "Training loss: 3519280990999.8936 234 2.411566486642595\n",
      "Training loss: 3529480196150.6133 235 2.410982921088102\n",
      "Training loss: 3494929197520.2134 236 2.4105847767081\n",
      "Training loss: 3526504141728.427 237 2.41017612403215\n",
      "Training loss: 3518649496589.6533 238 2.4099182798472802\n",
      "Training loss: 3528192377050.453 239 2.4096763509879255\n",
      "Training loss: 3525861015115.0933 240 2.4096290643717015\n",
      "Training loss: 3506871890957.6533 241 2.409791578089848\n",
      "Training loss: 3524034088140.8 242 2.410231324228248\n",
      "Training loss: 3502314528003.4136 243 2.4107743350862476\n",
      "Training loss: 3505485869219.84 244 2.4113888685413807\n",
      "Training loss: 3507714107200.8535 245 2.411723399417454\n",
      "Training loss: 3506022740131.84 246 2.4117584161521233\n",
      "Training loss: 3508552296912.2134 247 2.411507235862883\n",
      "Training loss: 3512791563851.0933 248 2.4112459547061484\n",
      "Training loss: 3520876392393.3867 249 2.411060036011741\n",
      "Training loss: 3519295083861.3335 250 2.410769743190869\n",
      "Training loss: 3506916853896.533 251 2.4104229732152174\n",
      "Training loss: 3515522894615.8936 252 2.410339268044613\n",
      "Training loss: 3521781243576.32 253 2.4104398048336093\n",
      "Training loss: 3503554923506.3467 254 2.4106348832107716\n",
      "Training loss: 3509828707505.493 255 2.4108831974868083\n",
      "Training loss: 3511317182109.013 256 2.411183725784266\n",
      "Training loss: 3500693177849.1733 257 2.411374448912501\n",
      "Training loss: 3494984897877.3335 258 2.411586453578637\n",
      "Training loss: 3515472115575.467 259 2.411755162572765\n",
      "Training loss: 3506551110587.7334 260 2.4118774352590098\n",
      "Training loss: 3520573731416.7466 261 2.411825651808909\n",
      "Training loss: 3501871609501.013 262 2.411414075294168\n",
      "Training loss: 3495640327782.4 263 2.411068381521834\n",
      "Training loss: 3515796251388.5864 264 2.410790242646027\n",
      "Training loss: 3511830341222.4 265 2.410553169549942\n",
      "Training loss: 3510415015280.64 266 2.410509095082294\n",
      "Training loss: 3507333823638.1865 267 2.4107633964785604\n",
      "Training loss: 3503616887357.44 268 2.411041255363973\n",
      "Training loss: 3510022875818.6665 269 2.4111046281205755\n",
      "Training loss: 3506537465118.72 270 2.411245778841999\n",
      "Training loss: 3487084395014.8267 271 2.4114071848144523\n",
      "Training loss: 3503948181449.3867 272 2.411516522239919\n",
      "Training loss: 3496961701314.56 273 2.4115561857673677\n",
      "Training loss: 3500123199897.6 274 2.411729010546035\n",
      "Training loss: 3496642039425.7065 275 2.411724887524455\n",
      "Training loss: 3508763466137.6 276 2.4115908054905812\n",
      "Training loss: 3486377291284.48 277 2.4113208447262773\n",
      "Training loss: 3514574870063.7866 278 2.411092937867912\n",
      "Training loss: 3501938494668.8 279 2.4109326092963417\n",
      "Training loss: 3484841840476.16 280 2.410767783519165\n",
      "Training loss: 3491380480791.8936 281 2.4106799258788536\n",
      "Training loss: 3494259451057.493 282 2.4108153710196225\n",
      "Training loss: 3497471057592.32 283 2.4111410621738965\n",
      "Training loss: 3491711998580.053 284 2.411451589129009\n",
      "Training loss: 3509593826481.493 285 2.411677121371535\n",
      "Training loss: 3523584682448.2134 286 2.4117683318717735\n",
      "Training loss: 3499799064084.48 287 2.4115985055998874\n",
      "Training loss: 3523310207194.453 288 2.4113208410094105\n",
      "Training loss: 3507390642476.3735 289 2.4110754520493463\n",
      "Training loss: 3499835302871.04 290 2.4107843171686247\n",
      "Training loss: 3501331159449.6 291 2.4106253324655174\n",
      "Training loss: 3504396692357.12 292 2.410644021484423\n",
      "Training loss: 3506859587665.92 293 2.410784875368552\n",
      "Training loss: 3491953143098.027 294 2.4109789667580013\n",
      "Training loss: 3516916745721.1733 295 2.411071277606208\n",
      "Training loss: 3498551510302.72 296 2.4111528554240627\n",
      "Training loss: 3520923368598.1865 297 2.4110686481842096\n",
      "Training loss: 3511506876497.92 298 2.4110280874756502\n",
      "Training loss: 3510943162040.32 299 2.411024329804213\n",
      "Training loss: 3521318639807.1465 300 2.4108413051581747\n",
      "Training loss: 3502934837602.987 301 2.4107482206926956\n",
      "Training loss: 3520695869549.2266 302 2.4105808614403115\n",
      "Training loss: 3506178432696.32 303 2.4106129774969007\n",
      "Training loss: 3508882248826.88 304 2.410798367222678\n",
      "Training loss: 3517024790992.2134 305 2.411159624143626\n",
      "Training loss: 3507042123776.0 306 2.4116181652255375\n",
      "Training loss: 3495159604619.947 307 2.4119210038347165\n",
      "Training loss: 3501700258201.6 308 2.412060146804397\n",
      "Training loss: 3526099027886.08 309 2.411911516515114\n",
      "Training loss: 3511059931463.68 310 2.411691563404045\n",
      "Training loss: 3500057880603.3066 311 2.411460979216847\n",
      "Training loss: 3507822376168.1064 312 2.411243987627423\n",
      "Training loss: 3509878591761.067 313 2.410967988426551\n",
      "Training loss: 3498711453095.2534 314 2.4107026856737392\n",
      "Training loss: 3537287417692.16 315 2.4104979506109245\n",
      "Training loss: 3501168532302.507 316 2.4105731841277045\n",
      "Training loss: 3517054542588.5864 317 2.4107499194147954\n",
      "Training loss: 3514364819319.467 318 2.410951289481628\n",
      "Training loss: 3506560729524.9067 319 2.411270200975087\n",
      "Training loss: 3494365706758.8267 320 2.4116952273327223\n",
      "Training loss: 3512525812749.6533 321 2.411946343919138\n",
      "Training loss: 3497914199790.933 322 2.412004090840158\n",
      "Training loss: 3517327899361.28 323 2.411859760148356\n",
      "Training loss: 3511701939595.947 324 2.4117218188619356\n",
      "Training loss: 3494673289052.16 325 2.41170939886859\n",
      "Training loss: 3503479314186.24 326 2.4116058159099962\n",
      "Training loss: 3516794160196.2666 327 2.4116904563815758\n",
      "Training loss: 3508013412734.2935 328 2.4118537565940703\n",
      "Training loss: 3515379281646.933 329 2.4118057460146396\n",
      "Training loss: 3510674726584.32 330 2.411707539496472\n",
      "Training loss: 3501710100834.987 331 2.4116110115337923\n",
      "Training loss: 3521415052875.0933 332 2.4115625447946756\n",
      "Training loss: 3494261688019.6265 333 2.411580339954823\n",
      "Training loss: 3499640910861.6533 334 2.4115153360377812\n",
      "Training loss: 3507758622747.3066 335 2.4115497151846763\n",
      "Training loss: 3524370079853.2266 336 2.4115428289366028\n",
      "Training loss: 3506998279318.1865 337 2.4115849431990957\n",
      "Training loss: 3533886116768.427 338 2.4115427587174376\n",
      "Training loss: 3499497074196.48 339 2.411446311973627\n",
      "Training loss: 3513264233949.8667 340 2.4112975060888084\n",
      "Training loss: 3526684440876.3735 341 2.4112590138076286\n",
      "Training loss: 3521179277066.24 342 2.4112285627809023\n",
      "Training loss: 3512713717568.8535 343 2.4111487151115276\n",
      "Training loss: 3500478876876.8 344 2.4113708744734166\n",
      "Training loss: 3509422027789.6533 345 2.411535213217433\n",
      "Training loss: 3513240969543.68 346 2.4114568076614993\n",
      "Training loss: 3485391238376.1064 347 2.411461190801899\n",
      "Training loss: 3521124247797.76 348 2.411422316039743\n",
      "Training loss: 3502111859234.1333 349 2.4113398605124963\n",
      "Training loss: 3478291791653.547 350 2.4112213154739797\n",
      "Training loss: 3511540878322.3467 351 2.411155478660231\n",
      "Training loss: 3497804588646.4 352 2.4113461948840564\n",
      "Training loss: 3497008230126.933 353 2.4116032584404468\n",
      "Training loss: 3501501392267.947 354 2.4117300668802644\n",
      "Training loss: 3505716500015.7866 355 2.4118253394232125\n",
      "Training loss: 3514781789061.12 356 2.4118647834254765\n",
      "Training loss: 3512067682904.7466 357 2.411794976664812\n",
      "Training loss: 3484190884495.36 358 2.411717885202297\n",
      "Training loss: 3542921206824.96 359 2.411559298357506\n",
      "Training loss: 3518538766964.053 360 2.411608491116009\n",
      "Training loss: 3498487980578.1333 361 2.4117625346967246\n",
      "Training loss: 3525529049934.507 362 2.41183278032986\n",
      "Training loss: 3505186339990.1865 363 2.411813051470266\n",
      "Training loss: 3517307543005.8667 364 2.411668853408568\n",
      "Training loss: 3489591134781.44 365 2.411578789209563\n",
      "Training loss: 3525096645154.1333 366 2.4116179390226065\n",
      "Training loss: 3515617518114.1333 367 2.411573021277476\n",
      "Training loss: 3540711983022.08 368 2.411394093564234\n",
      "Training loss: 3538896688250.88 369 2.411061388283561\n",
      "Training loss: 3512447295378.7734 370 2.410927527782899\n",
      "Training loss: 3518210828315.3066 371 2.4109907874450514\n",
      "Training loss: 3497472847162.027 372 2.4113909217984153\n",
      "Training loss: 3512496061153.28 373 2.411810955134509\n",
      "Training loss: 3507145918818.987 374 2.4120488085586897\n",
      "Training loss: 3479495053585.067 375 2.41250022191205\n",
      "Training loss: 3525248087490.56 376 2.4129227956457044\n",
      "Training loss: 3531837283150.507 377 2.412891169065374\n",
      "Training loss: 3494592311022.933 378 2.4126170710099655\n",
      "Training loss: 3500410649531.7334 379 2.4121882455120653\n",
      "Training loss: 3509282441352.533 380 2.411377148678269\n",
      "Training loss: 3511377580086.6133 381 2.4105790009862615\n",
      "Training loss: 3529760263809.7065 382 2.4104971847925047\n",
      "Training loss: 3503601452318.72 383 2.4105186234329183\n",
      "Training loss: 3494488515979.947 384 2.410971145311296\n",
      "Training loss: 3511798128967.68 385 2.411533019080777\n",
      "Training loss: 3500968771584.0 386 2.4119656942575105\n",
      "Training loss: 3496764624950.6133 387 2.412300594963852\n",
      "Training loss: 3503299909823.1465 388 2.4125161246871722\n",
      "Training loss: 3494549585046.1865 389 2.4125622488780634\n",
      "Training loss: 3524563353381.547 390 2.4124163069235407\n",
      "Training loss: 3518783490621.44 391 2.4121263573686025\n",
      "Training loss: 3505755423156.9067 392 2.4119268506852\n",
      "Training loss: 3513500233454.933 393 2.4114719674201184\n",
      "Training loss: 3502128189057.7065 394 2.411229868421112\n",
      "Training loss: 3516094662137.1733 395 2.4112496687289693\n",
      "Training loss: 3505982251117.2266 396 2.411350165113271\n",
      "Training loss: 3508165973551.7866 397 2.411462722806536\n",
      "Training loss: 3503018276290.56 398 2.4116457745330218\n",
      "Training loss: 3514976181070.507 399 2.4118345207132212\n",
      "Training loss: 3513526182215.68 400 2.411916321034501\n",
      "Training loss: 3512912359806.2935 401 2.4118540364033954\n",
      "Training loss: 3510330010719.573 402 2.411685920788524\n",
      "Training loss: 3490434022113.28 403 2.411541458098667\n",
      "Training loss: 3518323123814.4 404 2.411400286865557\n",
      "Training loss: 3524596013028.6934 405 2.411321344798791\n",
      "Training loss: 3517077359602.3467 406 2.4115038614977844\n",
      "Training loss: 3499795932337.493 407 2.4116801441808584\n",
      "Training loss: 3510426871179.947 408 2.4118261784927832\n",
      "Training loss: 3520842837961.3867 409 2.412062347326975\n",
      "Training loss: 3502070699130.88 410 2.4123373418029344\n",
      "Training loss: 3528307356904.1064 411 2.4125643513123682\n",
      "Training loss: 3509290718112.427 412 2.4125822345294456\n",
      "Training loss: 3508996781288.1064 413 2.4124203663188504\n",
      "Training loss: 3487059117342.72 414 2.412136082234118\n",
      "Training loss: 3502987629909.3335 415 2.411760290397552\n",
      "Training loss: 3492730487439.36 416 2.4115302054654526\n",
      "Training loss: 3494251174297.6 417 2.411267345902775\n",
      "Training loss: 3514587397051.7334 418 2.4112787501721944\n",
      "Training loss: 3516418574254.08 419 2.411435693720519\n",
      "Training loss: 3501699587112.96 420 2.411610947807181\n",
      "Training loss: 3521010162728.96 421 2.41171108178955\n",
      "Training loss: 3502217891239.2534 422 2.411950467337859\n",
      "Training loss: 3507397800755.2 423 2.412196867577953\n",
      "Training loss: 3520117167445.3335 424 2.4121900985287485\n",
      "Training loss: 3500067723236.6934 425 2.412199764096787\n",
      "Training loss: 3527073001198.933 426 2.4121839936066416\n",
      "Training loss: 3492504554263.8936 427 2.4120868256611003\n",
      "Training loss: 3498273903301.973 428 2.4120183066679295\n",
      "Training loss: 3519115679498.24 429 2.411884824064333\n",
      "Training loss: 3496902421818.027 430 2.4119252373700286\n",
      "Training loss: 3507890379816.96 431 2.4119126194602716\n",
      "Training loss: 3491018316622.507 432 2.41194340303329\n",
      "Training loss: 3517836360854.1865 433 2.4119433707760254\n",
      "Training loss: 3518696920186.88 434 2.411879295283475\n",
      "Training loss: 3524375448562.3467 435 2.411921551970798\n",
      "Training loss: 3510386829557.76 436 2.4119197230507243\n",
      "Training loss: 3516500223371.947 437 2.4119252934924775\n",
      "Training loss: 3494441316078.933 438 2.411771088626528\n",
      "Training loss: 3503659613334.1865 439 2.411903517559003\n",
      "Training loss: 3515730037309.44 440 2.412009815430072\n",
      "Training loss: 3489099450504.533 441 2.4120195234494157\n",
      "Training loss: 3514235970300.5864 442 2.412142266924681\n",
      "Training loss: 3503831412026.027 443 2.412219691540418\n",
      "Training loss: 3502723444681.3867 444 2.412379175592382\n",
      "Training loss: 3491688510477.6533 445 2.412425127499002\n",
      "Training loss: 3502339805675.52 446 2.4123659699093616\n",
      "Training loss: 3493248120477.013 447 2.412099479446275\n",
      "Training loss: 3510079918353.067 448 2.411944118067824\n",
      "Training loss: 3515903178178.56 449 2.411773067058286\n",
      "Training loss: 3525028194112.8535 450 2.4116051236556846\n",
      "Training loss: 3506721343406.08 451 2.4114716540628205\n",
      "Training loss: 3487523734377.8135 452 2.411452554063579\n",
      "Training loss: 3514927191599.7866 453 2.4115010671058412\n",
      "Training loss: 3509010426757.12 454 2.4117309218873912\n",
      "Training loss: 3514773735997.44 455 2.4119543037876663\n",
      "Training loss: 3504402508458.6665 456 2.4123631925330646\n",
      "Training loss: 3515248419362.1333 457 2.412804019774971\n",
      "Training loss: 3502203798377.8135 458 2.4129008168605233\n",
      "Training loss: 3485981125290.6665 459 2.41302782386337\n",
      "Training loss: 3502471115352.7466 460 2.4129677334444564\n",
      "Training loss: 3488087672531.6265 461 2.4127581940269565\n",
      "Training loss: 3505514949727.573 462 2.4123001383569136\n",
      "Training loss: 3499334894441.8135 463 2.411941960829761\n",
      "Training loss: 3500750444079.7866 464 2.411879901521799\n",
      "Training loss: 3477307975707.3066 465 2.4118104475301547\n",
      "Training loss: 3494473975726.08 466 2.412072109050093\n",
      "Training loss: 3512462506721.28 467 2.4123035110947004\n",
      "Training loss: 3514931889220.2666 468 2.412447626906142\n",
      "Training loss: 3508353430978.56 469 2.4124942826933253\n",
      "Training loss: 3495003912055.467 470 2.4125867424113565\n",
      "Training loss: 3515184889637.547 471 2.412613940957871\n",
      "Training loss: 3491842189776.2134 472 2.4125274887698573\n",
      "Training loss: 3500143332556.8 473 2.412380819297704\n",
      "Training loss: 3513618121359.36 474 2.412175900300928\n",
      "Training loss: 3506554466030.933 475 2.4121675780575207\n",
      "Training loss: 3512397187426.987 476 2.4119836294267096\n",
      "Training loss: 3488686730990.933 477 2.411814355610266\n",
      "Training loss: 3505707328471.04 478 2.411794629810329\n",
      "Training loss: 3510168502053.547 479 2.4119846742343958\n",
      "Training loss: 3505937288178.3467 480 2.4123684031135597\n",
      "Training loss: 3550485717975.04 481 2.4124427864092426\n",
      "Training loss: 3502282763141.12 482 2.412407743495989\n",
      "Training loss: 3505199538066.7734 483 2.4122589883932006\n",
      "Training loss: 3532362969251.84 484 2.412202050510427\n",
      "Training loss: 3527318843337.3867 485 2.412020288257849\n",
      "Training loss: 3504000078970.88 486 2.4118014295387145\n",
      "Training loss: 3496559271826.7734 487 2.4116608628101543\n",
      "Training loss: 3521659552836.2666 488 2.4116203746689377\n",
      "Training loss: 3503480208971.0933 489 2.4117307990487564\n",
      "Training loss: 3505348519744.8535 490 2.4118515761750627\n",
      "Training loss: 3512175504479.573 491 2.412011773295647\n",
      "Training loss: 3518035003091.6265 492 2.412304653602539\n",
      "Training loss: 3505956973445.12 493 2.4126678184150094\n",
      "Training loss: 3509094089140.9067 494 2.412809987334586\n",
      "Training loss: 3487385266421.76 495 2.412969006369736\n",
      "Training loss: 3524237875391.1465 496 2.412936771223267\n",
      "Training loss: 3496567324890.453 497 2.4128439698117248\n",
      "Training loss: 3486431649464.32 498 2.412710163920226\n",
      "Training loss: 3495841206981.973 499 2.412465884654774\n",
      "Training loss: 3524446136565.76 500 2.412138931883918\n",
      "Training loss: 3519733752135.68 501 2.411985597693219\n",
      "Training loss: 3510638487797.76 502 2.4119615063258144\n",
      "Training loss: 3481636497435.3066 503 2.4120654659634337\n",
      "Training loss: 3515449522257.92 504 2.4122314253411457\n",
      "Training loss: 3522354353274.88 505 2.412467705059803\n",
      "Training loss: 3528573108005.547 506 2.412648946452917\n",
      "Training loss: 3533108996123.3066 507 2.4127327498066884\n",
      "Training loss: 3519871996395.52 508 2.4129391802556097\n",
      "Training loss: 3513565105356.8 509 2.4129305245979054\n",
      "Training loss: 3515581726720.0 510 2.4128705952823775\n",
      "Training loss: 3500894057048.7466 511 2.4128619544552237\n",
      "Training loss: 3495249754193.92 512 2.4129343153104403\n",
      "Training loss: 3505790543462.4 513 2.4128372516194863\n",
      "Training loss: 3509606353469.44 514 2.4127297853430973\n",
      "Training loss: 3498486862097.067 515 2.4126491335447335\n",
      "Training loss: 3495413947214.507 516 2.412500782540582\n",
      "Training loss: 3505564610286.933 517 2.412502638049218\n",
      "Training loss: 3519320808925.8667 518 2.4124304122155626\n",
      "Training loss: 3505502870132.053 519 2.4124700998469453\n",
      "Training loss: 3504930878914.56 520 2.4125631156198866\n",
      "Training loss: 3501238996609.7065 521 2.4126725476067827\n",
      "Training loss: 3511828327956.48 522 2.4127705236981765\n",
      "Training loss: 3503828951367.68 523 2.4126943000150627\n",
      "Training loss: 3519099573370.88 524 2.4130625992626893\n",
      "Training loss: 3509236136236.3735 525 2.413424544435049\n",
      "Training loss: 3518933590780.5864 526 2.413537691368467\n",
      "Training loss: 3520578652733.44 527 2.413547599068788\n",
      "Training loss: 3507070756891.3066 528 2.413234747582152\n",
      "Training loss: 3485988059873.28 529 2.412905076658703\n",
      "Training loss: 3523663870907.7334 530 2.412510780206033\n",
      "Training loss: 3513058433433.6 531 2.4124581137892673\n",
      "Training loss: 3525342934685.013 532 2.4124693103854353\n",
      "Training loss: 3496708924593.493 533 2.412562488784153\n",
      "Training loss: 3520967660448.427 534 2.412552773047563\n",
      "Training loss: 3493189064676.6934 535 2.4126506340016656\n",
      "Training loss: 3498751494717.44 536 2.4128649420526482\n",
      "Training loss: 3501923954414.933 537 2.4129150804152713\n",
      "Training loss: 3488912440470.1865 538 2.4130444699465854\n",
      "Training loss: 3495326258298.88 539 2.4130671887643484\n",
      "Training loss: 3498317747759.7866 540 2.412933296755275\n",
      "Training loss: 3524435846539.947 541 2.4130399414229915\n",
      "Training loss: 3508603523345.067 542 2.4129952306927183\n",
      "Training loss: 3485493914938.027 543 2.412921615578688\n",
      "Training loss: 3523025441914.88 544 2.4127903454982613\n",
      "Training loss: 3517890942730.24 545 2.4125932584408667\n",
      "Training loss: 3511052549488.64 546 2.412423837385614\n",
      "Training loss: 3509191396993.7065 547 2.412242725378678\n",
      "Training loss: 3500502588675.4136 548 2.412129018853932\n",
      "Training loss: 3512716849315.84 549 2.412038181775784\n",
      "Training loss: 3498612579368.96 550 2.4122148636035927\n",
      "Training loss: 3495762242218.6665 551 2.41244905807937\n",
      "Training loss: 3507015280230.4 552 2.4127920197481556\n",
      "Training loss: 3517998316912.64 553 2.413011936876059\n",
      "Training loss: 3488344923176.96 554 2.413125220720422\n",
      "Training loss: 3512457585404.5864 555 2.413248044508003\n",
      "Training loss: 3499514298804.9067 556 2.413300211083855\n",
      "Training loss: 3514199507817.8135 557 2.41300899763066\n",
      "Training loss: 3504704050954.24 558 2.4126985601728586\n",
      "Training loss: 3505846467515.7334 559 2.412685532707951\n",
      "Training loss: 3499526378400.427 560 2.4126450796812193\n",
      "Training loss: 3516093096263.68 561 2.412601328200031\n",
      "Training loss: 3503153388803.4136 562 2.4124518380950715\n",
      "Training loss: 3529917298551.467 563 2.4123337910136082\n",
      "Training loss: 3525582513329.493 564 2.4123058312996686\n",
      "Training loss: 3506562295398.4 565 2.4122336608844437\n",
      "Training loss: 3513089303511.04 566 2.4122470692598417\n",
      "Training loss: 3505638653733.547 567 2.4122646913317936\n",
      "Training loss: 3514667927688.533 568 2.412440985092343\n",
      "Training loss: 3506788675966.2935 569 2.4126947871801\n",
      "Training loss: 3512879476462.933 570 2.4129181090690808\n",
      "Training loss: 3488893649988.2666 571 2.413248997282416\n",
      "Training loss: 3517879981615.7866 572 2.4136275763512107\n",
      "Training loss: 3498629356584.96 573 2.4139940894625678\n",
      "Training loss: 3503914850713.6 574 2.41410812017994\n",
      "Training loss: 3506773688320.0 575 2.413913330723357\n",
      "Training loss: 3494107561328.64 576 2.4134603619757957\n",
      "Training loss: 3507170301706.24 577 2.412979460829818\n",
      "Training loss: 3508950252475.7334 578 2.412831673511994\n",
      "Training loss: 3529996710707.2 579 2.4126124028644576\n",
      "Training loss: 3504369177722.88 580 2.41252797867952\n",
      "Training loss: 3512724454987.0933 581 2.4124107355983724\n",
      "Training loss: 3526679519559.68 582 2.4124465814690934\n",
      "Training loss: 3515373912937.8135 583 2.4125562013808053\n",
      "Training loss: 3515737866676.9067 584 2.412837431200979\n",
      "Training loss: 3516428416887.467 585 2.413015958976371\n",
      "Training loss: 3503967195627.52 586 2.4131978900517055\n",
      "Training loss: 3487170965449.3867 587 2.413247231325883\n",
      "Training loss: 3512845922030.933 588 2.4132762651218456\n",
      "Training loss: 3504444115954.3467 589 2.413274226170681\n",
      "Training loss: 3530871810293.76 590 2.4132012983131883\n",
      "Training loss: 3532349100086.6133 591 2.4130438725988266\n",
      "Training loss: 3508616050333.013 592 2.4130498395413738\n",
      "Training loss: 3500493193434.453 593 2.4131762712139397\n",
      "Training loss: 3510298245857.28 594 2.413330376818395\n",
      "Training loss: 3515171691560.96 595 2.4135397234552447\n",
      "Training loss: 3513091764169.3867 596 2.413709079700017\n",
      "Training loss: 3519200460363.0933 597 2.4137833394937482\n",
      "Training loss: 3499975336700.5864 598 2.4138461480341484\n",
      "Training loss: 3497104419498.6665 599 2.4137830371286695\n",
      "Training loss: 3511457215938.56 600 2.413566569515665\n",
      "Training loss: 3493121061027.84 601 2.413373628032234\n",
      "Training loss: 3487170741753.1733 602 2.413389619711057\n",
      "Training loss: 3513148135615.1465 603 2.41350757873291\n",
      "Training loss: 3515061633024.0 604 2.413513495847827\n",
      "Training loss: 3499334894441.8135 605 2.4134620758856395\n",
      "Training loss: 3502061751282.3467 606 2.4136141386094394\n",
      "Training loss: 3529020500432.2134 607 2.413447199640301\n",
      "Training loss: 3519153036765.8667 608 2.41323188115792\n",
      "Training loss: 3531532832604.16 609 2.4128499442947393\n",
      "Training loss: 3488390780900.6934 610 2.4126965900934656\n",
      "Training loss: 3517204866443.947 611 2.412674089086561\n",
      "Training loss: 3480616442702.507 612 2.412965595281223\n",
      "Training loss: 3512400319173.973 613 2.413399927218621\n",
      "Training loss: 3522601313894.4 614 2.4136957023661147\n",
      "Training loss: 3500076671085.2266 615 2.413821520172407\n",
      "Training loss: 3525891885192.533 616 2.413659245034487\n",
      "Training loss: 3478730459927.8936 617 2.4133841566284953\n",
      "Training loss: 3497831432192.0 618 2.413089825039368\n",
      "Training loss: 3522804430056.1064 619 2.4127033887603364\n",
      "Training loss: 3506199012747.947 620 2.412615050459622\n",
      "Training loss: 3516261986904.7466 621 2.4127472113151236\n",
      "Training loss: 3518428932123.3066 622 2.4130709477420487\n",
      "Training loss: 3517574412588.3735 623 2.413674635679489\n",
      "Training loss: 3498304102290.7734 624 2.4141971135817952\n",
      "Training loss: 3537671056698.027 625 2.414576678581987\n",
      "Training loss: 3517265711813.973 626 2.414523280116409\n",
      "Training loss: 3542627717393.067 627 2.4141010525560413\n",
      "Training loss: 3512192952784.2134 628 2.413434854867614\n",
      "Training loss: 3510015717539.84 629 2.4128312327704937\n",
      "Training loss: 3501342567956.48 630 2.412497105508223\n",
      "Training loss: 3508670408512.8535 631 2.4124760934051173\n",
      "Training loss: 3496111208311.467 632 2.412715130581744\n",
      "Training loss: 3525873542103.04 633 2.412912070410155\n",
      "Training loss: 3500959600039.2534 634 2.4132809055906588\n",
      "Training loss: 3498081524558.507 635 2.413692488794958\n",
      "Training loss: 3525251890326.1865 636 2.4139248587119133\n",
      "Training loss: 3498429595866.453 637 2.4140947700406143\n",
      "Training loss: 3506217355837.44 638 2.414113645602753\n",
      "Training loss: 3512187360378.88 639 2.4139157700915277\n",
      "Training loss: 3504373427950.933 640 2.413676638623616\n",
      "Training loss: 3498029850733.2266 641 2.413479779565953\n",
      "Training loss: 3527690402747.7334 642 2.4130816891476385\n",
      "Training loss: 3504912535825.067 643 2.4128517622289176\n",
      "Training loss: 3524909858816.0 644 2.4127046222416673\n",
      "Training loss: 3519410734803.6265 645 2.4126375066192844\n",
      "Training loss: 3519707803374.933 646 2.4128864192802677\n",
      "Training loss: 3519595507875.84 647 2.413229111062099\n",
      "Training loss: 3533162012125.8667 648 2.413592070646713\n",
      "Training loss: 3516409402709.3335 649 2.4139379664572886\n",
      "Training loss: 3505316531186.3467 650 2.4140322577224196\n",
      "Training loss: 3506199907532.8 651 2.4140974481876247\n",
      "Training loss: 3516872677567.1465 652 2.414013955325668\n",
      "Training loss: 3521923738064.2134 653 2.4138070605613744\n",
      "Training loss: 3510267375779.84 654 2.4135429202231684\n",
      "Training loss: 3514722733260.8 655 2.4132866218391804\n",
      "Training loss: 3515906086229.3335 656 2.413082480252749\n",
      "Training loss: 3484578773729.28 657 2.4130420211604866\n",
      "Training loss: 3499077867492.6934 658 2.4130255515656547\n",
      "Training loss: 3528503985875.6265 659 2.4130598235985943\n",
      "Training loss: 3504888824026.453 660 2.41329469356569\n",
      "Training loss: 3486768759657.8135 661 2.4137505744507353\n",
      "Training loss: 3479975776747.52 662 2.4140432890422128\n",
      "Training loss: 3507928855565.6533 663 2.4141933120265935\n",
      "Training loss: 3516506710562.1333 664 2.414154858246911\n",
      "Training loss: 3496890789614.933 665 2.413902658409116\n",
      "Training loss: 3479196642836.48 666 2.4136561023336816\n",
      "Training loss: 3491739289518.08 667 2.4136841746135054\n",
      "Training loss: 3508996110199.467 668 2.4136118580328536\n",
      "Training loss: 3496645618565.12 669 2.4137005594198233\n",
      "Training loss: 3507941606249.8135 670 2.413693904316856\n",
      "Training loss: 3502662823007.573 671 2.413653254912993\n",
      "Training loss: 3508563929115.3066 672 2.413672626691882\n",
      "Training loss: 3512570104599.8936 673 2.4137873194799844\n",
      "Training loss: 3508108483624.96 674 2.4138608413688654\n",
      "Training loss: 3526386253824.0 675 2.4138347850185173\n",
      "Training loss: 3497696319679.1465 676 2.4137888902823406\n",
      "Training loss: 3529647744614.4 677 2.4136174631245106\n",
      "Training loss: 3501016418877.44 678 2.413584656588013\n",
      "Training loss: 3501497142039.8936 679 2.4135335570033503\n",
      "Training loss: 3523693622504.1064 680 2.413407994536941\n",
      "Training loss: 3496482544025.6 681 2.4133674162922465\n",
      "Training loss: 3504258224401.067 682 2.4133460475367214\n",
      "Training loss: 3497679318766.933 683 2.4133799022944955\n",
      "Training loss: 3522784073700.6934 684 2.413477901697799\n",
      "Training loss: 3508316744799.573 685 2.413560651771267\n",
      "Training loss: 3493070729379.84 686 2.4138291687367346\n",
      "Training loss: 3524264718936.7466 687 2.4139620929670342\n",
      "Training loss: 3515140597787.3066 688 2.414215542079255\n",
      "Training loss: 3498807195074.56 689 2.414344395000493\n",
      "Training loss: 3507825955307.52 690 2.4144256212458117\n",
      "Training loss: 3507470502024.533 691 2.414510033106166\n",
      "Training loss: 3522914712289.28 692 2.414409783536461\n",
      "Training loss: 3514089672977.067 693 2.414149325317147\n",
      "Training loss: 3488334633151.1465 694 2.41381861057155\n",
      "Training loss: 3496842247536.64 695 2.4135363893051784\n",
      "Training loss: 3499961691231.573 696 2.4133828803756687\n",
      "Training loss: 3510984993232.2134 697 2.413369726479722\n",
      "Training loss: 3512903188261.547 698 2.4135568778745524\n",
      "Training loss: 3547043704340.48 699 2.4136910172236665\n",
      "Training loss: 3495291809082.027 700 2.4137928772435107\n",
      "Training loss: 3503525171909.973 701 2.4138675717185505\n",
      "Training loss: 3512227402001.067 702 2.413759048048724\n",
      "Training loss: 3510136737191.2534 703 2.413514547020574\n",
      "Training loss: 3515739656246.6133 704 2.413149897345445\n",
      "Training loss: 3509054718607.36 705 2.4129950706925465\n",
      "Training loss: 3520046926834.3467 706 2.4129115489710826\n",
      "Training loss: 3482671092421.973 707 2.4131332292674283\n",
      "Training loss: 3485040035321.1733 708 2.4135151864132744\n",
      "Training loss: 3489223825599.1465 709 2.413955305532673\n",
      "Training loss: 3498532272428.3735 710 2.41430888371815\n",
      "Training loss: 3523574392422.4 711 2.414324961853149\n",
      "Training loss: 3508403538930.3467 712 2.4145531946790078\n",
      "Training loss: 3519330651559.2534 713 2.414520681880304\n",
      "Training loss: 3504912535825.067 714 2.414428420882362\n",
      "Training loss: 3524846552787.6265 715 2.4142245138568343\n",
      "Training loss: 3497360551662.933 716 2.414177378828093\n",
      "Training loss: 3509611498482.3467 717 2.4141665728567734\n",
      "Training loss: 3533993043558.4 718 2.4139807477660926\n",
      "Training loss: 3517581794563.4136 719 2.413881761508133\n",
      "Training loss: 3514582923127.467 720 2.4138172451257103\n",
      "Training loss: 3515817502528.8535 721 2.4137723973108764\n",
      "Training loss: 3531968816523.947 722 2.4137546758549857\n",
      "Training loss: 3501042143941.973 723 2.4137681522761385\n",
      "Training loss: 3495597601805.6533 724 2.413899423270816\n",
      "Training loss: 3522209174432.427 725 2.413978882122103\n",
      "Training loss: 3492339242762.24 726 2.414131771437949\n",
      "Training loss: 3513445204186.453 727 2.414340711694422\n",
      "Training loss: 3499286799755.947 728 2.4146694481489286\n",
      "Training loss: 3495184434899.6265 729 2.4149445401639986\n",
      "Training loss: 3506057860437.3335 730 2.414953459320423\n",
      "Training loss: 3527561330032.64 731 2.414721064322111\n",
      "Training loss: 3507429118225.067 732 2.414262541183071\n",
      "Training loss: 3506200354925.2266 733 2.4140341778173435\n",
      "Training loss: 3502973537047.8936 734 2.4138759227987574\n",
      "Training loss: 3479270909979.3066 735 2.4139170395130476\n",
      "Training loss: 3508986043869.8667 736 2.4138362240930498\n",
      "Training loss: 3499374041279.1465 737 2.41380768576782\n",
      "Training loss: 3517240434141.8667 738 2.413842976198902\n",
      "Training loss: 3496359287412.053 739 2.414082228171241\n",
      "Training loss: 3507693079756.8 740 2.4144834766321392\n",
      "Training loss: 3493045004315.3066 741 2.414886897197284\n",
      "Training loss: 3507007003470.507 742 2.415065342500304\n",
      "Training loss: 3505035568742.4 743 2.415274781601961\n",
      "Training loss: 3512191386910.72 744 2.4152708810214754\n",
      "Training loss: 3509073956481.7065 745 2.4149402268176474\n",
      "Training loss: 3497104419498.6665 746 2.4146522514244575\n",
      "Training loss: 3532792018589.013 747 2.4142388047622783\n",
      "Training loss: 3515106372266.6665 748 2.4137471703012263\n",
      "Training loss: 3538165201633.28 749 2.413288915696013\n",
      "Training loss: 3487534919188.48 750 2.413137596347284\n",
      "Training loss: 3515699167232.0 751 2.4133949034754485\n",
      "Training loss: 3505489000966.8267 752 2.4139111637764166\n",
      "Training loss: 3499959454269.44 753 2.4145385884891777\n",
      "Training loss: 3513150819969.7065 754 2.415048614173823\n",
      "Training loss: 3525482297425.92 755 2.4154141248281262\n",
      "Training loss: 3513308973192.533 756 2.415698142349924\n",
      "Training loss: 3522728820736.0 757 2.4156219443676257\n",
      "Training loss: 3513034497938.7734 758 2.4152823667966064\n",
      "Training loss: 3515531171375.7866 759 2.414760138625919\n",
      "Training loss: 3511048522956.8 760 2.4141736346621148\n",
      "Training loss: 3510731992814.933 761 2.413633468486883\n",
      "Training loss: 3495823534981.12 762 2.413281831433373\n",
      "Training loss: 3517256092876.8 763 2.4133189302504148\n",
      "Training loss: 3490034053283.84 764 2.4135283584029383\n",
      "Training loss: 3495502083522.56 765 2.4139287056299485\n",
      "Training loss: 3504857282860.3735 766 2.414520177104794\n",
      "Training loss: 3502487445176.32 767 2.4150402944705873\n",
      "Training loss: 3514172440576.0 768 2.4153651790958555\n",
      "Training loss: 3527638505226.24 769 2.415471622017223\n",
      "Training loss: 3516032027197.44 770 2.4152684227093926\n",
      "Training loss: 3505918497696.427 771 2.4149025185567825\n",
      "Training loss: 3514840621165.2266 772 2.41439413442664\n",
      "Training loss: 3512261627521.7065 773 2.413872040318534\n",
      "Training loss: 3530335386774.1865 774 2.4135552536232656\n",
      "Training loss: 3509079548887.04 775 2.413190592823753\n",
      "Training loss: 3510737585220.2666 776 2.413034884312397\n",
      "Training loss: 3518503646658.56 777 2.413339385945846\n",
      "Training loss: 3520751122513.92 778 2.413894895966499\n",
      "Training loss: 3513215468175.36 779 2.4145700210206757\n",
      "Training loss: 3515747709310.2935 780 2.4151589002621665\n",
      "Training loss: 3518861784296.1064 781 2.4154739265524974\n",
      "Training loss: 3503527408872.1064 782 2.4154633922685154\n",
      "Training loss: 3498354881331.2 783 2.4153513648068623\n",
      "Training loss: 3491095044423.68 784 2.415180510616462\n",
      "Training loss: 3532238370461.013 785 2.4147160042787443\n",
      "Training loss: 3501446810391.8936 786 2.4141757256365266\n",
      "Training loss: 3517383599718.4 787 2.413635952267266\n",
      "Training loss: 3501282617371.3066 788 2.41354552506965\n",
      "Training loss: 3504760646096.2134 789 2.413791942269546\n",
      "Training loss: 3498446820474.88 790 2.414309126348989\n",
      "Training loss: 3510797983197.8667 791 2.4150774313658285\n",
      "Training loss: 3523902331071.1465 792 2.4155444888900592\n",
      "Training loss: 3516956787343.36 793 2.4156475937654394\n",
      "Training loss: 3505856981237.76 794 2.415630050567046\n",
      "Training loss: 3514745997666.987 795 2.4152071151826457\n",
      "Training loss: 3508118997346.987 796 2.414777125056148\n",
      "Training loss: 3508725661477.547 797 2.4142987720006315\n",
      "Training loss: 3505473789624.32 798 2.4141786391940823\n",
      "Training loss: 3516322161186.1333 799 2.413994352668634\n",
      "Training loss: 3497490742859.0933 800 2.413980572255603\n",
      "Training loss: 3527421296203.0933 801 2.414096572425842\n",
      "Training loss: 3514601266216.96 802 2.4141849044573442\n",
      "Training loss: 3497354064472.7466 803 2.4144182229052933\n",
      "Training loss: 3475440336022.1865 804 2.414816693249549\n",
      "Training loss: 3494921591848.96 805 2.4151340365821747\n",
      "Training loss: 3505074491883.52 806 2.4155158813452844\n",
      "Training loss: 3525501311604.053 807 2.4154901596687295\n",
      "Training loss: 3503428087753.3867 808 2.415384214364121\n",
      "Training loss: 3519643602561.7065 809 2.4150099751903604\n",
      "Training loss: 3527881215617.7065 810 2.4145755696913995\n",
      "Training loss: 3495473897799.68 811 2.414235926004431\n",
      "Training loss: 3515634742722.56 812 2.4141076434600865\n",
      "Training loss: 3520621155013.973 813 2.414210518191771\n",
      "Training loss: 3523477531962.027 814 2.4142112441111148\n",
      "Training loss: 3494788268905.8135 815 2.4145754864568545\n",
      "Training loss: 3504988816233.8135 816 2.4150567938393737\n",
      "Training loss: 3496626156994.56 817 2.4155301550154067\n",
      "Training loss: 3497119407144.96 818 2.415835058292413\n",
      "Training loss: 3519536899467.947 819 2.4158934348347962\n",
      "Training loss: 3487563104911.36 820 2.4159392767694996\n",
      "Training loss: 3523135500451.84 821 2.415700469203112\n",
      "Training loss: 3503654692017.493 822 2.4153522319535625\n",
      "Training loss: 3486878147106.1333 823 2.4150396851190448\n",
      "Training loss: 3517911299085.6533 824 2.4145907926733208\n",
      "Training loss: 3514913546130.7734 825 2.4142315153134772\n",
      "Training loss: 3505080531681.28 826 2.4141876364621964\n",
      "Training loss: 3505440235192.32 827 2.4142001830870505\n",
      "Training loss: 3501819711979.52 828 2.4145460984829548\n",
      "Training loss: 3503619795408.2134 829 2.415141696339023\n",
      "Training loss: 3522116116807.68 830 2.4156705042718634\n",
      "Training loss: 3534107352323.4136 831 2.4159977698557245\n",
      "Training loss: 3524899121397.76 832 2.4161304506819734\n",
      "Training loss: 3523786232736.427 833 2.416007255869943\n",
      "Training loss: 3492188024122.027 834 2.4157182952537735\n",
      "Training loss: 3505235553157.12 835 2.415226043396302\n",
      "Training loss: 3515881703342.08 836 2.4147711549242032\n",
      "Training loss: 3473902424555.52 837 2.414545447281389\n",
      "Training loss: 3495656881302.1865 838 2.414552863610631\n",
      "Training loss: 3511401291885.2266 839 2.414672206902926\n",
      "Training loss: 3518111954589.013 840 2.414942105383121\n",
      "Training loss: 3513859489573.547 841 2.4152183643315657\n",
      "Training loss: 3500641951416.32 842 2.4155036382690542\n",
      "Training loss: 3516119268720.64 843 2.4157258740623218\n",
      "Training loss: 3532405247836.16 844 2.4158309561260825\n",
      "Training loss: 3505004474968.7466 845 2.415535556392616\n",
      "Training loss: 3512363185602.56 846 2.415101001612699\n",
      "Training loss: 3529709932161.7065 847 2.414658643382581\n",
      "Training loss: 3511148515164.16 848 2.4145796420567422\n",
      "Training loss: 3503149585967.7866 849 2.4148087289630378\n",
      "Training loss: 3488955166446.933 850 2.415062543791266\n",
      "Training loss: 3539864845462.1865 851 2.4152805095202186\n",
      "Training loss: 3520714212638.72 852 2.4152393525409876\n",
      "Training loss: 3497096813827.4136 853 2.4153745333958856\n",
      "Training loss: 3499372699101.8667 854 2.4153633269291603\n",
      "Training loss: 3515099885076.48 855 2.4154089471818567\n",
      "Training loss: 3511694333924.6934 856 2.4154300690603288\n",
      "Training loss: 3528381847743.1465 857 2.4153145172014723\n",
      "Training loss: 3500033497716.053 858 2.4151719054809613\n",
      "Training loss: 3510551022578.3467 859 2.4151259796704174\n",
      "Training loss: 3477371952824.32 860 2.4151897025074605\n",
      "Training loss: 3528361043995.3066 861 2.4152509579496693\n",
      "Training loss: 3521656421089.28 862 2.415301237089881\n",
      "Training loss: 3497264362291.2 863 2.415591924774132\n",
      "Training loss: 3499373817582.933 864 2.4157951610385644\n",
      "Training loss: 3515941206534.8267 865 2.4157738889409153\n",
      "Training loss: 3507206987885.2266 866 2.415626070324718\n",
      "Training loss: 3501492668115.6265 867 2.4154646575709404\n",
      "Training loss: 3526033037503.1465 868 2.415401981448552\n",
      "Training loss: 3505311833565.8667 869 2.415486208949089\n",
      "Training loss: 3528063304335.36 870 2.415554929161903\n",
      "Training loss: 3504013948136.1064 871 2.415462969116692\n",
      "Training loss: 3504887258152.96 872 2.415392394559757\n",
      "Training loss: 3520100613925.547 873 2.415237401985151\n",
      "Training loss: 3509989321386.6665 874 2.415212274312201\n",
      "Training loss: 3521084877264.2134 875 2.4152856165589043\n",
      "Training loss: 3504443668561.92 876 2.4153172516679597\n",
      "Training loss: 3511497033864.533 877 2.415228974569578\n",
      "Training loss: 3512247087267.84 878 2.415161342110881\n",
      "Training loss: 3503876374964.9067 879 2.415221350112681\n",
      "Training loss: 3526274405717.3335 880 2.415347687549729\n",
      "Training loss: 3511700597418.6665 881 2.415552959355978\n",
      "Training loss: 3513938901729.28 882 2.415697278139092\n",
      "Training loss: 3522884289604.2666 883 2.415632134546762\n",
      "Training loss: 3525608462090.24 884 2.415709132301056\n",
      "Training loss: 3506377746022.4 885 2.4158174219475046\n",
      "Training loss: 3541072805014.1865 886 2.4157508330961743\n",
      "Training loss: 3531693222789.12 887 2.4154961278006826\n",
      "Training loss: 3532334559832.7466 888 2.4151464185081597\n",
      "Training loss: 3504577438897.493 889 2.4148953143276266\n",
      "Training loss: 3500563434045.44 890 2.4149156422352434\n",
      "Training loss: 3489806554234.88 891 2.415164792881335\n",
      "Training loss: 3495521992485.547 892 2.4154847156345842\n",
      "Training loss: 3516409402709.3335 893 2.4158034721629087\n",
      "Training loss: 3504174114624.8535 894 2.4160282158316697\n",
      "Training loss: 3508732596060.16 895 2.4161867942265456\n",
      "Training loss: 3523227886987.947 896 2.41627046059334\n",
      "Training loss: 3512927571148.8 897 2.416092901970516\n",
      "Training loss: 3515261617438.72 898 2.4158186673131374\n",
      "Training loss: 3503969879982.08 899 2.4154089920400827\n",
      "Training loss: 3514339765343.573 900 2.414920655229428\n",
      "Training loss: 3509170816942.08 901 2.4147244319546446\n",
      "Training loss: 3479613836274.3467 902 2.41499331338408\n",
      "Training loss: 3519865061812.9067 903 2.415377934385659\n",
      "Training loss: 3508717384717.6533 904 2.4157764416515586\n",
      "Training loss: 3500050274932.053 905 2.416154259340379\n",
      "Training loss: 3519822335836.16 906 2.416371142729444\n",
      "Training loss: 3527638505226.24 907 2.416186402512546\n",
      "Training loss: 3495360931211.947 908 2.4159860292740483\n",
      "Training loss: 3546304612051.6265 909 2.415816332698901\n",
      "Training loss: 3511653173821.44 910 2.415522003536314\n",
      "Training loss: 3496097339146.24 911 2.415347603061785\n",
      "Training loss: 3508410249816.7466 912 2.41528600684031\n",
      "Training loss: 3507472291594.24 913 2.4153482616543807\n",
      "Training loss: 3507937356021.76 914 2.415519393899463\n",
      "Training loss: 3486526944051.2 915 2.415789742888947\n",
      "Training loss: 3464940706856.96 916 2.4162117185488317\n",
      "Training loss: 3507611206942.72 917 2.4164594837410074\n",
      "Training loss: 3515649954065.067 918 2.4165888363028523\n",
      "Training loss: 3471214714552.32 919 2.4166755260023516\n",
      "Training loss: 3496339825841.493 920 2.4167435381253\n",
      "Training loss: 3511224124484.2666 921 2.4163552773332415\n",
      "Training loss: 3523620250146.1333 922 2.4159263315308386\n",
      "Training loss: 3513767103037.44 923 2.4154875341196376\n",
      "Training loss: 3509477504450.56 924 2.415232229952977\n",
      "Training loss: 3492573900090.027 925 2.4150826651412416\n",
      "Training loss: 3499563064579.4136 926 2.415123187714595\n",
      "Training loss: 3511520298270.72 927 2.415149611658808\n",
      "Training loss: 3525475586539.52 928 2.415399905356356\n",
      "Training loss: 3490620137362.7734 929 2.4157887775141744\n",
      "Training loss: 3517969907493.547 930 2.416140358177334\n",
      "Training loss: 3489201679674.027 931 2.416497584109708\n",
      "Training loss: 3524491546897.067 932 2.416872769870004\n",
      "Training loss: 3514253865997.6533 933 2.416937614498322\n",
      "Training loss: 3504697563764.053 934 2.416875958750758\n",
      "Training loss: 3526410189318.8267 935 2.4166158961263204\n",
      "Training loss: 3533809836359.68 936 2.4160785118636614\n",
      "Training loss: 3526326303238.8267 937 2.415511902871897\n",
      "Training loss: 3510188634712.7466 938 2.415206871630175\n",
      "Training loss: 3500613094604.8 939 2.4150211981865666\n",
      "Training loss: 3528431731998.72 940 2.4151060109328686\n",
      "Training loss: 3484474531293.8667 941 2.415417690864296\n",
      "Training loss: 3520950883232.427 942 2.415867814634645\n",
      "Training loss: 3530727749932.3735 943 2.4162194546030533\n",
      "Training loss: 3506448434025.8135 944 2.416527449731373\n",
      "Training loss: 3502750959315.6265 945 2.4166965046209414\n",
      "Training loss: 3519291728418.1333 946 2.4167095635089573\n",
      "Training loss: 3511397265353.3867 947 2.4166551115599804\n",
      "Training loss: 3500860726312.96 948 2.416267541768676\n",
      "Training loss: 3504368954026.6665 949 2.4160988292419066\n",
      "Training loss: 3521060046984.533 950 2.416065371340726\n",
      "Training loss: 3503790923011.4136 951 2.415962395818899\n",
      "Training loss: 3501667374858.24 952 2.415867631576703\n",
      "Training loss: 3496986978986.6665 953 2.4158465290312443\n",
      "Training loss: 3510027573439.1465 954 2.415757195480736\n",
      "Training loss: 3521307902388.9067 955 2.4156459293830888\n",
      "Training loss: 3518715934365.013 956 2.4155115431253344\n",
      "Training loss: 3519323045888.0 957 2.415254761827563\n",
      "Training loss: 3506813729942.1865 958 2.4151566961644684\n",
      "Training loss: 3501830896790.1865 959 2.4152073319046834\n",
      "Training loss: 3523030139535.36 960 2.4157465444525283\n",
      "Training loss: 3506808137536.8535 961 2.4162333433863488\n",
      "Training loss: 3491491210417.493 962 2.4167803648066637\n",
      "Training loss: 3485132869249.7065 963 2.4171609761736415\n",
      "Training loss: 3517284725992.1064 964 2.417310913456568\n",
      "Training loss: 3491795213571.4136 965 2.417144340527451\n",
      "Training loss: 3502383650133.3335 966 2.4168354164947243\n",
      "Training loss: 3510440964041.3867 967 2.4166272913198212\n",
      "Training loss: 3497493874606.08 968 2.4162822496556813\n",
      "Training loss: 3505048095730.3467 969 2.4159962481028425\n",
      "Training loss: 3511820722285.2266 970 2.4156842297638286\n",
      "Training loss: 3470275190456.32 971 2.4155169038404454\n",
      "Training loss: 3532300781704.533 972 2.4155760201398273\n",
      "Training loss: 3546435921728.8535 973 2.4156809177398793\n",
      "Training loss: 3509915277940.053 974 2.4157852368151578\n",
      "Training loss: 3503074647736.32 975 2.416049048480423\n",
      "Training loss: 3527382596758.1865 976 2.416207262030949\n",
      "Training loss: 3494661209456.64 977 2.4164522259251484\n",
      "Training loss: 3526726495764.48 978 2.416442268947446\n",
      "Training loss: 3500083829364.053 979 2.416238832155198\n",
      "Training loss: 3509795824162.1333 980 2.416060698059601\n",
      "Training loss: 3506266345308.16 981 2.4158692498050547\n",
      "Training loss: 3495727345609.3867 982 2.4158510232929267\n",
      "Training loss: 3505261054525.44 983 2.4158709819365796\n",
      "Training loss: 3497674173754.027 984 2.4159164268701603\n",
      "Training loss: 3499420346395.3066 985 2.4158906945167793\n",
      "Training loss: 3512838987448.32 986 2.41605349485815\n",
      "Training loss: 3517193457937.067 987 2.4162759188876084\n",
      "Training loss: 3516567779628.3735 988 2.4165234974630687\n",
      "Training loss: 3491554292749.6533 989 2.4167914204812897\n",
      "Training loss: 3517650469300.9067 990 2.4169800641329946\n",
      "Training loss: 3500726732281.1733 991 2.4171952766536524\n",
      "Training loss: 3490593741209.6 992 2.4172371763051124\n",
      "Training loss: 3512270799066.453 993 2.417187353046912\n",
      "Training loss: 3497116722790.4 994 2.4170370709620066\n",
      "Training loss: 3502110293360.64 995 2.4169923897731103\n",
      "Training loss: 3480260542027.0933 996 2.4170331686049358\n",
      "Training loss: 3491971486187.52 997 2.416836545500248\n",
      "Training loss: 3516411639671.467 998 2.4164968453311606\n",
      "Training loss: 3508951818349.2266 999 2.4162513220742436\n"
     ]
    }
   ],
   "source": [
    "training_loop(1000, optimizer, model, criterion, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10573585594013.014 0 3.1397525732674727\n",
      "Training loss: 10512898153512.96 1 3.1388177355547544\n",
      "Training loss: 10543762862243.84 2 3.1377827375897898\n",
      "Training loss: 10515998135637.334 3 3.13667740496998\n",
      "Training loss: 10510647769606.826 4 3.1355529359430303\n",
      "Training loss: 10513249803960.32 5 3.1343725817679324\n",
      "Training loss: 10503932409282.56 6 3.133241553355136\n",
      "Training loss: 10561778907873.28 7 3.1321661875337226\n",
      "Training loss: 10481497916047.36 8 3.13107002413294\n",
      "Training loss: 10487322741746.346 9 3.1300028084628617\n",
      "Training loss: 10568049783821.654 10 3.128910874232627\n",
      "Training loss: 10490753346874.027 11 3.127830488030291\n",
      "Training loss: 10495931466820.268 12 3.126721356909152\n",
      "Training loss: 10546181465702.4 13 3.125651046042983\n",
      "Training loss: 10545868291003.732 14 3.124601692893663\n",
      "Training loss: 10584780023616.854 15 3.1235416795716713\n",
      "Training loss: 10526557939083.947 16 3.122504329703179\n",
      "Training loss: 10551499395781.973 17 3.1214965065540574\n",
      "Training loss: 10478010044689.066 18 3.1205219832455864\n",
      "Training loss: 10476530965326.506 19 3.1195968232888966\n",
      "Training loss: 10488639865050.453 20 3.1186756642559095\n",
      "Training loss: 10456401437873.494 21 3.117694821897282\n",
      "Training loss: 10594122919662.934 22 3.116698067096887\n",
      "Training loss: 10589796411200.854 23 3.115701615796094\n",
      "Training loss: 10478220990218.24 24 3.114670341704168\n",
      "Training loss: 10558552984780.8 25 3.1136572918641576\n",
      "Training loss: 10515170235951.787 26 3.1126399707125394\n",
      "Training loss: 10518424344767.146 27 3.1115852317761226\n",
      "Training loss: 10534756405302.613 28 3.110538017920597\n",
      "Training loss: 10604150326121.812 29 3.109516692211064\n",
      "Training loss: 10466843129719.467 30 3.108568674104311\n",
      "Training loss: 10507005100468.906 31 3.1076611261673848\n",
      "Training loss: 10526956342039.893 32 3.1067392067635224\n",
      "Training loss: 10546064696279.04 33 3.1058170498279853\n",
      "Training loss: 10568263861097.812 34 3.1049103524152506\n",
      "Training loss: 10586190428241.92 35 3.1040450838177165\n",
      "Training loss: 10573770143389.014 36 3.1031948145993673\n",
      "Training loss: 10566122640943.787 37 3.1023271164102164\n",
      "Training loss: 10608218465457.494 38 3.1014393058187464\n",
      "Training loss: 10546659057117.867 39 3.10051543387252\n",
      "Training loss: 10541486082184.533 40 3.0995744507358616\n",
      "Training loss: 10516096114578.773 41 3.0986082852537318\n",
      "Training loss: 10546070959773.014 42 3.097625620467942\n",
      "Training loss: 10536612860177.066 43 3.096628061241761\n",
      "Training loss: 10515023267539.627 44 3.0956727919977385\n",
      "Training loss: 10538295055701.334 45 3.094778765141154\n",
      "Training loss: 10489592587223.04 46 3.0938864785442157\n",
      "Training loss: 10546293313809.066 47 3.0930147277369557\n",
      "Training loss: 10566988568985.6 48 3.0921380999995103\n",
      "Training loss: 10551233868376.746 49 3.091200301660904\n",
      "Training loss: 10486382993954.133 50 3.0902317989609966\n",
      "Training loss: 10544084984791.04 51 3.0892614643173517\n",
      "Training loss: 10563208774068.906 52 3.0882989370609697\n",
      "Training loss: 10523078791877.973 53 3.0873788341594377\n",
      "Training loss: 10603730895721.812 54 3.0864820455986544\n",
      "Training loss: 10537427561786.027 55 3.0855748873642987\n",
      "Training loss: 10551265856935.254 56 3.084728750546707\n",
      "Training loss: 10588398533563.732 57 3.083926049975698\n",
      "Training loss: 10558711138003.627 58 3.083084855085021\n",
      "Training loss: 10519593157481.812 59 3.0821901144377133\n",
      "Training loss: 10558394160469.334 60 3.081256834881119\n",
      "Training loss: 10546397332548.268 61 3.080302250809239\n",
      "Training loss: 10529870208914.773 62 3.0792925438816305\n",
      "Training loss: 10536892480443.732 63 3.0782717305511094\n",
      "Training loss: 10585181782016.0 64 3.077281213635837\n",
      "Training loss: 10531122907709.44 65 3.0763277470474266\n",
      "Training loss: 10532779378169.174 66 3.0754428207975515\n",
      "Training loss: 10492832603176.96 67 3.0745868832330316\n",
      "Training loss: 10575788106929.494 68 3.0737446447543966\n",
      "Training loss: 10549898178286.934 69 3.0728634263325274\n",
      "Training loss: 10548476365154.986 70 3.07196755303007\n",
      "Training loss: 10519133461763.414 71 3.0710892697414733\n",
      "Training loss: 10512136020514.133 72 3.0701963886847152\n",
      "Training loss: 10572652333410.986 73 3.0692955680606757\n",
      "Training loss: 10555028427243.52 74 3.06838875315492\n",
      "Training loss: 10567527229467.307 75 3.0675182112504586\n",
      "Training loss: 10570700583949.654 76 3.0666626328220588\n",
      "Training loss: 10543495992661.334 77 3.065806790384632\n",
      "Training loss: 10592717212658.346 78 3.0649759329741495\n",
      "Training loss: 10516077100400.64 79 3.064144457998537\n",
      "Training loss: 10528722199947.947 80 3.0633119328882095\n",
      "Training loss: 10585527168969.387 81 3.062455542192552\n",
      "Training loss: 10541573994796.373 82 3.061595666163187\n",
      "Training loss: 10507707730274.986 83 3.060725947310604\n",
      "Training loss: 10530011808617.812 84 3.0598526881907953\n",
      "Training loss: 10590675313623.04 85 3.0589810332285\n",
      "Training loss: 10606198488651.094 86 3.0581317636207603\n",
      "Training loss: 10580043479995.732 87 3.0572703474555705\n",
      "Training loss: 10561166651337.387 88 3.0564207782733903\n",
      "Training loss: 10633933911053.654 89 3.0556000282832807\n",
      "Training loss: 10558040273059.84 90 3.054777464289782\n",
      "Training loss: 10568267663933.44 91 3.053952288642409\n",
      "Training loss: 10556909488701.44 92 3.053121215175697\n",
      "Training loss: 10453020045912.746 93 3.052245666634061\n",
      "Training loss: 10573921362029.227 94 3.051351187952741\n",
      "Training loss: 10599439060172.8 95 3.050468500353759\n",
      "Training loss: 10582015809508.693 96 3.04959165469844\n",
      "Training loss: 10566214580087.467 97 3.0487282114970733\n",
      "Training loss: 10535578041494.188 98 3.0478558909122033\n",
      "Training loss: 10548071475008.854 99 3.0469992337460887\n",
      "Training loss: 10535103805521.92 100 3.0461692535819256\n",
      "Training loss: 10473104610426.88 101 3.0453149038961165\n",
      "Training loss: 10553796084804.268 102 3.044462219276097\n",
      "Training loss: 10523067830763.52 103 3.0436459872184614\n",
      "Training loss: 10519204149766.826 104 3.042839991831874\n",
      "Training loss: 10515427486597.12 105 3.0420439297443704\n",
      "Training loss: 10517349931854.506 106 3.0412702896683133\n",
      "Training loss: 10577768265809.92 107 3.0405043751590757\n",
      "Training loss: 10519735651969.707 108 3.039725811163358\n",
      "Training loss: 10555071376916.48 109 3.038934922138075\n",
      "Training loss: 10603810307877.547 110 3.0381549394386065\n",
      "Training loss: 10586698889734.826 111 3.0373763386500343\n",
      "Training loss: 10521380713922.56 112 3.036611267424853\n",
      "Training loss: 10573992273728.854 113 3.0358061761908712\n",
      "Training loss: 10556545982354.773 114 3.0349937499563007\n",
      "Training loss: 10520362896151.893 115 3.0341622023011863\n",
      "Training loss: 10528086679005.867 116 3.033297363673151\n",
      "Training loss: 10519358276457.812 117 3.0324221008696006\n",
      "Training loss: 10523191758465.707 118 3.0315825658631543\n",
      "Training loss: 10546946506752.0 119 3.030779205911319\n",
      "Training loss: 10510341529490.773 120 3.029998056215122\n",
      "Training loss: 10490595641043.627 121 3.0291939964282792\n",
      "Training loss: 10530403053294.934 122 3.02838234030221\n",
      "Training loss: 10627522553883.307 123 3.0276089388995233\n",
      "Training loss: 10517087759892.48 124 3.0268595481170077\n",
      "Training loss: 10527231264686.08 125 3.026084212591141\n",
      "Training loss: 10513354717484.373 126 3.0252944384305738\n",
      "Training loss: 10534563355470.506 127 3.0245008907966833\n",
      "Training loss: 10537552607969.28 128 3.023706214809727\n",
      "Training loss: 10541786282502.826 129 3.0229033304065647\n",
      "Training loss: 10530090773381.12 130 3.022139733857519\n",
      "Training loss: 10548022709234.346 131 3.0214269119539665\n",
      "Training loss: 10481400608194.56 132 3.020693102246397\n",
      "Training loss: 10560967785403.732 133 3.0199635731406396\n",
      "Training loss: 10514571177492.48 134 3.019211509916485\n",
      "Training loss: 10539400114995.2 135 3.0184483503107256\n",
      "Training loss: 10557583709088.426 136 3.0177027731741375\n",
      "Training loss: 10604922078057.812 137 3.01700407324935\n",
      "Training loss: 10467038192817.494 138 3.0163148047037867\n",
      "Training loss: 10569110998657.707 139 3.015626089367455\n",
      "Training loss: 10540748779465.387 140 3.014902535410739\n",
      "Training loss: 10483339159579.307 141 3.0141671020180354\n",
      "Training loss: 10543938911163.732 142 3.0134372441427457\n",
      "Training loss: 10517425764870.826 143 3.012690153437415\n",
      "Training loss: 10526088177035.947 144 3.0119403022024804\n",
      "Training loss: 10520719244219.732 145 3.0112026882579213\n",
      "Training loss: 10512158837527.893 146 3.010473334179643\n",
      "Training loss: 10567049414355.627 147 3.0097401868578726\n",
      "Training loss: 10589190865551.36 148 3.0090415176542558\n",
      "Training loss: 10476137036294.826 149 3.0083380931820383\n",
      "Training loss: 10603016410016.426 150 3.0076296772890405\n",
      "Training loss: 10490960936960.0 151 3.0069058035347798\n",
      "Training loss: 10587979774252.373 152 3.0061865116559585\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(X_train), batch_size):\n\u001b[1;32m      6\u001b[0m     \u001b[39m# get the training data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(X_train[i:i\u001b[39m+\u001b[39;49mbatch_size])\u001b[39m.\u001b[39;49mcuda()\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m     label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(y_train[i:i\u001b[39m+\u001b[39mbatch_size])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      9\u001b[0m     \u001b[39m# zero the parameter gradients\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).cuda().float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).cuda().float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_cuda_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10474209893416.96 0 2.4051506778807608\n",
      "Training loss: 10502994898452.48 1 2.4053545278953536\n",
      "Training loss: 10525501421868.373 2 2.4054608494733123\n",
      "Training loss: 10503175868689.066 3 2.4055277030230084\n",
      "Training loss: 10518290798127.787 4 2.4054672985799095\n",
      "Training loss: 10532129988061.867 5 2.405295121247068\n",
      "Training loss: 10554358680780.8 6 2.4051941556168113\n",
      "Training loss: 10585958231572.48 7 2.4050582729953316\n",
      "Training loss: 10617272345995.947 8 2.4049150096052667\n",
      "Training loss: 10553085178238.293 9 2.4049163651565526\n",
      "Training loss: 10527354744995.84 10 2.4049523798858643\n",
      "Training loss: 10552050806947.84 11 2.405048814551919\n",
      "Training loss: 10498707536827.732 12 2.4051097878191046\n",
      "Training loss: 10541238226780.16 13 2.4052076477480764\n",
      "Training loss: 10521949126000.64 14 2.4052302620163726\n",
      "Training loss: 10512537331520.854 15 2.405202078411037\n",
      "Training loss: 10624627030097.92 16 2.4051851570179217\n",
      "Training loss: 10475470645275.307 17 2.4050452220475402\n",
      "Training loss: 10569476070877.867 18 2.404860138938843\n",
      "Training loss: 10477334482124.8 19 2.4047746628575113\n",
      "Training loss: 10558496389638.826 20 2.4047959058990123\n",
      "Training loss: 10511741644090.027 21 2.4047139986879356\n",
      "Training loss: 10547346475581.44 22 2.4047377745845875\n",
      "Training loss: 10541903499318.613 23 2.4047005515174886\n",
      "Training loss: 10593420513553.066 24 2.4047447202504837\n",
      "Training loss: 10533065038233.6 25 2.4048812634229333\n",
      "Training loss: 10492260164567.04 26 2.4048526118998126\n",
      "Training loss: 10528870957929.812 27 2.4049543158708757\n",
      "Training loss: 10491449936882.346 28 2.405060452196253\n",
      "Training loss: 10536140190078.293 29 2.405021342158303\n",
      "Training loss: 10561965917907.627 30 2.404840205355285\n",
      "Training loss: 10527650023997.44 31 2.4046851022035707\n",
      "Training loss: 10498653849736.533 32 2.4047270555900186\n",
      "Training loss: 10486363084991.146 33 2.4047920571861843\n",
      "Training loss: 10551364059572.906 34 2.404927339014814\n",
      "Training loss: 10527802137422.506 35 2.4051199885337207\n",
      "Training loss: 10487860731139.414 36 2.405164180856528\n",
      "Training loss: 10515393037380.268 37 2.4050919451140826\n",
      "Training loss: 10502948145943.893 38 2.405060058275144\n",
      "Training loss: 10582163001617.066 39 2.405050191516402\n",
      "Training loss: 10468115066388.48 40 2.4051346455677587\n",
      "Training loss: 10530880197317.973 41 2.4050626348395503\n",
      "Training loss: 10549113004578.133 42 2.4048865773350614\n",
      "Training loss: 10502420446576.64 43 2.4048110771000313\n",
      "Training loss: 10518022362671.787 44 2.404713802029415\n",
      "Training loss: 10560059578777.6 45 2.4045645761061163\n",
      "Training loss: 10572925242791.254 46 2.404477714358205\n",
      "Training loss: 10496782407215.787 47 2.40446274011412\n",
      "Training loss: 10541907749546.666 48 2.4045671654675465\n",
      "Training loss: 10532285456930.133 49 2.404804953537268\n",
      "Training loss: 10513436142906.027 50 2.4049407984454145\n",
      "Training loss: 10610673307702.613 51 2.405064289537787\n",
      "Training loss: 10556166593576.96 52 2.405191812757273\n",
      "Training loss: 10508492903983.787 53 2.405186013720251\n",
      "Training loss: 10545154252690.773 54 2.405242067270189\n",
      "Training loss: 10499739447459.84 55 2.405211044940221\n",
      "Training loss: 10565728711912.107 56 2.4050808551038454\n",
      "Training loss: 10546326868241.066 57 2.4048889289160518\n",
      "Training loss: 10580902249758.72 58 2.404768858699017\n",
      "Training loss: 10548262511575.04 59 2.404607742763318\n",
      "Training loss: 10507561880343.893 60 2.4046400726571093\n",
      "Training loss: 10507840382129.494 61 2.4046603043501946\n",
      "Training loss: 10534408333994.666 62 2.4047769280863003\n",
      "Training loss: 10513622705547.947 63 2.404998628696575\n",
      "Training loss: 10524787607251.627 64 2.4051792255346824\n",
      "Training loss: 10518083208041.812 65 2.405284834825895\n",
      "Training loss: 10524068200229.547 66 2.405380302596751\n",
      "Training loss: 10530464569753.6 67 2.4053968809609745\n",
      "Training loss: 10456911241543.68 68 2.4053080884044995\n",
      "Training loss: 10498253433514.666 69 2.4052006606102805\n",
      "Training loss: 10559340171755.52 70 2.405204895898521\n",
      "Training loss: 10522421125010.773 71 2.405089867469899\n",
      "Training loss: 10500271173358.934 72 2.40497869650515\n",
      "Training loss: 10486829267899.732 73 2.4049279791667937\n",
      "Training loss: 10529314323824.64 74 2.404917144049318\n",
      "Training loss: 10528327599827.627 75 2.404972389221369\n",
      "Training loss: 10478060600033.28 76 2.4049980476368185\n",
      "Training loss: 10543278336245.76 77 2.4051281659458845\n",
      "Training loss: 10498015420743.68 78 2.405199067921139\n",
      "Training loss: 10490004188255.574 79 2.4053767302436837\n",
      "Training loss: 10484362346059.094 80 2.405490178956186\n",
      "Training loss: 10584217875032.746 81 2.4054052521766036\n",
      "Training loss: 10574390452988.586 82 2.4052784094790463\n",
      "Training loss: 10564624323706.88 83 2.405191054835032\n",
      "Training loss: 10496529630494.72 84 2.405224283241557\n",
      "Training loss: 10589447445108.053 85 2.4052225647773704\n",
      "Training loss: 10473818425043.627 86 2.4052571112434693\n",
      "Training loss: 10497205416755.2 87 2.4053647562367253\n",
      "Training loss: 10586800000423.254 88 2.4053439207037917\n",
      "Training loss: 10516126760960.0 89 2.4052035628213564\n",
      "Training loss: 10500273186624.854 90 2.4050811671594134\n",
      "Training loss: 10566021530255.36 91 2.405075540410933\n",
      "Training loss: 10509618543329.28 92 2.405198760678391\n",
      "Training loss: 10551133652473.174 93 2.4052399951320527\n",
      "Training loss: 10458079159473.494 94 2.4052395529402353\n",
      "Training loss: 10503176092385.28 95 2.405162424866944\n",
      "Training loss: 10471996195689.812 96 2.4051601852797897\n",
      "Training loss: 10505884606136.32 97 2.4052820614266506\n",
      "Training loss: 10476345968558.08 98 2.4054202105317146\n",
      "Training loss: 10571998693075.627 99 2.405360510247478\n",
      "Training loss: 10553909275088.213 100 2.405387359167046\n",
      "Training loss: 10470813961202.346 101 2.405356989030865\n",
      "Training loss: 10563221077360.64 102 2.4052719746121434\n",
      "Training loss: 10570710202886.826 103 2.4051110770628346\n",
      "Training loss: 10553337507566.934 104 2.405002396613481\n",
      "Training loss: 10546653912104.96 105 2.404938770726977\n",
      "Training loss: 10535761696085.334 106 2.4049821669154734\n",
      "Training loss: 10536274184110.08 107 2.4051397111326627\n",
      "Training loss: 10484319843778.56 108 2.405258128962395\n",
      "Training loss: 10502217554111.146 109 2.405294810524097\n",
      "Training loss: 10517026243433.812 110 2.4053790605292416\n",
      "Training loss: 10532187701684.906 111 2.4054376027772775\n",
      "Training loss: 10453496518847.146 112 2.405457224997245\n",
      "Training loss: 10592228436432.213 113 2.4053804621833073\n",
      "Training loss: 10489578494361.6 114 2.4053548872540005\n",
      "Training loss: 10587464154480.64 115 2.4052319997697693\n",
      "Training loss: 10515678921140.906 116 2.405244543904662\n",
      "Training loss: 10534331158801.066 117 2.4052467602909995\n",
      "Training loss: 10463113442754.56 118 2.4052600781602798\n",
      "Training loss: 10527588283842.56 119 2.4052452464294696\n",
      "Training loss: 10554694672493.227 120 2.4052310047721694\n",
      "Training loss: 10469191716263.254 121 2.405213163744333\n",
      "Training loss: 10514414142750.72 122 2.405227305275578\n",
      "Training loss: 10559858028489.387 123 2.4053066036757746\n",
      "Training loss: 10586577646387.2 124 2.4052356955382135\n",
      "Training loss: 10482460257157.12 125 2.4051201977255965\n",
      "Training loss: 10517397579147.947 126 2.4050193923622802\n",
      "Training loss: 10480673595501.227 127 2.4049682421185232\n",
      "Training loss: 10552400220433.066 128 2.4050085288139043\n",
      "Training loss: 10533110224868.693 129 2.405140870789401\n",
      "Training loss: 10426111181018.453 130 2.4052917660128847\n",
      "Training loss: 10560424427301.547 131 2.4054195843322614\n",
      "Training loss: 10490089416512.854 132 2.405474005341582\n",
      "Training loss: 10566124654209.707 133 2.4054470965245054\n",
      "Training loss: 10556854906825.387 134 2.405455356944321\n",
      "Training loss: 10510049605932.373 135 2.405510700198599\n",
      "Training loss: 10580274781880.32 136 2.4055922702511823\n",
      "Training loss: 10499249105360.213 137 2.405624651040822\n",
      "Training loss: 10482919281786.88 138 2.405604588689862\n",
      "Training loss: 10556391631967.574 139 2.405446921634721\n",
      "Training loss: 10489882497515.52 140 2.4052176444483084\n",
      "Training loss: 10517506742900.053 141 2.4052145012619737\n",
      "Training loss: 10539828493243.732 142 2.4052708954644517\n",
      "Training loss: 10532010310587.732 143 2.40546238173998\n",
      "Training loss: 10555749623835.307 144 2.405525414971807\n",
      "Training loss: 10483255049803.094 145 2.4053721876241263\n",
      "Training loss: 10504124340633.6 146 2.405331630495963\n",
      "Training loss: 10582021849306.453 147 2.4052718772456627\n",
      "Training loss: 10514457539816.107 148 2.4052244637844167\n",
      "Training loss: 10463541821003.094 149 2.4052672032667526\n",
      "Training loss: 10544624540057.6 150 2.405273633499639\n",
      "Training loss: 10531794667438.08 151 2.405298492764452\n",
      "Training loss: 10557730677500.586 152 2.405383821373296\n",
      "Training loss: 10553443315875.84 153 2.405534973254922\n",
      "Training loss: 10460381440901.12 154 2.405536609309682\n",
      "Training loss: 10502412169816.746 155 2.4056690482201475\n",
      "Training loss: 10527512003433.812 156 2.4055918512696235\n",
      "Training loss: 10535495721287.68 157 2.405449356405881\n",
      "Training loss: 10531016428311.893 158 2.4052954325379843\n",
      "Training loss: 10533407964528.64 159 2.4052548101383437\n",
      "Training loss: 10516971437861.547 160 2.405264559231733\n",
      "Training loss: 10550274659013.973 161 2.4053843519695643\n",
      "Training loss: 10576106874033.494 162 2.4055310853245855\n",
      "Training loss: 10524251854820.693 163 2.405652066275553\n",
      "Training loss: 10563351939645.44 164 2.4057086952987152\n",
      "Training loss: 10539434564212.053 165 2.405499395488616\n",
      "Training loss: 10476712606651.732 166 2.40549981238669\n",
      "Training loss: 10587680021326.506 167 2.405557778097842\n",
      "Training loss: 10528399853704.533 168 2.40557500540614\n",
      "Training loss: 10514232948817.92 169 2.405343450916784\n",
      "Training loss: 10523489274429.44 170 2.4051868528732023\n",
      "Training loss: 10541873076633.6 171 2.4051434503277767\n",
      "Training loss: 10515787637500.586 172 2.405248647822092\n",
      "Training loss: 10548521999182.506 173 2.4054451420037495\n",
      "Training loss: 10455926754508.8 174 2.4055354906487456\n",
      "Training loss: 10453792916329.812 175 2.4056411269924887\n",
      "Training loss: 10546667557573.973 176 2.4056207748471716\n",
      "Training loss: 10514802479377.066 177 2.4054860802209093\n",
      "Training loss: 10499523133221.547 178 2.40535456998576\n",
      "Training loss: 10540012818923.52 179 2.405278498704158\n",
      "Training loss: 10488202538953.387 180 2.4053116072814458\n",
      "Training loss: 10512074504055.467 181 2.405389088422109\n",
      "Training loss: 10499602769073.494 182 2.4054226521066258\n",
      "Training loss: 10516589588425.387 183 2.4055205418683268\n",
      "Training loss: 10534691757096.96 184 2.4055696757111096\n",
      "Training loss: 10535457916627.627 185 2.4056595387143878\n",
      "Training loss: 10537055554983.254 186 2.4057488605824036\n",
      "Training loss: 10521826316779.52 187 2.4056824875083893\n",
      "Training loss: 10504413356141.227 188 2.405606645650733\n",
      "Training loss: 10532228414395.732 189 2.4056649577535967\n",
      "Training loss: 10522602542639.787 190 2.405834518072888\n",
      "Training loss: 10507630331385.174 191 2.4059187767206702\n",
      "Training loss: 10508444138209.28 192 2.405873849333561\n",
      "Training loss: 10554864681615.36 193 2.4058308285635914\n",
      "Training loss: 10579508622349.654 194 2.4057823588964644\n",
      "Training loss: 10500654141276.16 195 2.4058206519218004\n",
      "Training loss: 10549751880963.414 196 2.405850473425471\n",
      "Training loss: 10486746276604.586 197 2.405819795075489\n",
      "Training loss: 10529705568501.76 198 2.405773340724269\n",
      "Training loss: 10597903609364.48 199 2.4056995629772264\n",
      "Training loss: 10568507689970.346 200 2.4056783556701475\n",
      "Training loss: 10497161348601.174 201 2.4056797305859985\n",
      "Training loss: 10528252214203.732 202 2.4056938898066687\n",
      "Training loss: 10598646057096.533 203 2.4056669432304782\n",
      "Training loss: 10536299014389.76 204 2.4056007622620914\n",
      "Training loss: 10492046087290.88 205 2.4055382140092414\n",
      "Training loss: 10573112700218.027 206 2.4054669596400777\n",
      "Training loss: 10486596400141.654 207 2.405401586958572\n",
      "Training loss: 10557997099690.666 208 2.405420821622086\n",
      "Training loss: 10495029971080.533 209 2.4055162666394345\n",
      "Training loss: 10496427177629.014 210 2.4056210815415944\n",
      "Training loss: 10528910328463.36 211 2.4056439761132116\n",
      "Training loss: 10496276853773.654 212 2.405671560196437\n",
      "Training loss: 10521423663595.52 213 2.4057735161744147\n",
      "Training loss: 10430321367449.6 214 2.4058940828824293\n",
      "Training loss: 10494446123963.732 215 2.406021540336737\n",
      "Training loss: 10461012711615.146 216 2.4061174673664656\n",
      "Training loss: 10549859478842.027 217 2.406129768845982\n",
      "Training loss: 10554289111258.453 218 2.4061067003296066\n",
      "Training loss: 10592860378234.88 219 2.4059480457628206\n",
      "Training loss: 10530732557817.174 220 2.4058155270610864\n",
      "Training loss: 10512268001280.0 221 2.4057758000772513\n",
      "Training loss: 10606725516929.707 222 2.4057688022179384\n",
      "Training loss: 10536016709768.533 223 2.4057520987561314\n",
      "Training loss: 10475721408730.453 224 2.4058151995206156\n",
      "Training loss: 10567845325482.666 225 2.4057656200119717\n",
      "Training loss: 10459349306572.8 226 2.4057652797676026\n",
      "Training loss: 10548817501880.32 227 2.4057405784638592\n",
      "Training loss: 10510364570200.746 228 2.4056669803781907\n",
      "Training loss: 10517954359022.934 229 2.405664496734358\n",
      "Training loss: 10473474156571.307 230 2.4057286954538597\n",
      "Training loss: 10533612422867.627 231 2.4058711138680438\n",
      "Training loss: 10545426490982.4 232 2.40607141406579\n",
      "Training loss: 10501815795712.0 233 2.4061123628429852\n",
      "Training loss: 10547418729458.346 234 2.4060147928244633\n",
      "Training loss: 10556653803929.6 235 2.4058587985458733\n",
      "Training loss: 10512705774769.494 236 2.4057899127610454\n",
      "Training loss: 10539623363816.107 237 2.4057209385278178\n",
      "Training loss: 10560515024267.947 238 2.4056928153095662\n",
      "Training loss: 10549072515563.52 239 2.405757017319013\n",
      "Training loss: 10512603321903.787 240 2.4058379889906125\n",
      "Training loss: 10551806754379.094 241 2.405825294042186\n",
      "Training loss: 10452873748589.227 242 2.4058475387448524\n",
      "Training loss: 10486218577237.334 243 2.405856666064392\n",
      "Training loss: 10558686531420.16 244 2.4059388281987517\n",
      "Training loss: 10509652992546.133 245 2.4059993391347914\n",
      "Training loss: 10546961270702.08 246 2.4060095046489005\n",
      "Training loss: 10525099216076.8 247 2.4059789168168946\n",
      "Training loss: 10553037083552.426 248 2.405881049047466\n",
      "Training loss: 10540168287791.787 249 2.405824619338963\n",
      "Training loss: 10500024883828.053 250 2.4056593903258934\n",
      "Training loss: 10525274817604.268 251 2.405597293783257\n",
      "Training loss: 10554905170629.973 252 2.405548017271113\n",
      "Training loss: 10449323242291.2 253 2.4056309035424976\n",
      "Training loss: 10531344814353.066 254 2.4056427998473917\n",
      "Training loss: 10528134549995.52 255 2.4056604675072495\n",
      "Training loss: 10543788363612.16 256 2.4056860585714697\n",
      "Training loss: 10513791148796.586 257 2.4057201379113238\n",
      "Training loss: 10572673808247.467 258 2.4057825675411313\n",
      "Training loss: 10520132936444.586 259 2.4057947540226694\n",
      "Training loss: 10526493290878.293 260 2.405891385676364\n",
      "Training loss: 10521725653483.52 261 2.405979274961961\n",
      "Training loss: 10575057291400.533 262 2.405996787667709\n",
      "Training loss: 10518824760989.014 263 2.4059678752058997\n",
      "Training loss: 10497955470158.506 264 2.4059866193961454\n",
      "Training loss: 10490388722046.293 265 2.4059426159289856\n",
      "Training loss: 10544237769304.746 266 2.405844576568054\n",
      "Training loss: 10507678202374.826 267 2.405800490197996\n",
      "Training loss: 10454639382801.066 268 2.405812269135015\n",
      "Training loss: 10508297393493.334 269 2.4058892260800135\n",
      "Training loss: 10512291489382.4 270 2.405817935058338\n",
      "Training loss: 10466820760098.133 271 2.4058549180487936\n",
      "Training loss: 10481157226714.453 272 2.406018215761264\n",
      "Training loss: 10513560294304.426 273 2.406071369373335\n",
      "Training loss: 10525407469458.773 274 2.4060996846918457\n",
      "Training loss: 10531916358178.133 275 2.406111423531732\n",
      "Training loss: 10512073609270.613 276 2.406146737985648\n",
      "Training loss: 10549906231350.613 277 2.4061050193595857\n",
      "Training loss: 10531161830850.56 278 2.405966260193831\n",
      "Training loss: 10495582724423.68 279 2.4059873296634717\n",
      "Training loss: 10513458736223.574 280 2.4060638558348453\n",
      "Training loss: 10480529311443.627 281 2.406085500487633\n",
      "Training loss: 10524796555100.16 282 2.406136101656077\n",
      "Training loss: 10477384590076.586 283 2.406120721936555\n",
      "Training loss: 10492853630621.014 284 2.406059092658741\n",
      "Training loss: 10481191228538.88 285 2.406115074000866\n",
      "Training loss: 10496552447508.48 286 2.406119281112447\n",
      "Training loss: 10482136792432.64 287 2.406206962789217\n",
      "Training loss: 10561299079495.68 288 2.406237412939999\n",
      "Training loss: 10508700717765.973 289 2.406164183029242\n",
      "Training loss: 10549896388717.227 290 2.4060738920286973\n",
      "Training loss: 10524885586193.066 291 2.4060090435707986\n",
      "Training loss: 10514263147806.72 292 2.4061159918324746\n",
      "Training loss: 10481568604050.773 293 2.4062802390304388\n",
      "Training loss: 10428950333358.08 294 2.406338528042641\n",
      "Training loss: 10565965829898.24 295 2.406347915019408\n",
      "Training loss: 10558611145796.268 296 2.406329410059255\n",
      "Training loss: 10539402351957.334 297 2.406303394441619\n",
      "Training loss: 10488789965209.6 298 2.406193810668879\n",
      "Training loss: 10474189089669.12 299 2.406091711666467\n",
      "Training loss: 10532863264249.174 300 2.4059739346876574\n",
      "Training loss: 10407605240681.812 301 2.4059295186090544\n",
      "Training loss: 10476528952060.586 302 2.4059490812539557\n",
      "Training loss: 10575096214541.654 303 2.405865677700195\n",
      "Training loss: 10533502588026.88 304 2.4057726573070277\n",
      "Training loss: 10498405546939.732 305 2.405889338068244\n",
      "Training loss: 10484546000650.24 306 2.4061329783261933\n",
      "Training loss: 10611656452560.213 307 2.4063326849126687\n",
      "Training loss: 10511006802029.227 308 2.406275533746687\n",
      "Training loss: 10541935040484.693 309 2.406090923068544\n",
      "Training loss: 10626082621358.08 310 2.40582432860357\n",
      "Training loss: 10468472756633.6 311 2.4057428195855137\n",
      "Training loss: 10505007716980.053 312 2.4057889817701\n",
      "Training loss: 10572786774835.2 313 2.4057447940276586\n",
      "Training loss: 10495325026385.92 314 2.4057425159309753\n",
      "Training loss: 10507007561127.254 315 2.4057778930904727\n",
      "Training loss: 10544197280290.133 316 2.405909928072445\n",
      "Training loss: 10543573838943.574 317 2.406041378670295\n",
      "Training loss: 10462650615289.174 318 2.40617379964932\n",
      "Training loss: 10515158156356.268 319 2.406319611298788\n",
      "Training loss: 10539945262667.094 320 2.4064935824162186\n",
      "Training loss: 10524871493331.627 321 2.4066055644311044\n",
      "Training loss: 10558414069432.32 322 2.4065626816632633\n",
      "Training loss: 10535448521386.666 323 2.4064900324967433\n",
      "Training loss: 10451382365934.934 324 2.4065018330299264\n",
      "Training loss: 10545193623224.32 325 2.4064763448717\n",
      "Training loss: 10575771329713.494 326 2.4063619320960528\n",
      "Training loss: 10491718819730.773 327 2.4062844889596793\n",
      "Training loss: 10562878822154.24 328 2.4061476625021014\n",
      "Training loss: 10510573949856.426 329 2.4060217585938592\n",
      "Training loss: 10559846619982.506 330 2.4059694899264668\n",
      "Training loss: 10561760788480.0 331 2.405925210298488\n",
      "Training loss: 10508830461569.707 332 2.4060215700931393\n",
      "Training loss: 10521418742278.826 333 2.4061748380437704\n",
      "Training loss: 10527761872104.107 334 2.406339709694708\n",
      "Training loss: 10500486592812.373 335 2.40644712652977\n",
      "Training loss: 10527231935774.72 336 2.4064046353683697\n",
      "Training loss: 10554079731602.773 337 2.4063370856047905\n",
      "Training loss: 10558484533739.52 338 2.4063832622925423\n",
      "Training loss: 10486589912951.467 339 2.406431576258553\n",
      "Training loss: 10533888240298.666 340 2.4064469476844104\n",
      "Training loss: 10560470732417.707 341 2.406500523178078\n",
      "Training loss: 10524567490177.707 342 2.406429903806226\n",
      "Training loss: 10482199427372.373 343 2.4063652803689974\n",
      "Training loss: 10540393773574.826 344 2.4061857802840745\n",
      "Training loss: 10516324732108.8 345 2.4059824117344206\n",
      "Training loss: 10537228695852.373 346 2.4058631106410493\n",
      "Training loss: 10551775213213.014 347 2.405905842909503\n",
      "Training loss: 10562136374422.188 348 2.4059512649766828\n",
      "Training loss: 10550875730739.2 349 2.4059644057149883\n",
      "Training loss: 10474206090581.334 350 2.406115858377688\n",
      "Training loss: 10495990298924.373 351 2.4062578091620233\n",
      "Training loss: 10494463348572.16 352 2.4063438739157994\n",
      "Training loss: 10527444223481.174 353 2.4063638551556426\n",
      "Training loss: 10499306595287.04 354 2.4062468167623403\n",
      "Training loss: 10568966267207.68 355 2.4061664741600923\n",
      "Training loss: 10470920440599.893 356 2.4061813187067576\n",
      "Training loss: 10493494072879.787 357 2.406291582708519\n",
      "Training loss: 10501666366641.494 358 2.4063606381962397\n",
      "Training loss: 10545979468021.76 359 2.406404019386855\n",
      "Training loss: 10561029749254.826 360 2.406454814210394\n",
      "Training loss: 10495284984763.732 361 2.406503953169246\n",
      "Training loss: 10542296980957.867 362 2.4064282453519423\n",
      "Training loss: 10485604978524.16 363 2.4064135994193685\n",
      "Training loss: 10529735320098.133 364 2.4063312291001644\n",
      "Training loss: 10566850772118.188 365 2.4062046943498343\n",
      "Training loss: 10524465261008.213 366 2.406203900597399\n",
      "Training loss: 10556659396334.934 367 2.405797468902622\n",
      "Training loss: 10531410581039.787 368 2.4058350699992928\n",
      "Training loss: 10438607298887.68 369 2.405767468339813\n",
      "Training loss: 10490240858849.28 370 2.405393926367503\n",
      "Training loss: 10515733950409.387 371 2.404852007599332\n",
      "Training loss: 10511589754361.174 372 2.4047588897972387\n",
      "Training loss: 10515143616102.4 373 2.405355613053489\n",
      "Training loss: 10545234112238.934 374 2.406123043434594\n",
      "Training loss: 10500349914426.027 375 2.4069149305279454\n",
      "Training loss: 10504547126476.8 376 2.4076060533215746\n",
      "Training loss: 10534331382497.28 377 2.407772218109144\n",
      "Training loss: 10527797887194.453 378 2.40767660662001\n",
      "Training loss: 10516940791480.32 379 2.407437020596726\n",
      "Training loss: 10469755207024.64 380 2.4074042671477778\n",
      "Training loss: 10489945579847.68 381 2.4074872178826143\n",
      "Training loss: 10503440948701.867 382 2.4073152507605617\n",
      "Training loss: 10444923361471.146 383 2.4070962978885895\n",
      "Training loss: 10541488766539.094 384 2.4070130980758764\n",
      "Training loss: 10493997613056.0 385 2.4070018628225722\n",
      "Training loss: 10522162979580.586 386 2.407032634655462\n",
      "Training loss: 10501584941219.84 387 2.406931453892029\n",
      "Training loss: 10494897766618.453 388 2.406852262366216\n",
      "Training loss: 10591413511127.04 389 2.406705098793639\n",
      "Training loss: 10551640995485.014 390 2.406601107553803\n",
      "Training loss: 10485749709974.188 391 2.4065676346045106\n",
      "Training loss: 10489399761087.146 392 2.406527329977233\n",
      "Training loss: 10485618176600.746 393 2.4064752643905325\n",
      "Training loss: 10527520280193.707 394 2.4064405793760333\n",
      "Training loss: 10505776784561.494 395 2.406502275852204\n",
      "Training loss: 10607159711279.787 396 2.4065833453497936\n",
      "Training loss: 10441403277858.133 397 2.4066757988147507\n",
      "Training loss: 10553953790634.666 398 2.4065998209711053\n",
      "Training loss: 10546423281309.014 399 2.4065419981305367\n",
      "Training loss: 10596886910074.88 400 2.406481324700499\n",
      "Training loss: 10520717230953.812 401 2.4065355808623505\n",
      "Training loss: 10461884903150.934 402 2.406628889650158\n",
      "Training loss: 10553297689640.96 403 2.4066625531036805\n",
      "Training loss: 10554234976774.826 404 2.4064983589580047\n",
      "Training loss: 10503572034682.88 405 2.4063487014756717\n",
      "Training loss: 10453292060508.16 406 2.4064987465298473\n",
      "Training loss: 10551651061814.613 407 2.4066576331144245\n",
      "Training loss: 10517647224122.027 408 2.406650819054363\n",
      "Training loss: 10511986591443.627 409 2.406571070726659\n",
      "Training loss: 10509980707498.666 410 2.406542208785734\n",
      "Training loss: 10556588932027.732 411 2.4064960040116805\n",
      "Training loss: 10562077766014.293 412 2.4065298501092816\n",
      "Training loss: 10485717721415.68 413 2.4064658279231947\n",
      "Training loss: 10551807649163.947 414 2.4064077916986566\n",
      "Training loss: 10466160408876.373 415 2.406552935541271\n",
      "Training loss: 10533033720763.732 416 2.4066819736518874\n",
      "Training loss: 10519382883041.28 417 2.4068093532332084\n",
      "Training loss: 10506398883730.773 418 2.4069337751204674\n",
      "Training loss: 10537754381953.707 419 2.4069078051116395\n",
      "Training loss: 10544679793022.293 420 2.4067672664468724\n",
      "Training loss: 10527694092151.467 421 2.4066404566615693\n",
      "Training loss: 10484938587504.64 422 2.4066952449247307\n",
      "Training loss: 10527551150271.146 423 2.406645302317897\n",
      "Training loss: 10574024933376.0 424 2.406627378631698\n",
      "Training loss: 10505858433679.36 425 2.4065972573225136\n",
      "Training loss: 10494195807901.014 426 2.406619161533848\n",
      "Training loss: 10546618791799.467 427 2.406605736817695\n",
      "Training loss: 10515116772556.8 428 2.406659387823906\n",
      "Training loss: 10517323088308.906 429 2.406805946940165\n",
      "Training loss: 10521011167778.133 430 2.4069716013535807\n",
      "Training loss: 10480604473371.307 431 2.40699567921362\n",
      "Training loss: 10504631907341.654 432 2.4070555262136004\n",
      "Training loss: 10570016297233.066 433 2.4070155862391793\n",
      "Training loss: 10495423900112.213 434 2.406950603893683\n",
      "Training loss: 10478277361664.0 435 2.406865405899427\n",
      "Training loss: 10537771606562.133 436 2.4066827609191006\n",
      "Training loss: 10549909586793.812 437 2.4065907425421806\n",
      "Training loss: 10494982100090.88 438 2.4065417061649605\n",
      "Training loss: 10462051109437.44 439 2.4065926795667867\n",
      "Training loss: 10478061494818.133 440 2.4067056407941503\n",
      "Training loss: 10565902971262.293 441 2.406823933376619\n",
      "Training loss: 10512747382265.174 442 2.406774159040245\n",
      "Training loss: 10516433672164.693 443 2.4067250977084016\n",
      "Training loss: 10524995197337.6 444 2.4066796309528757\n",
      "Training loss: 10510680205557.76 445 2.4066220488930488\n",
      "Training loss: 10513487145642.666 446 2.406649241308364\n",
      "Training loss: 10554121339098.453 447 2.4065614530804447\n",
      "Training loss: 10554103890793.812 448 2.4064775728631744\n",
      "Training loss: 10540003871074.986 449 2.406522216914246\n",
      "Training loss: 10536156967294.293 450 2.4065814373295704\n",
      "Training loss: 10535472904273.92 451 2.4067720047085275\n",
      "Training loss: 10537774290916.693 452 2.4068036106203374\n",
      "Training loss: 10556037297165.654 453 2.4068015348916503\n",
      "Training loss: 10436428945162.24 454 2.406810262759229\n",
      "Training loss: 10475012068037.973 455 2.40688739759441\n",
      "Training loss: 10565714171658.24 456 2.407027718096323\n",
      "Training loss: 10540052636849.494 457 2.4070772861495806\n",
      "Training loss: 10499995803320.32 458 2.4071212903871704\n",
      "Training loss: 10507690505666.56 459 2.407104593247097\n",
      "Training loss: 10512967946731.52 460 2.4070128277805503\n",
      "Training loss: 10507251389999.787 461 2.406906361606433\n",
      "Training loss: 10546412320194.56 462 2.4068692786309507\n",
      "Training loss: 10554265846852.268 463 2.406937257580552\n",
      "Training loss: 10532999495243.094 464 2.4070889727063127\n",
      "Training loss: 10491093588814.506 465 2.407223009659663\n",
      "Training loss: 10500049490411.52 466 2.406981838730777\n",
      "Training loss: 10499762488169.812 467 2.4067609852691723\n",
      "Training loss: 10517012150572.373 468 2.4066640153156236\n",
      "Training loss: 10543600011400.533 469 2.406573931076432\n",
      "Training loss: 10503429987587.414 470 2.4065827590297415\n",
      "Training loss: 10496607700473.174 471 2.406762197419692\n",
      "Training loss: 10480832643508.906 472 2.406993941492175\n",
      "Training loss: 10485447048997.547 473 2.4071631084255003\n",
      "Training loss: 10537948326570.666 474 2.407181448559913\n",
      "Training loss: 10544327918878.72 475 2.4072143924561282\n",
      "Training loss: 10517500032013.654 476 2.407265376693704\n",
      "Training loss: 10559521813080.746 477 2.40727115153088\n",
      "Training loss: 10501882233487.36 478 2.4072262666224313\n",
      "Training loss: 10518077615636.48 479 2.4071102081241014\n",
      "Training loss: 10555884960044.373 480 2.4068988737528922\n",
      "Training loss: 10561346055700.48 481 2.4066778485363534\n",
      "Training loss: 10522787986800.64 482 2.406649131007714\n",
      "Training loss: 10520064932795.732 483 2.4067116505655135\n",
      "Training loss: 10490294769636.693 484 2.406800765312162\n",
      "Training loss: 10446660138871.467 485 2.40690588314921\n",
      "Training loss: 10516211989217.28 486 2.4069771581375012\n",
      "Training loss: 10539214670834.346 487 2.407061177041294\n",
      "Training loss: 10506978704315.732 488 2.407261521746416\n",
      "Training loss: 10517447239707.307 489 2.407470156044835\n",
      "Training loss: 10519432767296.854 490 2.407618263059291\n",
      "Training loss: 10486463748287.146 491 2.4076142317688314\n",
      "Training loss: 10474785016381.44 492 2.4076120626913236\n",
      "Training loss: 10460906679610.027 493 2.407629672729231\n",
      "Training loss: 10521270879081.812 494 2.4075191733677825\n",
      "Training loss: 10564846901439.146 495 2.407334597797233\n",
      "Training loss: 10511570740183.04 496 2.407170445345673\n",
      "Training loss: 10503882972419.414 497 2.407126474462839\n",
      "Training loss: 10486363532383.574 498 2.407208460830132\n",
      "Training loss: 10502011977291.094 499 2.407210042173068\n",
      "Training loss: 10502629826232.32 500 2.4072301420365356\n",
      "Training loss: 10441171081188.693 501 2.407320433345134\n",
      "Training loss: 10519167910980.268 502 2.4072912805468896\n",
      "Training loss: 10523574502686.72 503 2.4072794042175074\n",
      "Training loss: 10503453028297.387 504 2.4073016319093075\n",
      "Training loss: 10541198185157.973 505 2.4072228968180243\n",
      "Training loss: 10594019572012.373 506 2.407097422241575\n",
      "Training loss: 10528038584320.0 507 2.4069901544162655\n",
      "Training loss: 10629549912664.746 508 2.406916492460973\n",
      "Training loss: 10446221023204.693 509 2.407181628948159\n",
      "Training loss: 10514642312888.32 510 2.4074578956980006\n",
      "Training loss: 10533879516146.346 511 2.407485948998761\n",
      "Training loss: 10539600994194.773 512 2.407613610012802\n",
      "Training loss: 10506424832491.52 513 2.407736359257797\n",
      "Training loss: 10541307348910.08 514 2.407749074279502\n",
      "Training loss: 10532710479735.467 515 2.4077770296789005\n",
      "Training loss: 10539928709147.307 516 2.407802887318868\n",
      "Training loss: 10517579891561.812 517 2.4077886521022402\n",
      "Training loss: 10546329999988.053 518 2.407727648500854\n",
      "Training loss: 10512505566658.56 519 2.40752636118341\n",
      "Training loss: 10540714330248.533 520 2.4073334450748876\n",
      "Training loss: 10525398745306.453 521 2.407188215856913\n",
      "Training loss: 10503480095539.2 522 2.4070885481021764\n",
      "Training loss: 10544030402914.986 523 2.4070638010154606\n",
      "Training loss: 10532265547967.146 524 2.4071321103510868\n",
      "Training loss: 10488544794159.787 525 2.4072756823030526\n",
      "Training loss: 10506606250120.533 526 2.4075336697624428\n",
      "Training loss: 10538932813605.547 527 2.407667355253298\n",
      "Training loss: 10591791557727.574 528 2.4077490098479126\n",
      "Training loss: 10544226584494.08 529 2.4078745582423258\n",
      "Training loss: 10534023576507.732 530 2.4078508339054574\n",
      "Training loss: 10546841369531.732 531 2.407629438526451\n",
      "Training loss: 10512020593268.053 532 2.407483057660185\n",
      "Training loss: 10509137596470.613 533 2.4074378017531624\n",
      "Training loss: 10489742239989.76 534 2.4074295145074514\n",
      "Training loss: 10491034309317.973 535 2.4075516753074346\n",
      "Training loss: 10457157754770.773 536 2.4076229530198145\n",
      "Training loss: 10467564773703.68 537 2.407667974764061\n",
      "Training loss: 10473443733886.293 538 2.4076615328674587\n",
      "Training loss: 10542272821766.826 539 2.407550873798754\n",
      "Training loss: 10517581457435.307 540 2.4071486382245406\n",
      "Training loss: 10516750649698.986 541 2.4068795874853843\n",
      "Training loss: 10533985324455.254 542 2.4066056623676757\n",
      "Training loss: 10543928621137.92 543 2.406600416758808\n",
      "Training loss: 10549423942314.666 544 2.406951903972059\n",
      "Training loss: 10438762320363.52 545 2.407344434691156\n",
      "Training loss: 10544881567006.72 546 2.4075985031419576\n",
      "Training loss: 10500369152300.373 547 2.407728799294381\n",
      "Training loss: 10540843850356.053 548 2.4079212899562927\n",
      "Training loss: 10547512681867.947 549 2.4080216320981527\n",
      "Training loss: 10505277718309.547 550 2.4080212879406178\n",
      "Training loss: 10512436444528.64 551 2.408052686849858\n",
      "Training loss: 10524025474252.8 552 2.4079177214467125\n",
      "Training loss: 10510963404963.84 553 2.407671515946797\n",
      "Training loss: 10482041945238.188 554 2.407448350332491\n",
      "Training loss: 10503986767462.4 555 2.4073517440223275\n",
      "Training loss: 10562712615867.732 556 2.4072926626133917\n",
      "Training loss: 10472195285319.68 557 2.4073681404208784\n",
      "Training loss: 10520709625282.56 558 2.4073711580739277\n",
      "Training loss: 10472665271063.893 559 2.4073462420691203\n",
      "Training loss: 10508989956969.812 560 2.4072997007933945\n",
      "Training loss: 10617131641077.76 561 2.4074275842971797\n",
      "Training loss: 10511545238814.72 562 2.407556503285625\n",
      "Training loss: 10517679212680.533 563 2.4077266417643437\n",
      "Training loss: 10544669279300.268 564 2.4078711769759247\n",
      "Training loss: 10539493396316.16 565 2.40795024767464\n",
      "Training loss: 10554228265888.426 566 2.4079593281107057\n",
      "Training loss: 10504749124157.44 567 2.4080111880915727\n",
      "Training loss: 10508181742551.04 568 2.4080312016413927\n",
      "Training loss: 10567712002539.52 569 2.407914799312694\n",
      "Training loss: 10529925238183.254 570 2.40789632883123\n",
      "Training loss: 10531043495553.707 571 2.4079417022287015\n",
      "Training loss: 10510600569705.812 572 2.4079086090112094\n",
      "Training loss: 10574944101116.586 573 2.407808977519489\n",
      "Training loss: 10554901591490.56 574 2.40763723731766\n",
      "Training loss: 10532650081757.867 575 2.407599675569408\n",
      "Training loss: 10566558401167.36 576 2.407655553936049\n",
      "Training loss: 10549782527344.64 577 2.407772006208635\n",
      "Training loss: 10625752445747.2 578 2.4077919477264738\n",
      "Training loss: 10496971430516.053 579 2.407858978249551\n",
      "Training loss: 10476053150214.826 580 2.407958121040361\n",
      "Training loss: 10554577455677.44 581 2.407995926326518\n",
      "Training loss: 10486374940890.453 582 2.4080590481926945\n",
      "Training loss: 10572284353140.053 583 2.407948164332946\n",
      "Training loss: 10583708071362.56 584 2.407840234517153\n",
      "Training loss: 10519621119508.48 585 2.4077808593180783\n",
      "Training loss: 10555334667359.574 586 2.4077988432036626\n",
      "Training loss: 10491657303272.107 587 2.407787336830862\n",
      "Training loss: 10544572195143.68 588 2.407826303164333\n",
      "Training loss: 10553971238939.307 589 2.407794730434937\n",
      "Training loss: 10580146827646.293 590 2.407788933559245\n",
      "Training loss: 10493290733021.867 591 2.4077780064023377\n",
      "Training loss: 10555422803667.627 592 2.407769775320394\n",
      "Training loss: 10560178808859.307 593 2.407826285649103\n",
      "Training loss: 10549729287645.867 594 2.407783903052364\n",
      "Training loss: 10556086734028.8 595 2.4076959284289123\n",
      "Training loss: 10545224940694.188 596 2.407695259289982\n",
      "Training loss: 10498518737223.68 597 2.407771210436607\n",
      "Training loss: 10494722612483.414 598 2.4077921082379805\n",
      "Training loss: 10545196754971.307 599 2.4078358850888963\n",
      "Training loss: 10478097957300.906 600 2.407789560805286\n",
      "Training loss: 10530598116392.96 601 2.4077851328342685\n",
      "Training loss: 10545439017970.346 602 2.4077468998794025\n",
      "Training loss: 10499728038952.96 603 2.4077243114230353\n",
      "Training loss: 10484494774217.387 604 2.407612778249909\n",
      "Training loss: 10473111321313.28 605 2.4075345252865605\n",
      "Training loss: 10474232710430.72 606 2.407588403339726\n",
      "Training loss: 10570028153132.373 607 2.4076973406524402\n",
      "Training loss: 10496345081118.72 608 2.4077139506516203\n",
      "Training loss: 10452543572978.346 609 2.4078042805467303\n",
      "Training loss: 10469707112338.773 610 2.4079433234449596\n",
      "Training loss: 10525983263511.893 611 2.4080682788012755\n",
      "Training loss: 10528308138257.066 612 2.408201281771084\n",
      "Training loss: 10525691116257.28 613 2.408304258748507\n",
      "Training loss: 10505791101119.146 614 2.4083119895267657\n",
      "Training loss: 10517078812043.947 615 2.408124776868911\n",
      "Training loss: 10493793825805.654 616 2.407910208486271\n",
      "Training loss: 10483591041515.52 617 2.4079149007608405\n",
      "Training loss: 10531259362399.574 618 2.408092578508547\n",
      "Training loss: 10518755638859.094 619 2.4082704256120895\n",
      "Training loss: 10555897487032.32 620 2.408273632328729\n",
      "Training loss: 10482401872445.44 621 2.4081810126721046\n",
      "Training loss: 10507899661626.027 622 2.4081119705373277\n",
      "Training loss: 10544066418005.334 623 2.408102983371087\n",
      "Training loss: 10537888375985.494 624 2.407962396343804\n",
      "Training loss: 10509862819594.24 625 2.407889412577411\n",
      "Training loss: 10428396461533.867 626 2.4080291786398824\n",
      "Training loss: 10530433475979.947 627 2.408188410493403\n",
      "Training loss: 10493696517952.854 628 2.408188137692218\n",
      "Training loss: 10479631618539.52 629 2.4081782325927947\n",
      "Training loss: 10535754314110.293 630 2.4081533413898764\n",
      "Training loss: 10525613717367.467 631 2.407888974952259\n",
      "Training loss: 10527534373055.146 632 2.4078087772259535\n",
      "Training loss: 10528869392056.32 633 2.407681698241052\n",
      "Training loss: 10492890764192.426 634 2.407384303810967\n",
      "Training loss: 10551303437899.094 635 2.407143474335962\n",
      "Training loss: 10578979133412.693 636 2.407321834785606\n",
      "Training loss: 10411996173653.334 637 2.407635336026793\n",
      "Training loss: 10506787444053.334 638 2.4079475442145504\n",
      "Training loss: 10562692706904.746 639 2.4082556561166375\n",
      "Training loss: 10613295922107.732 640 2.4085000786698183\n",
      "Training loss: 10481445794829.654 641 2.408697590454653\n",
      "Training loss: 10557315049936.213 642 2.408826505637477\n",
      "Training loss: 10550926286083.414 643 2.408688764672801\n",
      "Training loss: 10503130234661.547 644 2.408496080921437\n",
      "Training loss: 10491326232876.373 645 2.408383184590916\n",
      "Training loss: 10511958853113.174 646 2.4083714934060754\n",
      "Training loss: 10529730398781.44 647 2.4082484127339994\n",
      "Training loss: 10478588746792.96 648 2.4082393180352093\n",
      "Training loss: 10504495005259.094 649 2.408298435925892\n",
      "Training loss: 10548763143700.48 650 2.4082818152574235\n",
      "Training loss: 10572266681139.2 651 2.408285633661751\n",
      "Training loss: 10515427262900.906 652 2.408343656753569\n",
      "Training loss: 10510897861973.334 653 2.408369029838847\n",
      "Training loss: 10603500264925.867 654 2.4083970560670225\n",
      "Training loss: 10503257741503.146 655 2.4084343300063504\n",
      "Training loss: 10493735441093.973 656 2.4084849188535893\n",
      "Training loss: 10458031959572.48 657 2.4087070676808042\n",
      "Training loss: 10459554883392.854 658 2.408917667284407\n",
      "Training loss: 10479780823913.812 659 2.409036986637948\n",
      "Training loss: 10533071301727.574 660 2.408997559875071\n",
      "Training loss: 10511946326125.227 661 2.4089186753192426\n",
      "Training loss: 10555973543744.854 662 2.4088159992093052\n",
      "Training loss: 10498749368019.627 663 2.408712980868879\n",
      "Training loss: 10439331627226.453 664 2.4085032228542773\n",
      "Training loss: 10486257052986.027 665 2.408393349995925\n",
      "Training loss: 10463278530560.0 666 2.40835990303699\n",
      "Training loss: 10483274511373.654 667 2.408272883538233\n",
      "Training loss: 10555945805414.4 668 2.4082569185287275\n",
      "Training loss: 10557142132763.307 669 2.4083218028176083\n",
      "Training loss: 10525300318972.586 670 2.4083517401590355\n",
      "Training loss: 10628930945242.453 671 2.408260213840366\n",
      "Training loss: 10528724884302.506 672 2.408198317804986\n",
      "Training loss: 10463586560245.76 673 2.408250131904796\n",
      "Training loss: 10458019656280.746 674 2.4083235848741893\n",
      "Training loss: 10544722742695.254 675 2.4083667807587172\n",
      "Training loss: 10490606602158.08 676 2.4083216198319244\n",
      "Training loss: 10520370278126.934 677 2.408408284207982\n",
      "Training loss: 10513331900470.613 678 2.4085393680932516\n",
      "Training loss: 10530556732593.494 679 2.4086882476405744\n",
      "Training loss: 10516842365146.453 680 2.4087202767824616\n",
      "Training loss: 10548189586609.494 681 2.4086597914031835\n",
      "Training loss: 10553113811353.6 682 2.40861317577384\n",
      "Training loss: 10570840841475.414 683 2.4084132345811895\n",
      "Training loss: 10498195048802.986 684 2.408293520172085\n",
      "Training loss: 10494353066338.986 685 2.4082717281416235\n",
      "Training loss: 10471045934175.574 686 2.4083006492387087\n",
      "Training loss: 10508227152882.346 687 2.4084222439895577\n",
      "Training loss: 10544299062067.2 688 2.4084894581787544\n",
      "Training loss: 10566298242471.254 689 2.408474394209139\n",
      "Training loss: 10499970749344.426 690 2.408446759526181\n",
      "Training loss: 10494911412087.467 691 2.40838384146469\n",
      "Training loss: 10484717351949.654 692 2.4083874125844336\n",
      "Training loss: 10495913794819.414 693 2.4084175191382795\n",
      "Training loss: 10545261403176.96 694 2.4085214890091344\n",
      "Training loss: 10501784701938.346 695 2.4086406027839606\n",
      "Training loss: 10569073641390.08 696 2.408802924497251\n",
      "Training loss: 10519108407787.52 697 2.408942856019718\n",
      "Training loss: 10501417840148.48 698 2.4090382168346\n",
      "Training loss: 10555218792721.066 699 2.4089804533393604\n",
      "Training loss: 10503203159627.094 700 2.408919447714163\n",
      "Training loss: 10547140451368.96 701 2.4088920505393885\n",
      "Training loss: 10486905772004.693 702 2.4089585278962913\n",
      "Training loss: 10509091962443.094 703 2.409073500587738\n",
      "Training loss: 10556343984674.133 704 2.409196614121389\n",
      "Training loss: 10574716154675.2 705 2.409162449697901\n",
      "Training loss: 10500682998087.68 706 2.408955729911548\n",
      "Training loss: 10530308429796.693 707 2.4088126815799376\n",
      "Training loss: 10572134924069.547 708 2.4086688471980424\n",
      "Training loss: 10468147054946.986 709 2.408578366704796\n",
      "Training loss: 10568670540813.654 710 2.408541289349265\n",
      "Training loss: 10486703774324.053 711 2.4084922996917077\n",
      "Training loss: 10552298214959.787 712 2.408466676534645\n",
      "Training loss: 10520041444693.334 713 2.4084540732095836\n",
      "Training loss: 10480640041069.227 714 2.408550575106411\n",
      "Training loss: 10466290823768.746 715 2.408619680330986\n",
      "Training loss: 10548552198171.307 716 2.408651779665275\n",
      "Training loss: 10490959147390.293 717 2.4088472423021217\n",
      "Training loss: 10495244048356.693 718 2.409003879504057\n",
      "Training loss: 10493819774566.4 719 2.4090661765673604\n",
      "Training loss: 10549140742908.586 720 2.4091751638371797\n",
      "Training loss: 10563954129851.732 721 2.4091694493667672\n",
      "Training loss: 10522296526219.947 722 2.409073996711994\n",
      "Training loss: 10539149127843.84 723 2.4089834988214416\n",
      "Training loss: 10547264602767.36 724 2.4088520264059285\n",
      "Training loss: 10541444250992.64 725 2.4086747267864212\n",
      "Training loss: 10506204268025.174 726 2.4085888130462823\n",
      "Training loss: 10490931409059.84 727 2.4085871778171013\n",
      "Training loss: 10494527996777.812 728 2.4086529961262273\n",
      "Training loss: 10502706330337.28 729 2.4087603883899518\n",
      "Training loss: 10500209880596.48 730 2.4088350324126218\n",
      "Training loss: 10513513541795.84 731 2.408984983252573\n",
      "Training loss: 10561777565696.0 732 2.409116288396753\n",
      "Training loss: 10508202546298.88 733 2.409297963202987\n",
      "Training loss: 10535715838361.6 734 2.4094336059455177\n",
      "Training loss: 10473789120839.68 735 2.4094535687278533\n",
      "Training loss: 10483945600013.654 736 2.4094995777561774\n",
      "Training loss: 10527811980055.893 737 2.4094347044283135\n",
      "Training loss: 10527812203752.107 738 2.4093401496817943\n",
      "Training loss: 10528781703140.693 739 2.409304899953479\n",
      "Training loss: 10487867665722.027 740 2.4092552393564985\n",
      "Training loss: 10514462237436.586 741 2.409259876374264\n",
      "Training loss: 10579515780628.48 742 2.4092950042343744\n",
      "Training loss: 10516157854733.654 743 2.409202822267314\n",
      "Training loss: 10486586110115.84 744 2.4090443762337292\n",
      "Training loss: 10499540581526.188 745 2.4090707738857793\n",
      "Training loss: 10502256924644.693 746 2.409073714097323\n",
      "Training loss: 10500685906138.453 747 2.409074437944227\n",
      "Training loss: 10462076387109.547 748 2.4091701690913463\n",
      "Training loss: 10470000825466.88 749 2.409238315301026\n",
      "Training loss: 10523580318788.268 750 2.4093944314124855\n",
      "Training loss: 10564939287975.254 751 2.4094596156973904\n",
      "Training loss: 10511795107485.014 752 2.409356495917066\n",
      "Training loss: 10532900397820.586 753 2.4091478582128962\n",
      "Training loss: 10506863500765.867 754 2.408989053418275\n",
      "Training loss: 10519106841914.027 755 2.408970198904191\n",
      "Training loss: 10513462539059.2 756 2.4090506945018357\n",
      "Training loss: 10515731042358.613 757 2.4091473502053384\n",
      "Training loss: 10512744026821.973 758 2.4091945767996172\n",
      "Training loss: 10538574004879.36 759 2.4092053797380335\n",
      "Training loss: 10545512390328.32 760 2.4091466960173458\n",
      "Training loss: 10499260066474.666 761 2.409125828432197\n",
      "Training loss: 10506135816983.893 762 2.4092000293284994\n",
      "Training loss: 10524560331898.88 763 2.4092893267004727\n",
      "Training loss: 10508587974874.453 764 2.409312877376976\n",
      "Training loss: 10510172638849.707 765 2.409214524277759\n",
      "Training loss: 10534442559515.307 766 2.4091284229283882\n",
      "Training loss: 10506309181549.227 767 2.409107767872003\n",
      "Training loss: 10498047185605.973 768 2.4090956440893483\n",
      "Training loss: 10479188923733.334 769 2.4090292569612917\n",
      "Training loss: 10532084130338.133 770 2.4091143890957736\n",
      "Training loss: 10530044244568.746 771 2.409221945865846\n",
      "Training loss: 10517161579642.88 772 2.409205758081519\n",
      "Training loss: 10444885780507.307 773 2.4091716083504444\n",
      "Training loss: 10495886280185.174 774 2.4093122532452136\n",
      "Training loss: 10514526661946.027 775 2.409490682987895\n",
      "Training loss: 10484483142014.293 776 2.409664108951918\n",
      "Training loss: 10475749370757.12 777 2.4097672363075406\n",
      "Training loss: 10483227311472.64 778 2.4098141353121756\n",
      "Training loss: 10525182878460.586 779 2.4097334978841474\n",
      "Training loss: 10565657129123.84 780 2.4096222504442055\n",
      "Training loss: 10577612573245.44 781 2.409524613503235\n",
      "Training loss: 10504708411446.613 782 2.4095343023734372\n",
      "Training loss: 10549418573605.547 783 2.4095305708486623\n",
      "Training loss: 10517264256204.8 784 2.40949460376563\n",
      "Training loss: 10467871461212.16 785 2.4094848113724656\n",
      "Training loss: 10485164744376.32 786 2.4094204325365283\n",
      "Training loss: 10562502788819.627 787 2.409397907553916\n",
      "Training loss: 10572187268983.467 788 2.4094126029866727\n",
      "Training loss: 10580779664233.812 789 2.4094416149607047\n",
      "Training loss: 10522701863758.506 790 2.409512414423757\n",
      "Training loss: 10532204478900.906 791 2.4094467384771634\n",
      "Training loss: 10543446332101.973 792 2.4093984676760436\n",
      "Training loss: 10528700501415.254 793 2.4094270716184685\n",
      "Training loss: 10509689678725.12 794 2.4095274943379086\n",
      "Training loss: 10562735656577.707 795 2.409625175630818\n",
      "Training loss: 10483330435426.986 796 2.4096553413619124\n",
      "Training loss: 10537909403429.547 797 2.409667793105603\n",
      "Training loss: 10487387837344.426 798 2.409673600691763\n",
      "Training loss: 10483271155930.453 799 2.409704343200126\n",
      "Training loss: 10531546364641.28 800 2.40972789868853\n",
      "Training loss: 10513078900053.334 801 2.409840328424037\n",
      "Training loss: 10561272459646.293 802 2.4098948210241415\n",
      "Training loss: 10497129136346.453 803 2.4098972206928937\n",
      "Training loss: 10536951983636.48 804 2.4098998728303345\n",
      "Training loss: 10506498652241.92 805 2.4098059901793043\n",
      "Training loss: 10555366432221.867 806 2.4097670774636533\n",
      "Training loss: 10532444057545.387 807 2.4096944380979792\n",
      "Training loss: 10493311536769.707 808 2.409752824484997\n",
      "Training loss: 10494371633124.693 809 2.4098883709578627\n",
      "Training loss: 10504733912814.934 810 2.4098869767701254\n",
      "Training loss: 10559071288907.094 811 2.4097177349701857\n",
      "Training loss: 10516007754574.506 812 2.409663516735823\n",
      "Training loss: 10506793036458.666 813 2.409651249560581\n",
      "Training loss: 10519669437890.56 814 2.409685036244582\n",
      "Training loss: 10494210571851.094 815 2.409850869002927\n",
      "Training loss: 10463625483386.88 816 2.410031565115747\n",
      "Training loss: 10488980778079.574 817 2.4101210074184336\n",
      "Training loss: 10504798561020.586 818 2.410193239148014\n",
      "Training loss: 10515109614277.973 819 2.4101898311750625\n",
      "Training loss: 10518802615063.893 820 2.410255950390384\n",
      "Training loss: 10533609067424.426 821 2.4102660721418405\n",
      "Training loss: 10475987607224.32 822 2.4103586718047167\n",
      "Training loss: 10515867720744.96 823 2.4102407272805846\n",
      "Training loss: 10580970700800.0 824 2.4099385560121735\n",
      "Training loss: 10548078856983.893 825 2.4096865969583905\n",
      "Training loss: 10506614079488.0 826 2.409575330899911\n",
      "Training loss: 10559878161148.586 827 2.4094619632309993\n",
      "Training loss: 10552134245635.414 828 2.4093495522453905\n",
      "Training loss: 10455929438863.36 829 2.4093446432186725\n",
      "Training loss: 10559979942925.654 830 2.409376679335933\n",
      "Training loss: 10497644532421.973 831 2.4095086060407693\n",
      "Training loss: 10581471332925.44 832 2.4096614050483693\n",
      "Training loss: 10518070009965.227 833 2.4097764357441283\n",
      "Training loss: 10564580702945.28 834 2.4098291198946784\n",
      "Training loss: 10514416156016.64 835 2.409795688791602\n",
      "Training loss: 10496602108067.84 836 2.4097761962262783\n",
      "Training loss: 10582969426466.133 837 2.4098693408363276\n",
      "Training loss: 10512290818293.76 838 2.4100106974056064\n",
      "Training loss: 10478334404198.4 839 2.4100302466040167\n",
      "Training loss: 10521923400936.107 840 2.4098796032302854\n",
      "Training loss: 10580160473115.307 841 2.4096576186095366\n",
      "Training loss: 10506776930331.307 842 2.409581393006867\n",
      "Training loss: 10562427850588.16 843 2.40949603424453\n",
      "Training loss: 10538166877771.094 844 2.409412541010598\n",
      "Training loss: 10536050264200.533 845 2.4095089145872532\n",
      "Training loss: 10539821782357.334 846 2.4097265558382306\n",
      "Training loss: 10450963606623.574 847 2.409922278364455\n",
      "Training loss: 10479285784193.707 848 2.410157352154394\n",
      "Training loss: 10477995280738.986 849 2.4103927398730525\n",
      "Training loss: 10562460286539.094 850 2.4104253892759306\n",
      "Training loss: 10575828372247.893 851 2.4103191173359875\n",
      "Training loss: 10527909064212.48 852 2.4101761506110795\n",
      "Training loss: 10535108950534.826 853 2.410121858309006\n",
      "Training loss: 10524985578400.426 854 2.410040793278829\n",
      "Training loss: 10540062479482.88 855 2.409995021866038\n",
      "Training loss: 10520445887447.04 856 2.4099774702397365\n",
      "Training loss: 10524826306696.533 857 2.409776712328945\n",
      "Training loss: 10485691101566.293 858 2.4096921279314873\n",
      "Training loss: 10550913088006.826 859 2.40968447557433\n",
      "Training loss: 10468208571405.654 860 2.4098012145966994\n",
      "Training loss: 10508698480803.84 861 2.4099614281168122\n",
      "Training loss: 10487259883110.4 862 2.4099689075325523\n",
      "Training loss: 10528338337245.867 863 2.4100627093372493\n",
      "Training loss: 10516306165323.094 864 2.4102149618046833\n",
      "Training loss: 10536600780581.547 865 2.4102856696394164\n",
      "Training loss: 10503356615229.44 866 2.410361236347411\n",
      "Training loss: 10535708903778.986 867 2.4105329244300577\n",
      "Training loss: 10521525221676.373 868 2.4106562691024176\n",
      "Training loss: 10566606048460.8 869 2.4106604166267833\n",
      "Training loss: 10454142553511.254 870 2.4106756199247346\n",
      "Training loss: 10570942399556.268 871 2.410637151917691\n",
      "Training loss: 10473376177629.867 872 2.4105828676380967\n",
      "Training loss: 10481468388147.2 873 2.4105220714455062\n",
      "Training loss: 10521632372162.56 874 2.410284162934156\n",
      "Training loss: 10475031977000.96 875 2.4101896843345654\n",
      "Training loss: 10518262612404.906 876 2.410159494170184\n",
      "Training loss: 10519807234757.973 877 2.4102085102410076\n",
      "Training loss: 10625385136564.906 878 2.410273164335226\n",
      "Training loss: 10533493863874.56 879 2.4103622771718545\n",
      "Training loss: 10482000114046.293 880 2.410533289980915\n",
      "Training loss: 10503335140392.96 881 2.410468594808216\n",
      "Training loss: 10555481412075.52 882 2.4103015798242127\n",
      "Training loss: 10551249750807.893 883 2.4103248282893275\n",
      "Training loss: 10521912887214.08 884 2.4105413089776495\n",
      "Training loss: 10567301743684.268 885 2.4107206363752938\n",
      "Training loss: 10507748442985.812 886 2.4108767648890606\n",
      "Training loss: 10521885148883.627 887 2.410832134601098\n",
      "Training loss: 10569127328481.28 888 2.410749404918409\n",
      "Training loss: 10454093116648.107 889 2.4107107213712227\n",
      "Training loss: 10520546774439.254 890 2.41067998228759\n",
      "Training loss: 10502091613143.04 891 2.4105273156993126\n",
      "Training loss: 10494734692078.934 892 2.4104599256857084\n",
      "Training loss: 10555655224033.28 893 2.410376961169204\n",
      "Training loss: 10537147941519.36 894 2.4103291956865966\n",
      "Training loss: 10534857963383.467 895 2.4103135055523164\n",
      "Training loss: 10502855983104.0 896 2.4102122935285273\n",
      "Training loss: 10451823718563.84 897 2.4102407904716636\n",
      "Training loss: 10511250407205.547 898 2.4104143164473264\n",
      "Training loss: 10518980453553.494 899 2.410582779186368\n",
      "Training loss: 10520692176977.92 900 2.4106100938162562\n",
      "Training loss: 10573416703371.947 901 2.4106281949686568\n",
      "Training loss: 10492723886817.28 902 2.4106010672120775\n",
      "Training loss: 10543657725023.574 903 2.410671853180763\n",
      "Training loss: 10546218599273.812 904 2.4108514435744457\n",
      "Training loss: 10515113640809.812 905 2.410934873807405\n",
      "Training loss: 10477388840304.64 906 2.4109399962515337\n",
      "Training loss: 10505538548094.293 907 2.41093283968954\n",
      "Training loss: 10534219758086.826 908 2.410827033706959\n",
      "Training loss: 10493444188624.213 909 2.410780651640625\n",
      "Training loss: 10531053114490.88 910 2.4106940157017616\n",
      "Training loss: 10507441084388.693 911 2.4106982779779553\n",
      "Training loss: 10502272583379.627 912 2.410778015202283\n",
      "Training loss: 10496413979552.426 913 2.4108237019922503\n",
      "Training loss: 10463428407022.934 914 2.4108462621677806\n",
      "Training loss: 10557546575517.014 915 2.4108770426435533\n",
      "Training loss: 10555178974795.094 916 2.4108304785371777\n",
      "Training loss: 10469453217136.64 917 2.4106852567324704\n",
      "Training loss: 10523796185634.133 918 2.410546779614095\n",
      "Training loss: 10516502794294.613 919 2.410560555874226\n",
      "Training loss: 10478092141199.36 920 2.4105993805056656\n",
      "Training loss: 10544940846503.254 921 2.410694090228233\n",
      "Training loss: 10485461141858.986 922 2.4106890892808517\n",
      "Training loss: 10487030147099.307 923 2.410680019065862\n",
      "Training loss: 10492764375831.893 924 2.4107685790590763\n",
      "Training loss: 10492958544145.066 925 2.4108210804911834\n",
      "Training loss: 10552052149125.12 926 2.410835930175598\n",
      "Training loss: 10548458916850.346 927 2.410947232002374\n",
      "Training loss: 10490793388496.213 928 2.4110307265563873\n",
      "Training loss: 10440450555685.547 929 2.4111199755983823\n",
      "Training loss: 10502886853181.44 930 2.411239840585271\n",
      "Training loss: 10560025353256.96 931 2.4112741350646743\n",
      "Training loss: 10582188279289.174 932 2.4111792006237702\n",
      "Training loss: 10408845412488.533 933 2.4110764950477512\n",
      "Training loss: 10580740517396.48 934 2.4109368946746037\n",
      "Training loss: 10489774675940.693 935 2.4108669487303795\n",
      "Training loss: 10533943045870.934 936 2.4108669600256265\n",
      "Training loss: 10561098200296.107 937 2.4109408074283096\n",
      "Training loss: 10466113880064.0 938 2.411028646738388\n",
      "Training loss: 10517592865942.188 939 2.4109233753893218\n",
      "Training loss: 10511371650553.174 940 2.410872398654231\n",
      "Training loss: 10520858383264.426 941 2.410807434592762\n",
      "Training loss: 10607071798667.947 942 2.4107140077114124\n",
      "Training loss: 10489476936280.746 943 2.41075997151082\n",
      "Training loss: 10444397899066.027 944 2.410828845328116\n",
      "Training loss: 10554974069063.68 945 2.4108198674588937\n",
      "Training loss: 10489200000368.64 946 2.4108522157254693\n",
      "Training loss: 10516800757650.773 947 2.4108921574623037\n",
      "Training loss: 10537553502754.133 948 2.410911862582595\n",
      "Training loss: 10476410616763.732 949 2.410937745312567\n",
      "Training loss: 10476625588824.746 950 2.4109339393156075\n",
      "Training loss: 10543796192979.627 951 2.410937078648422\n",
      "Training loss: 10534133187652.268 952 2.411123050764432\n",
      "Training loss: 10530825168049.494 953 2.411309960913977\n",
      "Training loss: 10593730780200.96 954 2.4112610195666395\n",
      "Training loss: 10490018952205.654 955 2.411224851873238\n",
      "Training loss: 10504305534566.4 956 2.411250665589121\n",
      "Training loss: 10500986330152.96 957 2.4112684037249474\n",
      "Training loss: 10538821189195.094 958 2.411237203422928\n",
      "Training loss: 10459399861917.014 959 2.4112736696027\n",
      "Training loss: 10572854107395.414 960 2.4113221372415907\n",
      "Training loss: 10493295654338.56 961 2.411354496937463\n",
      "Training loss: 10538424575808.854 962 2.4113867409543532\n",
      "Training loss: 10523290408495.787 963 2.411386394433296\n",
      "Training loss: 10459805199455.574 964 2.411488290461415\n",
      "Training loss: 10514328690797.227 965 2.411377616921052\n",
      "Training loss: 10514787939123.2 966 2.411302869752183\n",
      "Training loss: 10523461088706.56 967 2.411199599549787\n",
      "Training loss: 10547143135723.52 968 2.411123952640075\n",
      "Training loss: 10504975281029.12 969 2.411051822985117\n",
      "Training loss: 10485089358752.426 970 2.4111779476468973\n",
      "Training loss: 10436472789620.053 971 2.4114235899273253\n",
      "Training loss: 10594775888909.654 972 2.4116773680737724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(X_train), batch_size):\n\u001b[1;32m      6\u001b[0m     \u001b[39m# get the training data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(X_train[i:i\u001b[39m+\u001b[39;49mbatch_size])\u001b[39m.\u001b[39;49mcuda()\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m     label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(y_train[i:i\u001b[39m+\u001b[39mbatch_size])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      9\u001b[0m     \u001b[39m# zero the parameter gradients\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).cuda().float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).cuda().float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_cuda_mode(model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be hash collision when using FeatureHasher, some information is lost and may affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1209718335515.3066 0 2.7641163156271342\n",
      "Training loss: 1208368468677.9734 1 2.7641753325579703\n",
      "Training loss: 1206982279168.0 2 2.764016375929691\n",
      "Training loss: 1205637529381.5466 3 2.764121026263768\n",
      "Training loss: 1204250724706.9866 4 2.763975926006395\n",
      "Training loss: 1202902339857.0667 5 2.76418264201457\n",
      "Training loss: 1201516961245.8667 6 2.763947618178719\n",
      "Training loss: 1200145172179.6267 7 2.764365238948186\n",
      "Training loss: 1198716787971.4133 8 2.7640209004675027\n",
      "Training loss: 1197241791064.7466 9 2.7641675935415795\n",
      "Training loss: 1195720433117.8667 10 2.764161095294574\n",
      "Training loss: 1194124276749.6533 11 2.7635162498190837\n",
      "Training loss: 1192529350710.6133 12 2.7635814133478656\n",
      "Training loss: 1190931209038.5066 13 2.7633591880700314\n",
      "Training loss: 1189286426705.92 14 2.76353917319399\n",
      "Training loss: 1187626013600.4268 15 2.763390910472604\n",
      "Training loss: 1185944181582.5066 16 2.7632072533217253\n",
      "Training loss: 1184270989830.8267 17 2.7631213953333535\n",
      "Training loss: 1182592037901.6533 18 2.7630304216267323\n",
      "Training loss: 1180921502542.5066 19 2.7631184775447273\n",
      "Training loss: 1179255832576.0 20 2.7632019480302565\n",
      "Training loss: 1177604115660.8 21 2.7634242760950865\n",
      "Training loss: 1175964506303.1467 22 2.763586828407096\n",
      "Training loss: 1174336081756.16 23 2.7637684910113194\n",
      "Training loss: 1172717807424.8533 24 2.7638096317663767\n",
      "Training loss: 1171107893739.52 25 2.7638745585745492\n",
      "Training loss: 1169511569599.1467 26 2.763758277011209\n",
      "Training loss: 1167923494256.64 27 2.763882063686841\n",
      "Training loss: 1166346547800.7466 28 2.7637079895534793\n",
      "Training loss: 1164765714623.1467 29 2.763801850601633\n",
      "Training loss: 1163200652028.5867 30 2.763534577001132\n",
      "Training loss: 1161641992738.1333 31 2.7636189685681782\n",
      "Training loss: 1160095888397.6533 32 2.763368375369028\n",
      "Training loss: 1158564352273.0667 33 2.763459289792814\n",
      "Training loss: 1157026217110.1868 34 2.7633500300369063\n",
      "Training loss: 1155503852530.3467 35 2.7632947200775337\n",
      "Training loss: 1153985234862.08 36 2.7632062581566403\n",
      "Training loss: 1152478808637.44 37 2.763096626341387\n",
      "Training loss: 1150972410374.8267 38 2.7631215781601157\n",
      "Training loss: 1149473561859.4133 39 2.7630073637472456\n",
      "Training loss: 1147978600065.7068 40 2.763018476252693\n",
      "Training loss: 1146488839208.96 41 2.762831583024694\n",
      "Training loss: 1145008305821.0134 42 2.7628414642591297\n",
      "Training loss: 1143520446382.08 43 2.762681624912392\n",
      "Training loss: 1142051097804.8 44 2.762694434237225\n",
      "Training loss: 1140569138353.4934 45 2.762510814851417\n",
      "Training loss: 1139089304016.2134 46 2.7625473382466277\n",
      "Training loss: 1137580445095.2534 47 2.7623417626354105\n",
      "Training loss: 1136052516072.1067 48 2.762281946892146\n",
      "Training loss: 1134505852491.0933 49 2.762014902659404\n",
      "Training loss: 1132971687936.0 50 2.762013931190495\n",
      "Training loss: 1131429358469.12 51 2.7619359375204944\n",
      "Training loss: 1129901345559.8933 52 2.762001162390503\n",
      "Training loss: 1128371571042.9866 53 2.761963843243507\n",
      "Training loss: 1126854603134.2932 54 2.7619551647289367\n",
      "Training loss: 1125337020061.0134 55 2.761771745635348\n",
      "Training loss: 1123837304722.7734 56 2.761665575812018\n",
      "Training loss: 1122345223017.8132 57 2.7614703028501877\n",
      "Training loss: 1120869862604.8 58 2.7614506070022284\n",
      "Training loss: 1119397494128.64 59 2.7613843430602647\n",
      "Training loss: 1117937988184.7466 60 2.7613716918975753\n",
      "Training loss: 1116484857582.9333 61 2.7613011109893053\n",
      "Training loss: 1115044337855.1467 62 2.761256454073818\n",
      "Training loss: 1113612458393.6 63 2.7611988100464915\n",
      "Training loss: 1112190896919.8933 64 2.7611128614983613\n",
      "Training loss: 1110779429737.8132 65 2.761117958061696\n",
      "Training loss: 1109366816112.64 66 2.760976118936207\n",
      "Training loss: 1107969022361.6 67 2.7609664367027253\n",
      "Training loss: 1106572458939.7334 68 2.7608672361114497\n",
      "Training loss: 1105183333416.96 69 2.760918401250707\n",
      "Training loss: 1103784533032.96 70 2.760847930078188\n",
      "Training loss: 1102385089522.3467 71 2.760976601960038\n",
      "Training loss: 1100962605301.76 72 2.760837081818728\n",
      "Training loss: 1099528265181.8667 73 2.7608427510439992\n",
      "Training loss: 1098081985276.5867 74 2.7606696525985055\n",
      "Training loss: 1096616411572.9066 75 2.760683401071178\n",
      "Training loss: 1095140995235.84 76 2.7606132525669556\n",
      "Training loss: 1093655121100.8 77 2.7606678461595187\n",
      "Training loss: 1092171120421.5466 78 2.7606713568369474\n",
      "Training loss: 1090683932071.2533 79 2.760749394112976\n",
      "Training loss: 1089204041809.92 80 2.760677524003489\n",
      "Training loss: 1087724543016.96 81 2.760719885394107\n",
      "Training loss: 1086258493958.8267 82 2.760653116214144\n",
      "Training loss: 1084798624508.5867 83 2.760757657537315\n",
      "Training loss: 1083349492476.5867 84 2.76075824649242\n",
      "Training loss: 1081905086027.0934 85 2.7608597258884173\n",
      "Training loss: 1080468704679.2533 86 2.760825553984025\n",
      "Training loss: 1079041970230.6133 87 2.760858967421302\n",
      "Training loss: 1077617472744.1067 88 2.7607832213132286\n",
      "Training loss: 1076207431625.3867 89 2.760820491873761\n",
      "Training loss: 1074800662063.7866 90 2.7607950924028763\n",
      "Training loss: 1073406894844.5867 91 2.760783834499392\n",
      "Training loss: 1072017937093.9734 92 2.7607768055947535\n",
      "Training loss: 1070638039040.0 93 2.7607506072092316\n",
      "Training loss: 1069267787885.2267 94 2.7607315403784964\n",
      "Training loss: 1067906260882.7733 95 2.760724271416468\n",
      "Training loss: 1066551249032.5333 96 2.7607185577685516\n",
      "Training loss: 1065204709676.3733 97 2.760674069834289\n",
      "Training loss: 1063866055611.7333 98 2.7606028312321644\n",
      "Training loss: 1062535818117.12 99 2.760493113699507\n",
      "Training loss: 1061215143635.6267 100 2.7603922435732966\n",
      "Training loss: 1059901152078.5067 101 2.7603090015301053\n",
      "Training loss: 1058594234914.1333 102 2.7602398864471827\n",
      "Training loss: 1057295147117.2267 103 2.7601682941533214\n",
      "Training loss: 1056000728978.7733 104 2.7601055125279124\n",
      "Training loss: 1054713161536.8534 105 2.760026466073013\n",
      "Training loss: 1053433255690.24 106 2.7599749159027493\n",
      "Training loss: 1052158466894.5067 107 2.759905645993853\n",
      "Training loss: 1050893213149.8667 108 2.7598301115077417\n",
      "Training loss: 1049634055127.04 109 2.759781391682545\n",
      "Training loss: 1048379566762.6666 110 2.7597090773049384\n",
      "Training loss: 1047134585487.36 111 2.7596539474258712\n",
      "Training loss: 1045894553490.7733 112 2.759602448804586\n",
      "Training loss: 1044663217684.48 113 2.7595567361607287\n",
      "Training loss: 1043437082815.1466 114 2.759511353740425\n",
      "Training loss: 1042218357882.88 115 2.759475671859939\n",
      "Training loss: 1041006064216.7467 116 2.759457521405461\n",
      "Training loss: 1039801739728.2134 117 2.7594462647616322\n",
      "Training loss: 1038605048872.96 118 2.759466940499375\n",
      "Training loss: 1037413307296.4266 119 2.7594928210144105\n",
      "Training loss: 1036229506935.4667 120 2.7595254216370986\n",
      "Training loss: 1035051634524.16 121 2.7595450481318924\n",
      "Training loss: 1033881563518.2933 122 2.7595497552898536\n",
      "Training loss: 1032716413829.12 123 2.7595532190414267\n",
      "Training loss: 1031557275975.68 124 2.759527911913533\n",
      "Training loss: 1030405436211.2 125 2.7595248991690444\n",
      "Training loss: 1029257706864.64 126 2.7594919763253016\n",
      "Training loss: 1028117163758.9333 127 2.7594704434017383\n",
      "Training loss: 1026981318273.7067 128 2.7594320060392086\n",
      "Training loss: 1025853246231.8933 129 2.7594027045182212\n",
      "Training loss: 1024729676076.3733 130 2.7593699104560434\n",
      "Training loss: 1023610272262.8267 131 2.7593220603505775\n",
      "Training loss: 1022496880285.0133 132 2.7592852359466358\n",
      "Training loss: 1021386424320.0 133 2.7592631069443514\n",
      "Training loss: 1020278233279.1466 134 2.759214075046415\n",
      "Training loss: 1019173481567.5734 135 2.759147639790123\n",
      "Training loss: 1018072728425.8134 136 2.7591085275056857\n",
      "Training loss: 1016970828840.96 137 2.7591128039491766\n",
      "Training loss: 1015856681888.4266 138 2.7590142150016654\n",
      "Training loss: 1014759619734.1866 139 2.7589550106003005\n",
      "Training loss: 1013646759034.88 140 2.758918013131609\n",
      "Training loss: 1012531102132.9066 141 2.7587614711286403\n",
      "Training loss: 1011423498294.6133 142 2.7586767270717147\n",
      "Training loss: 1010305408696.32 143 2.758607652846517\n",
      "Training loss: 1009185501566.2933 144 2.7585076515729328\n",
      "Training loss: 1008072165512.5333 145 2.7584964303788166\n",
      "Training loss: 1006942835179.52 146 2.7586679090175306\n",
      "Training loss: 1005782886427.3066 147 2.7584332441723967\n",
      "Training loss: 1004627523447.4667 148 2.7584758325286662\n",
      "Training loss: 1003451999846.4 149 2.758267987348044\n",
      "Training loss: 1002285535941.9734 150 2.7582018638819052\n",
      "Training loss: 1001099498618.88 151 2.758169072057707\n",
      "Training loss: 999918885928.96 152 2.757993525466938\n",
      "Training loss: 998741656644.2667 153 2.7579073402074163\n",
      "Training loss: 997548712700.5867 154 2.7577512572396863\n",
      "Training loss: 996367037453.6533 155 2.757497530319284\n",
      "Training loss: 995184858890.24 156 2.757417726794743\n",
      "Training loss: 993991187933.8667 157 2.7572185828066464\n",
      "Training loss: 992814489927.68 158 2.7571725259658613\n",
      "Training loss: 991605132274.3467 159 2.7570811273160167\n",
      "Training loss: 990402149963.0934 160 2.7568826080339934\n",
      "Training loss: 989197378082.1333 161 2.7567512910471637\n",
      "Training loss: 987972389655.8933 162 2.756569677157477\n",
      "Training loss: 986751707381.76 163 2.7563415103841264\n",
      "Training loss: 985538742626.9867 164 2.7561414857185462\n",
      "Training loss: 984323624796.16 165 2.755966970610457\n",
      "Training loss: 983094833534.2933 166 2.7558765851939846\n",
      "Training loss: 981836905840.64 167 2.7559376605668215\n",
      "Training loss: 980579733121.7067 168 2.755897502592792\n",
      "Training loss: 979343056568.32 169 2.7558982241904273\n",
      "Training loss: 978070253076.48 170 2.7559841168628862\n",
      "Training loss: 976798008825.1733 171 2.755818041252049\n",
      "Training loss: 975561388195.84 172 2.755772726283484\n",
      "Training loss: 974297672362.6666 173 2.755785254234912\n",
      "Training loss: 973064686796.8 174 2.755719637028809\n",
      "Training loss: 971857566105.6 175 2.7558632062304715\n",
      "Training loss: 970614570134.1866 176 2.756045028013138\n",
      "Training loss: 969376355669.3334 177 2.7560146213782333\n",
      "Training loss: 968158721256.1067 178 2.7560999095160716\n",
      "Training loss: 966913348512.4266 179 2.756051546371997\n",
      "Training loss: 965713386100.0533 180 2.7561229751175467\n",
      "Training loss: 964464657913.1733 181 2.756240605539523\n",
      "Training loss: 963240032993.28 182 2.7562230829929297\n",
      "Training loss: 962008417566.72 183 2.7564189246072517\n",
      "Training loss: 960766735810.56 184 2.7564133311403305\n",
      "Training loss: 959553715131.7333 185 2.7565247564180577\n",
      "Training loss: 958317709666.9867 186 2.7565115212514355\n",
      "Training loss: 957117635406.5067 187 2.756603917161178\n",
      "Training loss: 955894184891.7333 188 2.7567072205827046\n",
      "Training loss: 954690615377.92 189 2.756775722388753\n",
      "Training loss: 953484473357.6533 190 2.756844773770068\n",
      "Training loss: 952283560236.3733 191 2.7568789653403987\n",
      "Training loss: 951084520570.88 192 2.7569279777383504\n",
      "Training loss: 949889870943.5734 193 2.756946368151176\n",
      "Training loss: 948701121303.8933 194 2.7570228266984658\n",
      "Training loss: 947509323803.3066 195 2.757064328937427\n",
      "Training loss: 946324349037.2267 196 2.757168622321316\n",
      "Training loss: 945130566232.7467 197 2.7572361818093394\n",
      "Training loss: 943947017530.0266 198 2.757355582440255\n",
      "Training loss: 942758239928.32 199 2.7574959777603656\n",
      "Training loss: 941591328631.4667 200 2.7575671161173654\n",
      "Training loss: 940429758081.7067 201 2.757672478248767\n",
      "Training loss: 939268662886.4 202 2.7576881786728027\n",
      "Training loss: 938126553907.2 203 2.757724757700651\n",
      "Training loss: 936983074788.6934 204 2.757748897209709\n",
      "Training loss: 935850165316.2667 205 2.7577523310373198\n",
      "Training loss: 934727182363.3066 206 2.757784424521283\n",
      "Training loss: 933610938258.7733 207 2.7578151295054187\n",
      "Training loss: 932506773749.76 208 2.757863316876297\n",
      "Training loss: 931404035304.1067 209 2.7579265575172114\n",
      "Training loss: 930309349922.1333 210 2.757939943353002\n",
      "Training loss: 929228002426.88 211 2.7579805488972413\n",
      "Training loss: 928151128855.8933 212 2.758048050552486\n",
      "Training loss: 927077694614.1866 213 2.758095225712308\n",
      "Training loss: 926010300170.24 214 2.7580670447337785\n",
      "Training loss: 924949924194.9867 215 2.758032264454421\n",
      "Training loss: 923893183283.2 216 2.757988958930332\n",
      "Training loss: 922845026713.6 217 2.7579591120740568\n",
      "Training loss: 921804056384.8534 218 2.7579573083863367\n",
      "Training loss: 920769097891.84 219 2.757969248520892\n",
      "Training loss: 919742080614.4 220 2.757998422356681\n",
      "Training loss: 918721550527.1466 221 2.758043461352707\n",
      "Training loss: 917706836541.44 222 2.7580599160370114\n",
      "Training loss: 916701377986.56 223 2.7580832564114677\n",
      "Training loss: 915698743596.3733 224 2.75812917602701\n",
      "Training loss: 914699884079.7866 225 2.758131476765107\n",
      "Training loss: 913711706057.3867 226 2.7581292414722327\n",
      "Training loss: 912728421389.6533 227 2.7581707081857054\n",
      "Training loss: 911747625342.2933 228 2.758166476316233\n",
      "Training loss: 910778070029.6533 229 2.7581621726078325\n",
      "Training loss: 909813436033.7067 230 2.758175532352684\n",
      "Training loss: 908851738050.56 231 2.7581397914300814\n",
      "Training loss: 907901252840.1067 232 2.758006823442814\n",
      "Training loss: 906958261452.8 233 2.757766070138539\n",
      "Training loss: 905992508975.7866 234 2.757768082422924\n",
      "Training loss: 904995634763.0934 235 2.757465201003311\n",
      "Training loss: 904050769920.0 236 2.7573351462190985\n",
      "Training loss: 903075650164.0533 237 2.757405515472648\n",
      "Training loss: 902098405294.08 238 2.7571662191003883\n",
      "Training loss: 901160614843.7333 239 2.7573282040248484\n",
      "Training loss: 900154261504.0 240 2.7571393900995833\n",
      "Training loss: 899193346457.6 241 2.757035517857828\n",
      "Training loss: 898204217726.2933 242 2.756847819102763\n",
      "Training loss: 897225379020.8 243 2.7566698136266736\n",
      "Training loss: 896220675440.64 244 2.7566116581634037\n",
      "Training loss: 895226513544.5333 245 2.7563965283175342\n",
      "Training loss: 894239705661.44 246 2.756417040754097\n",
      "Training loss: 893240175056.2134 247 2.756347056212758\n",
      "Training loss: 892260693224.1067 248 2.756377356292021\n",
      "Training loss: 891274081075.2 249 2.756418258526171\n",
      "Training loss: 890298989281.28 250 2.7564256470769166\n",
      "Training loss: 889326330183.68 251 2.756467865154711\n",
      "Training loss: 888350119908.6934 252 2.756404498303051\n",
      "Training loss: 887389065052.16 253 2.7564132871532854\n",
      "Training loss: 886424011625.8134 254 2.7564463692126493\n",
      "Training loss: 885470730212.6934 255 2.7564245904038494\n",
      "Training loss: 884524718926.5067 256 2.7564807540893437\n",
      "Training loss: 883575436083.2 257 2.756496769289017\n",
      "Training loss: 882639295392.4266 258 2.75645756431046\n",
      "Training loss: 881710229094.4 259 2.756449963084735\n",
      "Training loss: 880779904505.1733 260 2.756437683920695\n",
      "Training loss: 879861939131.7333 261 2.756350951073413\n",
      "Training loss: 878961505949.0133 262 2.756304912953421\n",
      "Training loss: 878050754778.4534 263 2.756309344837667\n",
      "Training loss: 877136116886.1866 264 2.7561595806366723\n",
      "Training loss: 876248406425.6 265 2.7560409862030815\n",
      "Training loss: 875331168064.8534 266 2.7560136440715626\n",
      "Training loss: 874407246779.7333 267 2.7558771322700295\n",
      "Training loss: 873487352026.4534 268 2.7559167478844784\n",
      "Training loss: 872541648322.56 269 2.7558019255210504\n",
      "Training loss: 871632099519.1466 270 2.7557667788384212\n",
      "Training loss: 870703256917.3334 271 2.7558166505021675\n",
      "Training loss: 869775868340.9066 272 2.755765960505595\n",
      "Training loss: 868866850816.0 273 2.7557699909209545\n",
      "Training loss: 867945669809.4933 274 2.755746730075987\n",
      "Training loss: 867038637588.48 275 2.7557065463791726\n",
      "Training loss: 866141615773.0133 276 2.755728915622817\n",
      "Training loss: 865240203919.36 277 2.7557249706951947\n",
      "Training loss: 864349948914.3467 278 2.755703542706658\n",
      "Training loss: 863464950770.3467 279 2.7557163610173596\n",
      "Training loss: 862579812816.2134 280 2.7556865123710126\n",
      "Training loss: 861706782419.6267 281 2.755648181832643\n",
      "Training loss: 860835373820.5867 282 2.755639369101157\n",
      "Training loss: 859965083702.6133 283 2.755599966246239\n",
      "Training loss: 859107236686.5067 284 2.7555806480271943\n",
      "Training loss: 858250004834.9867 285 2.755587769757172\n",
      "Training loss: 857396799515.3066 286 2.755566395358819\n",
      "Training loss: 856553129246.72 287 2.7555642004047876\n",
      "Training loss: 855706299269.12 288 2.7555477958027748\n",
      "Training loss: 854866739418.4534 289 2.7554846298087963\n",
      "Training loss: 854037539498.6666 290 2.7554646577535213\n",
      "Training loss: 853206480104.1067 291 2.7554613986814096\n",
      "Training loss: 852378915962.88 292 2.755398690696584\n",
      "Training loss: 851561292322.1333 293 2.755313833161697\n",
      "Training loss: 850743570814.2933 294 2.7552481094283485\n",
      "Training loss: 849927275369.8134 295 2.7551398157253146\n",
      "Training loss: 849117467115.52 296 2.7550328761615197\n",
      "Training loss: 848307421184.0 297 2.7549681220789526\n",
      "Training loss: 847496005113.1733 298 2.754847408123771\n",
      "Training loss: 846696724561.92 299 2.7547018897235356\n",
      "Training loss: 845894354206.72 300 2.754626336136357\n",
      "Training loss: 845086810876.5867 301 2.7544490183404404\n",
      "Training loss: 844296100686.5067 302 2.7542595450583134\n",
      "Training loss: 843496163027.6267 303 2.7542811621919796\n",
      "Training loss: 842695777976.32 304 2.7540850303767788\n",
      "Training loss: 841903138406.4 305 2.7543000189376023\n",
      "Training loss: 841037000649.3867 306 2.754187039156235\n",
      "Training loss: 840223277711.36 307 2.7541435745970193\n",
      "Training loss: 839381243221.3334 308 2.7543618364831874\n",
      "Training loss: 838501390090.24 309 2.754001920802575\n",
      "Training loss: 837684829006.5067 310 2.7539149363473014\n",
      "Training loss: 836836209459.2 311 2.7539136921965355\n",
      "Training loss: 835981494190.08 312 2.7536056734452425\n",
      "Training loss: 835170064138.24 313 2.7535246155834816\n",
      "Training loss: 834338347636.0533 314 2.753573933049375\n",
      "Training loss: 833501122614.6133 315 2.7533906409198217\n",
      "Training loss: 832695298949.12 316 2.7532854227493027\n",
      "Training loss: 831875270574.08 317 2.753295717247358\n",
      "Training loss: 831059114939.7333 318 2.7530695190611203\n",
      "Training loss: 830254927052.8 319 2.753099945406583\n",
      "Training loss: 829403035948.3733 320 2.752885360344488\n",
      "Training loss: 828568677034.6666 321 2.7528532512099835\n",
      "Training loss: 827711626936.32 322 2.752741211780224\n",
      "Training loss: 826872975851.52 323 2.752754705506095\n",
      "Training loss: 826025768386.56 324 2.752800009799894\n",
      "Training loss: 825178043624.1067 325 2.752743495012702\n",
      "Training loss: 824352045356.3733 326 2.7528346844842817\n",
      "Training loss: 823510891670.1866 327 2.752904640995043\n",
      "Training loss: 822677008110.9333 328 2.7528928014561362\n",
      "Training loss: 821865382324.9066 329 2.7529742548300784\n",
      "Training loss: 821046388544.8534 330 2.753050375855173\n",
      "Training loss: 820226332207.7866 331 2.7529662124854566\n",
      "Training loss: 819433762542.9333 332 2.75297956957464\n",
      "Training loss: 818628358307.84 333 2.753053429523633\n",
      "Training loss: 817814355749.5466 334 2.7530193641016716\n",
      "Training loss: 817026050293.76 335 2.752994418464979\n",
      "Training loss: 816245783920.64 336 2.753141892877606\n",
      "Training loss: 815441358356.48 337 2.7531769433126043\n",
      "Training loss: 814654954318.5067 338 2.752993215388005\n",
      "Training loss: 813905264421.5466 339 2.753081168396697\n",
      "Training loss: 813124424826.88 340 2.7532242601770633\n",
      "Training loss: 812337685244.5867 341 2.7530414230335176\n",
      "Training loss: 811597166892.3733 342 2.752982048492656\n",
      "Training loss: 810849937653.76 343 2.7531854396087825\n",
      "Training loss: 810067699957.76 344 2.7531032050876565\n",
      "Training loss: 809322917396.48 345 2.752888403260894\n",
      "Training loss: 808606474349.2267 346 2.7530701254037955\n",
      "Training loss: 807836022647.4667 347 2.753048383610204\n",
      "Training loss: 807091883212.8 348 2.7527490405690074\n",
      "Training loss: 806391518330.88 349 2.752815379038442\n",
      "Training loss: 805639996921.1733 350 2.7528275266924163\n",
      "Training loss: 804899506530.9867 351 2.7525201081985617\n",
      "Training loss: 804207712010.24 352 2.7525079254108666\n",
      "Training loss: 803475386531.84 353 2.7525485735113917\n",
      "Training loss: 802729807052.8 354 2.7522458461367485\n",
      "Training loss: 802036824145.92 355 2.75207898734625\n",
      "Training loss: 801333746947.4133 356 2.752249042749932\n",
      "Training loss: 800588992348.16 357 2.7520587010644713\n",
      "Training loss: 799896624605.8667 358 2.7518616528540845\n",
      "Training loss: 799208269414.4 359 2.752006605019084\n",
      "Training loss: 798468743714.1333 360 2.7518535626906644\n",
      "Training loss: 797769371484.16 361 2.7515643754841332\n",
      "Training loss: 797092424799.5734 362 2.751631087109837\n",
      "Training loss: 796365370163.2 363 2.751501067646782\n",
      "Training loss: 795668878021.9734 364 2.751217923815122\n",
      "Training loss: 795001843875.84 365 2.7513440133253284\n",
      "Training loss: 794285484714.6666 366 2.751325288293875\n",
      "Training loss: 793593186877.44 367 2.7511171083708876\n",
      "Training loss: 792930361016.32 368 2.751225512799234\n",
      "Training loss: 792226403013.9734 369 2.75119253968434\n",
      "Training loss: 791542018430.2933 370 2.7509649582357087\n",
      "Training loss: 790882520050.3467 371 2.7510034491486857\n",
      "Training loss: 790194332631.04 372 2.7509580865453342\n",
      "Training loss: 789515666281.8134 373 2.750779756372904\n",
      "Training loss: 788832819609.6 374 2.750682733364911\n",
      "Training loss: 788129714449.0667 375 2.7504527885817227\n",
      "Training loss: 787453816340.48 376 2.750418665350846\n",
      "Training loss: 786760148363.9467 377 2.750458954035172\n",
      "Training loss: 786050765728.4266 378 2.750268043204402\n",
      "Training loss: 785365136834.56 379 2.750111131877983\n",
      "Training loss: 784683184947.2 380 2.750071555796927\n",
      "Training loss: 783982190919.68 381 2.749960959244759\n",
      "Training loss: 783287180765.8667 382 2.749743968759895\n",
      "Training loss: 782607829346.9867 383 2.749658652299635\n",
      "Training loss: 781911393129.8134 384 2.7495821602107697\n",
      "Training loss: 781213614735.36 385 2.7493948629697798\n",
      "Training loss: 780541197899.0934 386 2.7493258484520053\n",
      "Training loss: 779858001701.5466 387 2.749426625922552\n",
      "Training loss: 779145669072.2134 388 2.7492309647528828\n",
      "Training loss: 778476649622.1866 389 2.7490765037653895\n",
      "Training loss: 777823079191.8933 390 2.7493966933628906\n",
      "Training loss: 777103965771.0934 391 2.74932267922531\n",
      "Training loss: 776439322378.24 392 2.749234425308351\n",
      "Training loss: 775789890327.8933 393 2.7495516475744126\n",
      "Training loss: 775090741794.1333 394 2.749604541152396\n",
      "Training loss: 774426979205.12 395 2.749566206047105\n",
      "Training loss: 773773143135.5734 396 2.74975768559976\n",
      "Training loss: 773089555469.6533 397 2.749729820829935\n",
      "Training loss: 772436250678.6133 398 2.749682837598907\n",
      "Training loss: 771790621463.8933 399 2.749846596447593\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "batch_size = 800\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        # get the training data\n",
    "        inputs = torch.from_numpy(X_train[i:i+batch_size]).float()\n",
    "        label = torch.from_numpy(y_train[i:i+batch_size]).reshape(-1, 1).float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(X_train)}\", e, test_torch_mode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9852636169898046.00000000\n",
      "Iteration 2, loss = 4782843223819727.00000000\n",
      "Iteration 3, loss = 4464188199836452.00000000\n",
      "Iteration 4, loss = 4733131575039396.00000000\n",
      "Iteration 5, loss = 4472290547449394.50000000\n",
      "Iteration 6, loss = 4269837520448216.50000000\n",
      "Iteration 7, loss = 4252351230490286.00000000\n",
      "Iteration 8, loss = 4281510894202930.50000000\n",
      "Iteration 9, loss = 4214104031035255.50000000\n",
      "Iteration 10, loss = 4258927805230352.00000000\n",
      "Iteration 11, loss = 4248635872957139.00000000\n",
      "Iteration 12, loss = 4312872056249307.50000000\n",
      "Iteration 13, loss = 4283809214964793.00000000\n",
      "Iteration 14, loss = 4302902660721231.50000000\n",
      "Iteration 15, loss = 4290204102481290.00000000\n",
      "Training loss did not improve more than tol=0.000010 for 5 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.402850552228329"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(800, 200, 200, 500), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=5)  # best\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9854516901168444.00000000\n",
      "Iteration 2, loss = 4595335552527831.00000000\n",
      "Iteration 3, loss = 4469739844047478.50000000\n",
      "Iteration 4, loss = 4260818295737205.00000000\n",
      "Iteration 5, loss = 4339022870742403.50000000\n",
      "Iteration 6, loss = 4250233669606186.50000000\n",
      "Iteration 7, loss = 4242800917244683.00000000\n",
      "Iteration 8, loss = 4322731268131541.50000000\n",
      "Iteration 9, loss = 4260251642815720.00000000\n",
      "Iteration 10, loss = 4294661196346540.50000000\n",
      "Iteration 11, loss = 4227379049223653.50000000\n",
      "Iteration 12, loss = 4391905475063076.50000000\n",
      "Iteration 13, loss = 4374447492365332.50000000\n",
      "Iteration 14, loss = 4246667755614483.00000000\n",
      "Iteration 15, loss = 4341612288546292.00000000\n",
      "Iteration 16, loss = 4390742857691477.50000000\n",
      "Iteration 17, loss = 4422849052051387.00000000\n",
      "Training loss did not improve more than tol=0.000010 for 5 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3882717886877685"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1000, 200, 200, 500), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=5)  # best\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3727823519272846"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1070578721402.98205566\n",
      "Iteration 2, loss = 169154981609.45535278\n",
      "Iteration 3, loss = 40208648798.18979645\n",
      "Iteration 4, loss = 20221177680.94396591\n",
      "Iteration 5, loss = 4665536178.37637901\n",
      "Iteration 6, loss = 1700233403.78227735\n",
      "Iteration 7, loss = 592078433.94917178\n",
      "Iteration 8, loss = 106634006.82095124\n",
      "Iteration 9, loss = 35144780.76744615\n",
      "Iteration 10, loss = 12643713.28618718\n",
      "Iteration 11, loss = 17373984.17633588\n",
      "Iteration 12, loss = 169458137.15847519\n",
      "Iteration 13, loss = 105565254.15546705\n",
      "Iteration 14, loss = 67669582.42899403\n",
      "Training loss did not improve more than tol=0.000010 for 3 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5734036178410284"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1000, 200, ), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=3)\n",
    "\n",
    "mlp_regr.fit(X_train, y_train/1e5)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds*1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9180644562707392.00000000\n",
      "Iteration 2, loss = 4774383633193965.00000000\n",
      "Iteration 3, loss = 4541828974319462.00000000\n",
      "Iteration 4, loss = 4263188750547031.00000000\n",
      "Iteration 5, loss = 4239700343049356.00000000\n",
      "Iteration 6, loss = 4328264131995082.00000000\n",
      "Iteration 7, loss = 4300430317325314.50000000\n",
      "Iteration 8, loss = 4255010981248534.00000000\n",
      "Iteration 9, loss = 4270247582459696.50000000\n",
      "Training loss did not improve more than tol=0.000010 for 3 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4818402431593323"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1500, 200, 200, 500), max_iter=1000, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=36, early_stopping=False, tol=0.00001, n_iter_no_change=3)\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_regr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save the model to disk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfinalized_model_mlp.sav\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m pickle\u001b[39m.\u001b[39mdump(mlp_regr, \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlp_regr' is not defined"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model_mlp.sav'\n",
    "pickle.dump(mlp_regr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "mlp_regr = pickle.load(open('finalized_model_mlp.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9613999578834082.00000000\n",
      "Iteration 2, loss = 5274506705902056.00000000\n",
      "Iteration 3, loss = 4375524773283108.50000000\n",
      "Iteration 4, loss = 4374893037198315.50000000\n",
      "Iteration 5, loss = 4297481899681947.50000000\n",
      "Iteration 6, loss = 4297198571356853.00000000\n",
      "Iteration 7, loss = 4350095746821185.00000000\n",
      "Iteration 8, loss = 4327564641121450.00000000\n",
      "Iteration 9, loss = 4283455694872205.00000000\n",
      "Iteration 10, loss = 4245628883181325.00000000\n",
      "Iteration 11, loss = 4262620671572127.50000000\n",
      "Iteration 12, loss = 4242173005254726.00000000\n",
      "Iteration 13, loss = 4244069790362055.50000000\n",
      "Iteration 14, loss = 4229229553737124.00000000\n",
      "Iteration 15, loss = 4231335361051601.00000000\n",
      "Iteration 16, loss = 4308477256085541.50000000\n",
      "Iteration 17, loss = 4284650450555558.50000000\n",
      "Iteration 18, loss = 4302296566850923.50000000\n",
      "Training loss did not improve more than tol=0.000010 for 3 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.466465940541211"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_regr = MLPRegressor(hidden_layer_sizes=(1000, 200, 200), max_iter=40, alpha=0.00001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=False, tol=0.00001, n_iter_no_change=3)\n",
    "\n",
    "mlp_regr.fit(X_train, y_train)\n",
    "\n",
    "preds = mlp_regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 12413326378239124.00000000\n",
      "Validation score: -0.232501\n",
      "Iteration 2, loss = 12413325714564238.00000000\n",
      "Validation score: -0.232501\n",
      "Iteration 3, loss = 12413324159517366.00000000\n",
      "Validation score: -0.232500\n",
      "Iteration 4, loss = 12413321346996724.00000000\n",
      "Validation score: -0.232500\n",
      "Iteration 5, loss = 12413316111855992.00000000\n",
      "Validation score: -0.232499\n",
      "Iteration 6, loss = 12413307770656950.00000000\n",
      "Validation score: -0.232498\n",
      "Iteration 7, loss = 12413295316480594.00000000\n",
      "Validation score: -0.232497\n",
      "Validation score did not improve more than tol=0.000010 for 5 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.903050181562582"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(700, 700), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.00001, n_iter_no_change=5)\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9467108744254146.00000000\n",
      "Validation score: 0.349119\n",
      "Iteration 2, loss = 5108539062161769.00000000\n",
      "Validation score: 0.581433\n",
      "Iteration 3, loss = 4471654366310205.50000000\n",
      "Validation score: 0.635126\n",
      "Iteration 4, loss = 4166449026078603.50000000\n",
      "Validation score: 0.634182\n",
      "Iteration 5, loss = 4142180488720378.50000000\n",
      "Validation score: 0.633509\n",
      "Iteration 6, loss = 4167795321421011.00000000\n",
      "Validation score: 0.634597\n",
      "Validation score did not improve more than tol=0.000100 for 2 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.194647771455093"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(700, 700), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.0001, n_iter_no_change=2)\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(700, 700), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.0001, n_iter_no_change=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2299880113177872"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | hidden... | hidden... |\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11375587325816614.00000000\n",
      "Validation score: -0.034541\n",
      "Iteration 2, loss = 8428575149673836.00000000\n",
      "Validation score: 0.349458\n",
      "Iteration 3, loss = 4987947446972370.00000000\n",
      "Validation score: 0.284422\n",
      "Iteration 4, loss = 4622814866719912.00000000\n",
      "Validation score: 0.337244\n",
      "Iteration 5, loss = 4305961583378769.50000000\n",
      "Validation score: 0.397142\n",
      "Iteration 6, loss = 4330318488294520.50000000\n",
      "Validation score: 0.399170\n",
      "Iteration 7, loss = 4362668666187466.00000000\n",
      "Validation score: 0.408670\n",
      "Iteration 8, loss = 4374220486846312.50000000\n",
      "Validation score: 0.369903\n",
      "Iteration 9, loss = 4344932282600223.50000000\n",
      "Validation score: 0.388153\n",
      "Iteration 10, loss = 4334746168919979.50000000\n",
      "Validation score: 0.398735\n",
      "Iteration 11, loss = 4316292373711605.50000000\n",
      "Validation score: 0.391287\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-3.087   \u001b[0m | \u001b[0m342.0    \u001b[0m | \u001b[0m395.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11136908684676256.00000000\n",
      "Validation score: -0.001707\n",
      "Iteration 2, loss = 7864117027034809.00000000\n",
      "Validation score: 0.444219\n",
      "Iteration 3, loss = 4788909047674117.00000000\n",
      "Validation score: 0.637308\n",
      "Iteration 4, loss = 4648298797922020.00000000\n",
      "Validation score: 0.638748\n",
      "Iteration 5, loss = 4456929699388795.50000000\n",
      "Validation score: 0.608838\n",
      "Iteration 6, loss = 4452262832403810.50000000\n",
      "Validation score: 0.637822\n",
      "Iteration 7, loss = 4390110221490623.50000000\n",
      "Validation score: 0.631820\n",
      "Iteration 8, loss = 4403220870927273.00000000\n",
      "Validation score: 0.632284\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-3.211   \u001b[0m | \u001b[0m275.1    \u001b[0m | \u001b[0m557.2    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 8693071227611712.00000000\n",
      "Validation score: 0.421237\n",
      "Iteration 2, loss = 4654546067858742.00000000\n",
      "Validation score: 0.516407\n",
      "Iteration 3, loss = 4120482875237545.00000000\n",
      "Validation score: 0.559091\n",
      "Iteration 4, loss = 4117058269346890.00000000\n",
      "Validation score: 0.580572\n",
      "Iteration 5, loss = 4100515385158389.50000000\n",
      "Validation score: 0.580793\n",
      "Iteration 6, loss = 4148582334270706.00000000\n",
      "Validation score: 0.578222\n",
      "Iteration 7, loss = 4015564436968071.50000000\n",
      "Validation score: 0.579254\n",
      "Iteration 8, loss = 4027123699335781.50000000\n",
      "Validation score: 0.579672\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m-2.886   \u001b[0m | \u001b[95m994.8    \u001b[0m | \u001b[95m752.0    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9247440551976690.00000000\n",
      "Validation score: 0.162022\n",
      "Iteration 2, loss = 5482543490688441.00000000\n",
      "Validation score: 0.539323\n",
      "Iteration 3, loss = 4150719401376204.00000000\n",
      "Validation score: 0.565134\n",
      "Iteration 4, loss = 4024333395375183.00000000\n",
      "Validation score: 0.522289\n",
      "Iteration 5, loss = 3972513447051934.50000000\n",
      "Validation score: 0.551838\n",
      "Iteration 6, loss = 3986869655771956.00000000\n",
      "Validation score: 0.547437\n",
      "Iteration 7, loss = 4025876117393433.00000000\n",
      "Validation score: 0.523987\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-3.248   \u001b[0m | \u001b[0m781.5    \u001b[0m | \u001b[0m208.6    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11311973190775058.00000000\n",
      "Validation score: 0.133429\n",
      "Iteration 2, loss = 7646858506136882.00000000\n",
      "Validation score: 0.686175\n",
      "Iteration 3, loss = 4789964921571001.00000000\n",
      "Validation score: 0.534328\n",
      "Iteration 4, loss = 4621377652554061.00000000\n",
      "Validation score: 0.709341\n",
      "Iteration 5, loss = 4511785158984577.00000000\n",
      "Validation score: 0.718222\n",
      "Iteration 6, loss = 4571266070836014.00000000\n",
      "Validation score: 0.648982\n",
      "Iteration 7, loss = 4502132786099089.00000000\n",
      "Validation score: 0.712301\n",
      "Iteration 8, loss = 4477069476825215.00000000\n",
      "Validation score: 0.695501\n",
      "Iteration 9, loss = 4459296201690094.00000000\n",
      "Validation score: 0.688235\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-3.152   \u001b[0m | \u001b[0m266.9    \u001b[0m | \u001b[0m955.1    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11835810351496078.00000000\n",
      "Validation score: -0.103667\n",
      "Iteration 2, loss = 9038982762552266.00000000\n",
      "Validation score: 0.333247\n",
      "Iteration 3, loss = 5175900751564645.00000000\n",
      "Validation score: 0.297662\n",
      "Iteration 4, loss = 4957631243115706.00000000\n",
      "Validation score: 0.209818\n",
      "Iteration 5, loss = 4455529128173025.50000000\n",
      "Validation score: 0.423213\n",
      "Iteration 6, loss = 4542692053312009.00000000\n",
      "Validation score: 0.379150\n",
      "Iteration 7, loss = 4496105520335060.50000000\n",
      "Validation score: 0.347333\n",
      "Iteration 8, loss = 4485395506347235.50000000\n",
      "Validation score: 0.357777\n",
      "Iteration 9, loss = 4520350692311571.00000000\n",
      "Validation score: 0.402280\n",
      "Validation score did not improve more than tol=0.000600 for 3 consecutive epochs. Stopping.\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-3.158   \u001b[0m | \u001b[0m270.8    \u001b[0m | \u001b[0m587.7    \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9136602400278264.00000000\n",
      "Validation score: 0.260112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m7        \u001b[0m | \u001b[0m-3.59    \u001b[0m | \u001b[0m786.9    \u001b[0m | \u001b[0m821.3    \u001b[0m |\n",
      "Iteration 1, loss = 10518599238916330.00000000\n",
      "Validation score: 0.322232\n",
      "Iteration 2, loss = 5007060162897217.00000000\n",
      "Validation score: 0.472295\n",
      "Iteration 3, loss = 5091064493678721.00000000\n",
      "Validation score: 0.598908\n",
      "Iteration 4, loss = 4662544059594209.00000000\n",
      "Validation score: 0.598116\n",
      "Iteration 5, loss = 4623396090526691.00000000\n",
      "Validation score: 0.576150\n",
      "Iteration 6, loss = 4462688179875994.50000000\n",
      "Validation score: 0.598359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m8        \u001b[0m | \u001b[0m-3.207   \u001b[0m | \u001b[0m659.9    \u001b[0m | \u001b[0m641.2    \u001b[0m |\n",
      "Iteration 1, loss = 10251500311837104.00000000\n",
      "Validation score: -0.000713\n",
      "Iteration 2, loss = 6963454146581432.00000000\n",
      "Validation score: 0.405383\n",
      "Iteration 3, loss = 4078656998781455.50000000\n",
      "Validation score: 0.537691\n",
      "Iteration 4, loss = 4204342291245517.50000000\n",
      "Validation score: 0.539390\n",
      "Iteration 5, loss = 4008674322957336.50000000\n",
      "Validation score: 0.522439\n",
      "Iteration 6, loss = 3957388536890302.50000000\n",
      "Validation score: 0.539712\n",
      "Iteration 7, loss = 3970471676513728.50000000\n",
      "Validation score: 0.539092\n"
     ]
    }
   ],
   "source": [
    "# use Basyian Optimization for hyperparameter tuning mlp\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def mlp_cv(hidden_layer_sizes, hidden_layer_sizes1):\n",
    "    regr = MLPRegressor(hidden_layer_sizes=(int(hidden_layer_sizes), int(hidden_layer_sizes1)), max_iter=1000, alpha=0.01,\n",
    "                    solver='adam', verbose=10,  random_state=21, early_stopping=True, tol=0.0006, n_iter_no_change=3)\n",
    "    regr.fit(X_train_scaled, y_train)\n",
    "    preds = regr.predict(X_train_scaled)\n",
    "    return -rmsle(y_test, preds)\n",
    "\n",
    "mlp_bo = BayesianOptimization(\n",
    "        mlp_cv,\n",
    "        {\n",
    "            'hidden_layer_sizes': (100, 1000),\n",
    "            'hidden_layer_sizes1': (100, 1000),\n",
    "        }\n",
    "    )\n",
    "\n",
    "mlp_bo.maximize(n_iter=10, init_points=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11867311510283992.00000000\n",
      "Iteration 2, loss = 11867007958852102.00000000\n",
      "Iteration 3, loss = 11865243499438610.00000000\n",
      "Iteration 4, loss = 11859052896370234.00000000\n",
      "Iteration 5, loss = 11843172897697096.00000000\n",
      "Iteration 6, loss = 11807633379559452.00000000\n",
      "Iteration 7, loss = 11741986562520302.00000000\n",
      "Iteration 8, loss = 11613994102165054.00000000\n",
      "Iteration 9, loss = 11415648492407526.00000000\n",
      "Iteration 10, loss = 11114399338167910.00000000\n",
      "Iteration 11, loss = 10699740333967104.00000000\n",
      "Iteration 12, loss = 10129325436427210.00000000\n",
      "Iteration 13, loss = 9487662338565064.00000000\n",
      "Iteration 14, loss = 8779340193346599.00000000\n",
      "Iteration 15, loss = 8161701825609054.00000000\n",
      "Iteration 16, loss = 7632102388757609.00000000\n",
      "Iteration 17, loss = 7298021646151120.00000000\n",
      "Iteration 18, loss = 7037393193655040.00000000\n",
      "Iteration 19, loss = 6864654174271565.00000000\n",
      "Iteration 20, loss = 6697552651973083.00000000\n",
      "Iteration 21, loss = 6549667340940541.00000000\n",
      "Iteration 22, loss = 6404721557477468.00000000\n",
      "Iteration 23, loss = 6273090117248350.00000000\n",
      "Iteration 24, loss = 6136206762017560.00000000\n",
      "Iteration 25, loss = 6001447103474885.00000000\n",
      "Iteration 26, loss = 5873565159672845.00000000\n",
      "Iteration 27, loss = 5740725578298914.00000000\n",
      "Iteration 28, loss = 5606400776825841.00000000\n",
      "Iteration 29, loss = 5479539948093507.00000000\n",
      "Iteration 30, loss = 5341669142578653.00000000\n",
      "Iteration 31, loss = 5203229211381284.00000000\n",
      "Iteration 32, loss = 5062194164684983.00000000\n",
      "Iteration 33, loss = 4915307741069494.00000000\n",
      "Iteration 34, loss = 4774046487993329.00000000\n",
      "Iteration 35, loss = 4625280218032630.00000000\n",
      "Iteration 36, loss = 4482024216237778.00000000\n",
      "Iteration 37, loss = 4347578950352589.00000000\n",
      "Iteration 38, loss = 4186888954470734.50000000\n",
      "Iteration 39, loss = 4045111037860137.00000000\n",
      "Iteration 40, loss = 3899997417236941.50000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.213741745552174"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(800, 500, 100), max_iter=40, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11867311510283992.00000000\n",
      "Iteration 2, loss = 11867007958852102.00000000\n",
      "Iteration 3, loss = 11865243499438610.00000000\n",
      "Iteration 4, loss = 11859052896370234.00000000\n",
      "Iteration 5, loss = 11843172897697096.00000000\n",
      "Iteration 6, loss = 11807633379559452.00000000\n",
      "Iteration 7, loss = 11741986562520302.00000000\n",
      "Iteration 8, loss = 11613994102165054.00000000\n",
      "Iteration 9, loss = 11415648492407526.00000000\n",
      "Iteration 10, loss = 11114399338167910.00000000\n",
      "Iteration 11, loss = 10699740333967104.00000000\n",
      "Iteration 12, loss = 10129325436427210.00000000\n",
      "Iteration 13, loss = 9487662338565064.00000000\n",
      "Iteration 14, loss = 8779340193346599.00000000\n",
      "Iteration 15, loss = 8161701825609054.00000000\n",
      "Iteration 16, loss = 7632102388757609.00000000\n",
      "Iteration 17, loss = 7298021646151120.00000000\n",
      "Iteration 18, loss = 7037393193655040.00000000\n",
      "Iteration 19, loss = 6864654174271565.00000000\n",
      "Iteration 20, loss = 6697552651973083.00000000\n",
      "Iteration 21, loss = 6549667340940541.00000000\n",
      "Iteration 22, loss = 6404721557477468.00000000\n",
      "Iteration 23, loss = 6273090117248350.00000000\n",
      "Iteration 24, loss = 6136206762017560.00000000\n",
      "Iteration 25, loss = 6001447103474885.00000000\n",
      "Iteration 26, loss = 5873565159672845.00000000\n",
      "Iteration 27, loss = 5740725578298914.00000000\n",
      "Iteration 28, loss = 5606400776825841.00000000\n",
      "Iteration 29, loss = 5479539948093507.00000000\n",
      "Iteration 30, loss = 5341669142578653.00000000\n",
      "Iteration 31, loss = 5203229211381284.00000000\n",
      "Iteration 32, loss = 5062194164684983.00000000\n",
      "Iteration 33, loss = 4915307741069494.00000000\n",
      "Iteration 34, loss = 4774046487993329.00000000\n",
      "Iteration 35, loss = 4625280218032630.00000000\n",
      "Iteration 36, loss = 4482024216237778.00000000\n",
      "Iteration 37, loss = 4347578950352589.00000000\n",
      "Iteration 38, loss = 4186888954470734.50000000\n",
      "Iteration 39, loss = 4045111037860137.00000000\n",
      "Iteration 40, loss = 3899997417236941.50000000\n",
      "Iteration 41, loss = 3759271632654575.00000000\n",
      "Iteration 42, loss = 3621936016019620.00000000\n",
      "Iteration 43, loss = 3496162630462079.50000000\n",
      "Iteration 44, loss = 3354715657925859.00000000\n",
      "Iteration 45, loss = 3218761196044523.50000000\n",
      "Iteration 46, loss = 3096169089255044.50000000\n",
      "Iteration 47, loss = 2973296475338320.50000000\n",
      "Iteration 48, loss = 2854646427148609.50000000\n",
      "Iteration 49, loss = 2746863010162829.00000000\n",
      "Iteration 50, loss = 2631766895860788.00000000\n",
      "Iteration 51, loss = 2517099029006146.00000000\n",
      "Iteration 52, loss = 2416446852598399.50000000\n",
      "Iteration 53, loss = 2319282447499351.50000000\n",
      "Iteration 54, loss = 2216252235836323.00000000\n",
      "Iteration 55, loss = 2116931215346812.25000000\n",
      "Iteration 56, loss = 2030244406201221.50000000\n",
      "Iteration 57, loss = 1940490602826770.00000000\n",
      "Iteration 58, loss = 1857557258272393.75000000\n",
      "Iteration 59, loss = 1778103541310057.75000000\n",
      "Iteration 60, loss = 1695737704197584.50000000\n",
      "Iteration 61, loss = 1623475764375725.75000000\n",
      "Iteration 62, loss = 1556658714978481.00000000\n",
      "Iteration 63, loss = 1484267692405483.00000000\n",
      "Iteration 64, loss = 1417740979711618.50000000\n",
      "Iteration 65, loss = 1354227595477166.25000000\n",
      "Iteration 66, loss = 1294683114322430.00000000\n",
      "Iteration 67, loss = 1238065911770975.25000000\n",
      "Iteration 68, loss = 1184337016799379.75000000\n",
      "Iteration 69, loss = 1129612402736030.50000000\n",
      "Iteration 70, loss = 1081619363412499.87500000\n",
      "Iteration 71, loss = 1033077165816081.75000000\n",
      "Iteration 72, loss = 989864972578120.50000000\n",
      "Iteration 73, loss = 950163203027605.75000000\n",
      "Iteration 74, loss = 908268554893985.25000000\n",
      "Iteration 75, loss = 868215381273504.25000000\n",
      "Iteration 76, loss = 832278079425532.00000000\n",
      "Iteration 77, loss = 797428094170822.87500000\n",
      "Iteration 78, loss = 765135963944035.62500000\n",
      "Iteration 79, loss = 732005548382610.12500000\n",
      "Iteration 80, loss = 705095792935014.37500000\n",
      "Iteration 81, loss = 674556495044340.50000000\n",
      "Iteration 82, loss = 647036997927053.87500000\n",
      "Iteration 83, loss = 622242565133689.75000000\n",
      "Iteration 84, loss = 595422031232874.75000000\n",
      "Iteration 85, loss = 572920137810288.62500000\n",
      "Iteration 86, loss = 552974904000016.93750000\n",
      "Iteration 87, loss = 530083277700004.56250000\n",
      "Iteration 88, loss = 509304980883929.18750000\n",
      "Iteration 89, loss = 489214901379390.93750000\n",
      "Iteration 90, loss = 471451839090490.75000000\n",
      "Iteration 91, loss = 453025532563239.87500000\n",
      "Iteration 92, loss = 436067195157342.37500000\n",
      "Iteration 93, loss = 420412018204269.12500000\n",
      "Iteration 94, loss = 404547365490642.25000000\n",
      "Iteration 95, loss = 390095309443591.37500000\n",
      "Iteration 96, loss = 375118438833278.87500000\n",
      "Iteration 97, loss = 361417438706851.43750000\n",
      "Iteration 98, loss = 348643728004764.50000000\n",
      "Iteration 99, loss = 336913162742171.06250000\n",
      "Iteration 100, loss = 324227194170848.00000000\n",
      "Iteration 101, loss = 313817516124606.12500000\n",
      "Iteration 102, loss = 302278959383644.06250000\n",
      "Iteration 103, loss = 291840624841466.68750000\n",
      "Iteration 104, loss = 281684683464436.62500000\n",
      "Iteration 105, loss = 271615507172981.87500000\n",
      "Iteration 106, loss = 261844324334830.28125000\n",
      "Iteration 107, loss = 252898105770378.56250000\n",
      "Iteration 108, loss = 244346852907208.00000000\n",
      "Iteration 109, loss = 236649082898162.87500000\n",
      "Iteration 110, loss = 228172799519684.21875000\n",
      "Iteration 111, loss = 220361560852261.90625000\n",
      "Iteration 112, loss = 212945541189463.15625000\n",
      "Iteration 113, loss = 205491943343133.34375000\n",
      "Iteration 114, loss = 198614969343808.71875000\n",
      "Iteration 115, loss = 192083171332981.00000000\n",
      "Iteration 116, loss = 186680488863166.46875000\n",
      "Iteration 117, loss = 179560556525395.12500000\n",
      "Iteration 118, loss = 173935086499675.40625000\n",
      "Iteration 119, loss = 167938487813206.81250000\n",
      "Iteration 120, loss = 162164596432791.40625000\n",
      "Iteration 121, loss = 156768241871258.40625000\n",
      "Iteration 122, loss = 151509810230365.00000000\n",
      "Iteration 123, loss = 146313550183729.28125000\n",
      "Iteration 124, loss = 141918380114716.78125000\n",
      "Iteration 125, loss = 137054716344850.98437500\n",
      "Iteration 126, loss = 132614842243614.48437500\n",
      "Iteration 127, loss = 128353702548980.76562500\n",
      "Iteration 128, loss = 124161678865160.26562500\n",
      "Iteration 129, loss = 120133331780438.50000000\n",
      "Iteration 130, loss = 116096300086809.23437500\n",
      "Iteration 131, loss = 112162041472835.95312500\n",
      "Iteration 132, loss = 109193416065008.51562500\n",
      "Iteration 133, loss = 105067225202569.70312500\n",
      "Iteration 134, loss = 102032671661283.95312500\n",
      "Iteration 135, loss = 98607532354645.43750000\n",
      "Iteration 136, loss = 95441062333366.68750000\n",
      "Iteration 137, loss = 92282673270055.01562500\n",
      "Iteration 138, loss = 89471319326183.43750000\n",
      "Iteration 139, loss = 86605738894220.12500000\n",
      "Iteration 140, loss = 83623321631415.26562500\n",
      "Iteration 141, loss = 81080691732222.21875000\n",
      "Iteration 142, loss = 78626439406784.50000000\n",
      "Iteration 143, loss = 76138050871330.85937500\n",
      "Iteration 144, loss = 73838642388220.87500000\n",
      "Iteration 145, loss = 71378997509135.59375000\n",
      "Iteration 146, loss = 69028816252129.82812500\n",
      "Iteration 147, loss = 67000735082430.58593750\n",
      "Iteration 148, loss = 64890797314368.96093750\n",
      "Iteration 149, loss = 62886356640477.14843750\n",
      "Iteration 150, loss = 60799086469714.81250000\n",
      "Iteration 151, loss = 58967973224208.02343750\n",
      "Iteration 152, loss = 57027821972193.73437500\n",
      "Iteration 153, loss = 55339044423829.17968750\n",
      "Iteration 154, loss = 53608291363035.99218750\n",
      "Iteration 155, loss = 52107402044040.68750000\n",
      "Iteration 156, loss = 50327917800604.91406250\n",
      "Iteration 157, loss = 48715443949312.42968750\n",
      "Iteration 158, loss = 47265887686285.92187500\n",
      "Iteration 159, loss = 45911319661323.07812500\n",
      "Iteration 160, loss = 44561079590318.30468750\n",
      "Iteration 161, loss = 43155570681346.58593750\n",
      "Iteration 162, loss = 41815228732926.82031250\n",
      "Iteration 163, loss = 40607990884299.32812500\n",
      "Iteration 164, loss = 39340598584952.07812500\n",
      "Iteration 165, loss = 38165719688297.23437500\n",
      "Iteration 166, loss = 36961540710346.35937500\n",
      "Iteration 167, loss = 35978614183015.71093750\n",
      "Iteration 168, loss = 35028090564121.52734375\n",
      "Iteration 169, loss = 33866195627725.44140625\n",
      "Iteration 170, loss = 32797689469068.80859375\n",
      "Iteration 171, loss = 31871888565948.02734375\n",
      "Iteration 172, loss = 30914042572696.65234375\n",
      "Iteration 173, loss = 30191434089401.27343750\n",
      "Iteration 174, loss = 29141445140136.26171875\n",
      "Iteration 175, loss = 28282911422644.21093750\n",
      "Iteration 176, loss = 27464550209308.87500000\n",
      "Iteration 177, loss = 26658860452677.67968750\n",
      "Iteration 178, loss = 25906483806851.10156250\n",
      "Iteration 179, loss = 25100060861162.89843750\n",
      "Iteration 180, loss = 24379967808626.80078125\n",
      "Iteration 181, loss = 23693669382346.58593750\n",
      "Iteration 182, loss = 23061987559529.80859375\n",
      "Iteration 183, loss = 22397749021913.23828125\n",
      "Iteration 184, loss = 21727803590472.43359375\n",
      "Iteration 185, loss = 21180379926052.51171875\n",
      "Iteration 186, loss = 20581757929057.45312500\n",
      "Iteration 187, loss = 20014837850335.34375000\n",
      "Iteration 188, loss = 19472129741004.07812500\n",
      "Iteration 189, loss = 18909729439059.18750000\n",
      "Iteration 190, loss = 18407850274706.77343750\n",
      "Iteration 191, loss = 17905563444338.32421875\n",
      "Iteration 192, loss = 17486576250416.51953125\n",
      "Iteration 193, loss = 16919119775431.32031250\n",
      "Iteration 194, loss = 16507626927754.67578125\n",
      "Iteration 195, loss = 16083912460328.34375000\n",
      "Iteration 196, loss = 15617334210159.17968750\n",
      "Iteration 197, loss = 15208657877619.81640625\n",
      "Iteration 198, loss = 14836891108150.69921875\n",
      "Iteration 199, loss = 14436448487054.79687500\n",
      "Iteration 200, loss = 14051647118904.13671875\n",
      "Iteration 201, loss = 13708244101766.96093750\n",
      "Iteration 202, loss = 13377516678826.89257812\n",
      "Iteration 203, loss = 13024264750614.58398438\n",
      "Iteration 204, loss = 12686575920363.20117188\n",
      "Iteration 205, loss = 12400262506053.68554688\n",
      "Iteration 206, loss = 12081976076115.38671875\n",
      "Iteration 207, loss = 11759572027795.36914062\n",
      "Iteration 208, loss = 11529701540588.91406250\n",
      "Iteration 209, loss = 11207583580731.11523438\n",
      "Iteration 210, loss = 10928159515320.69335938\n",
      "Iteration 211, loss = 10657105265172.88281250\n",
      "Iteration 212, loss = 10419259126692.10742188\n",
      "Iteration 213, loss = 10154310776990.81250000\n",
      "Iteration 214, loss = 9916473785601.91015625\n",
      "Iteration 215, loss = 9680089831939.19726562\n",
      "Iteration 216, loss = 9479043435809.44921875\n",
      "Iteration 217, loss = 9278561622896.12890625\n",
      "Iteration 218, loss = 9068391943804.22070312\n",
      "Iteration 219, loss = 8839395301537.21093750\n",
      "Iteration 220, loss = 8641665707936.05664062\n",
      "Iteration 221, loss = 8432146768759.37011719\n",
      "Iteration 222, loss = 8260030136503.38378906\n",
      "Iteration 223, loss = 8061685367439.08691406\n",
      "Iteration 224, loss = 7899812282226.86035156\n",
      "Iteration 225, loss = 7729902478915.38867188\n",
      "Iteration 226, loss = 7566878465025.74804688\n",
      "Iteration 227, loss = 7399563200978.99707031\n",
      "Iteration 228, loss = 7258857144558.41992188\n",
      "Iteration 229, loss = 7101492208144.20214844\n",
      "Iteration 230, loss = 6939432182625.87500000\n",
      "Iteration 231, loss = 6797515863356.25292969\n",
      "Iteration 232, loss = 6676977178873.77246094\n",
      "Iteration 233, loss = 6531793100102.78515625\n",
      "Iteration 234, loss = 6393654511541.76269531\n",
      "Iteration 235, loss = 6276805880314.34863281\n",
      "Iteration 236, loss = 6146915459975.71289062\n",
      "Iteration 237, loss = 6027726778294.95703125\n",
      "Iteration 238, loss = 5908220180687.63281250\n",
      "Iteration 239, loss = 5810921088536.86132812\n",
      "Iteration 240, loss = 5698260897441.42285156\n",
      "Iteration 241, loss = 5574697094969.71875000\n",
      "Iteration 242, loss = 5481214061009.18750000\n",
      "Iteration 243, loss = 5380899811115.30957031\n",
      "Iteration 244, loss = 5269947415808.70800781\n",
      "Iteration 245, loss = 5185573163879.68945312\n",
      "Iteration 246, loss = 5083126118677.83300781\n",
      "Iteration 247, loss = 5004523760304.76269531\n",
      "Iteration 248, loss = 4906093288590.57324219\n",
      "Iteration 249, loss = 4816820495582.45996094\n",
      "Iteration 250, loss = 4740535380552.20605469\n",
      "Iteration 251, loss = 4653671659768.63183594\n",
      "Iteration 252, loss = 4579898426596.55566406\n",
      "Iteration 253, loss = 4504432446384.96679688\n",
      "Iteration 254, loss = 4420921494447.11328125\n",
      "Iteration 255, loss = 4350947413035.76757812\n",
      "Iteration 256, loss = 4278953086887.62060547\n",
      "Iteration 257, loss = 4209294731889.21337891\n",
      "Iteration 258, loss = 4134527508324.53662109\n",
      "Iteration 259, loss = 4072174857025.24755859\n",
      "Iteration 260, loss = 4006949882382.19091797\n",
      "Iteration 261, loss = 3944492500713.54931641\n",
      "Iteration 262, loss = 3879775204172.63916016\n",
      "Iteration 263, loss = 3816070112148.51660156\n",
      "Iteration 264, loss = 3757335220052.41406250\n",
      "Iteration 265, loss = 3703438239387.12353516\n",
      "Iteration 266, loss = 3644911055096.88378906\n",
      "Iteration 267, loss = 3589138521273.88134766\n",
      "Iteration 268, loss = 3537529645895.60839844\n",
      "Iteration 269, loss = 3490794204593.76855469\n",
      "Iteration 270, loss = 3433139619561.30273438\n",
      "Iteration 271, loss = 3384056979192.69775391\n",
      "Iteration 272, loss = 3337018491541.33300781\n",
      "Iteration 273, loss = 3286394008246.09960938\n",
      "Iteration 274, loss = 3238905662398.01220703\n",
      "Iteration 275, loss = 3192216048422.97802734\n",
      "Iteration 276, loss = 3152421025817.37500000\n",
      "Iteration 277, loss = 3105413077432.66601562\n",
      "Iteration 278, loss = 3062621710772.41796875\n",
      "Iteration 279, loss = 3022201122511.38037109\n",
      "Iteration 280, loss = 2977773647245.44482422\n",
      "Iteration 281, loss = 2938145453318.88818359\n",
      "Iteration 282, loss = 2897351583866.66601562\n",
      "Iteration 283, loss = 2860865514506.48095703\n",
      "Iteration 284, loss = 2822773353340.15527344\n",
      "Iteration 285, loss = 2787735342655.55273438\n",
      "Iteration 286, loss = 2752584380957.21337891\n",
      "Iteration 287, loss = 2714398309483.87255859\n",
      "Iteration 288, loss = 2678862964243.17773438\n",
      "Iteration 289, loss = 2645946304234.38574219\n",
      "Iteration 290, loss = 2613139136584.84619141\n",
      "Iteration 291, loss = 2580412302505.00830078\n",
      "Iteration 292, loss = 2546404195148.30175781\n",
      "Iteration 293, loss = 2515052753192.61523438\n",
      "Iteration 294, loss = 2486438265239.84423828\n",
      "Iteration 295, loss = 2455610357683.95898438\n",
      "Iteration 296, loss = 2424499065547.43603516\n",
      "Iteration 297, loss = 2395496640872.29833984\n",
      "Iteration 298, loss = 2366756061085.75976562\n",
      "Iteration 299, loss = 2339791939802.20605469\n",
      "Iteration 300, loss = 2312149349094.35351562\n",
      "Iteration 301, loss = 2284245048202.52929688\n",
      "Iteration 302, loss = 2257354335445.52880859\n",
      "Iteration 303, loss = 2232920168967.43652344\n",
      "Iteration 304, loss = 2207709047765.13134766\n",
      "Iteration 305, loss = 2181541290566.87207031\n",
      "Iteration 306, loss = 2157416128748.68701172\n",
      "Iteration 307, loss = 2130689895087.73803711\n",
      "Iteration 308, loss = 2108603714416.93115234\n",
      "Iteration 309, loss = 2082776787248.53588867\n",
      "Iteration 310, loss = 2059762116782.63696289\n",
      "Iteration 311, loss = 2036698241259.07177734\n",
      "Iteration 312, loss = 2013961405861.67968750\n",
      "Iteration 313, loss = 1991704514914.63378906\n",
      "Iteration 314, loss = 1969952953926.85156250\n",
      "Iteration 315, loss = 1949364136520.90332031\n",
      "Iteration 316, loss = 1928174237163.89208984\n",
      "Iteration 317, loss = 1908087436633.74365234\n",
      "Iteration 318, loss = 1887461678661.76879883\n",
      "Iteration 319, loss = 1866925551998.44580078\n",
      "Iteration 320, loss = 1848006173602.28540039\n",
      "Iteration 321, loss = 1829232517802.08300781\n",
      "Iteration 322, loss = 1809267762338.60229492\n",
      "Iteration 323, loss = 1791198586195.40722656\n",
      "Iteration 324, loss = 1772378023518.26171875\n",
      "Iteration 325, loss = 1755316012452.81616211\n",
      "Iteration 326, loss = 1737756410269.49633789\n",
      "Iteration 327, loss = 1720262402176.31616211\n",
      "Iteration 328, loss = 1703064805138.68945312\n",
      "Iteration 329, loss = 1688156784926.45581055\n",
      "Iteration 330, loss = 1671592910495.51660156\n",
      "Iteration 331, loss = 1654021600693.56420898\n",
      "Iteration 332, loss = 1637853631001.91406250\n",
      "Iteration 333, loss = 1621595530216.01831055\n",
      "Iteration 334, loss = 1606948651987.32202148\n",
      "Iteration 335, loss = 1592153660866.55590820\n",
      "Iteration 336, loss = 1577411413694.58325195\n",
      "Iteration 337, loss = 1562154168527.89233398\n",
      "Iteration 338, loss = 1548660148336.23144531\n",
      "Iteration 339, loss = 1533392365981.29101562\n",
      "Iteration 340, loss = 1520794410242.76440430\n",
      "Iteration 341, loss = 1507000596127.81079102\n",
      "Iteration 342, loss = 1492012353040.62158203\n",
      "Iteration 343, loss = 1480435360761.81030273\n",
      "Iteration 344, loss = 1466364908462.97607422\n",
      "Iteration 345, loss = 1453766757144.54296875\n",
      "Iteration 346, loss = 1441062827280.24316406\n",
      "Iteration 347, loss = 1430631512721.96899414\n",
      "Iteration 348, loss = 1417179258796.69433594\n",
      "Iteration 349, loss = 1404527657711.47167969\n",
      "Iteration 350, loss = 1393273051653.50000000\n",
      "Iteration 351, loss = 1381750609580.15966797\n",
      "Iteration 352, loss = 1370268691568.81860352\n",
      "Iteration 353, loss = 1359140261894.28857422\n",
      "Iteration 354, loss = 1347362933995.53930664\n",
      "Iteration 355, loss = 1336868380497.75415039\n",
      "Iteration 356, loss = 1324933925345.63085938\n",
      "Iteration 357, loss = 1315417227982.31005859\n",
      "Iteration 358, loss = 1304445805963.12670898\n",
      "Iteration 359, loss = 1294028198724.12695312\n",
      "Iteration 360, loss = 1284082068514.54443359\n",
      "Iteration 361, loss = 1273198076360.34008789\n",
      "Iteration 362, loss = 1263931930630.45825195\n",
      "Iteration 363, loss = 1253745154639.84252930\n",
      "Iteration 364, loss = 1243702956884.84423828\n",
      "Iteration 365, loss = 1234499194321.53491211\n",
      "Iteration 366, loss = 1225830094122.56665039\n",
      "Iteration 367, loss = 1215574169160.55322266\n",
      "Iteration 368, loss = 1206691225424.61938477\n",
      "Iteration 369, loss = 1197733004540.44287109\n",
      "Iteration 370, loss = 1188686905956.25927734\n",
      "Iteration 371, loss = 1179915993090.35253906\n",
      "Iteration 372, loss = 1171860302460.83251953\n",
      "Iteration 373, loss = 1162986589127.29077148\n",
      "Iteration 374, loss = 1154319903330.03710938\n",
      "Iteration 375, loss = 1146165758305.82788086\n",
      "Iteration 376, loss = 1138158021620.76147461\n",
      "Iteration 377, loss = 1129865449906.13427734\n",
      "Iteration 378, loss = 1121492693946.75585938\n",
      "Iteration 379, loss = 1113914517308.97851562\n",
      "Iteration 380, loss = 1105809850963.21752930\n",
      "Iteration 381, loss = 1096598405678.37109375\n",
      "Iteration 382, loss = 1088908671536.53100586\n",
      "Iteration 383, loss = 1081424914816.98229980\n",
      "Iteration 384, loss = 1073426385698.40209961\n",
      "Iteration 385, loss = 1065532833776.24975586\n",
      "Iteration 386, loss = 1057736709993.25500488\n",
      "Iteration 387, loss = 1049881607325.63562012\n",
      "Iteration 388, loss = 1042466458845.98376465\n",
      "Iteration 389, loss = 1034989909087.55627441\n",
      "Iteration 390, loss = 1027525033197.98229980\n",
      "Iteration 391, loss = 1020346970226.14294434\n",
      "Iteration 392, loss = 1012548400150.20275879\n",
      "Iteration 393, loss = 1006216896436.29663086\n",
      "Iteration 394, loss = 999158497926.51452637\n",
      "Iteration 395, loss = 991694790276.67431641\n",
      "Iteration 396, loss = 985248181609.69714355\n",
      "Iteration 397, loss = 978824832312.03662109\n",
      "Iteration 398, loss = 971319398994.60437012\n",
      "Iteration 399, loss = 964802257330.68835449\n",
      "Iteration 400, loss = 958538102863.52294922\n",
      "Iteration 401, loss = 951856347347.42687988\n",
      "Iteration 402, loss = 945121922847.78625488\n",
      "Iteration 403, loss = 939597773191.15954590\n",
      "Iteration 404, loss = 932728667852.17028809\n",
      "Iteration 405, loss = 927248281549.59472656\n",
      "Iteration 406, loss = 920922487325.51086426\n",
      "Iteration 407, loss = 915255155625.27832031\n",
      "Iteration 408, loss = 909420327087.32568359\n",
      "Iteration 409, loss = 902888085584.18151855\n",
      "Iteration 410, loss = 897509392543.17480469\n",
      "Iteration 411, loss = 891276999713.88500977\n",
      "Iteration 412, loss = 885697870371.88293457\n",
      "Iteration 413, loss = 880278021893.71252441\n",
      "Iteration 414, loss = 873758275593.81286621\n",
      "Iteration 415, loss = 868474329631.65551758\n",
      "Iteration 416, loss = 862830212977.75573730\n",
      "Iteration 417, loss = 856875318556.34667969\n",
      "Iteration 418, loss = 851372039115.44299316\n",
      "Iteration 419, loss = 845489806195.50708008\n",
      "Iteration 420, loss = 840178725959.38391113\n",
      "Iteration 421, loss = 834975881101.37231445\n",
      "Iteration 422, loss = 829300204354.01977539\n",
      "Iteration 423, loss = 823426026767.77722168\n",
      "Iteration 424, loss = 818484767643.47058105\n",
      "Iteration 425, loss = 813107831961.72265625\n",
      "Iteration 426, loss = 807796931311.51550293\n",
      "Iteration 427, loss = 802672522597.23681641\n",
      "Iteration 428, loss = 797312235603.55224609\n",
      "Iteration 429, loss = 792363432201.53955078\n",
      "Iteration 430, loss = 786366802299.75683594\n",
      "Iteration 431, loss = 780218085239.01782227\n",
      "Iteration 432, loss = 775051808927.40234375\n",
      "Iteration 433, loss = 768295915427.80639648\n",
      "Iteration 434, loss = 762696133947.48132324\n",
      "Iteration 435, loss = 755829770622.93933105\n",
      "Iteration 436, loss = 749960742203.05920410\n",
      "Iteration 437, loss = 743827194900.29150391\n",
      "Iteration 438, loss = 738885382476.52734375\n",
      "Iteration 439, loss = 732175189683.68713379\n",
      "Iteration 440, loss = 726420480457.31970215\n",
      "Iteration 441, loss = 720450373122.70690918\n",
      "Iteration 442, loss = 714580612897.61096191\n",
      "Iteration 443, loss = 708975339829.00671387\n",
      "Iteration 444, loss = 702138306585.95300293\n",
      "Iteration 445, loss = 697434158649.64135742\n",
      "Iteration 446, loss = 691724844080.22851562\n",
      "Iteration 447, loss = 685305268439.43823242\n",
      "Iteration 448, loss = 680092229012.28857422\n",
      "Iteration 449, loss = 674667659855.37182617\n",
      "Iteration 450, loss = 668992463056.13989258\n",
      "Iteration 451, loss = 663709134287.35217285\n",
      "Iteration 452, loss = 658089886839.68603516\n",
      "Iteration 453, loss = 652504675209.83410645\n",
      "Iteration 454, loss = 647245767578.83288574\n",
      "Iteration 455, loss = 642103292633.24621582\n",
      "Iteration 456, loss = 636693713282.96191406\n",
      "Iteration 457, loss = 631623841336.96667480\n",
      "Iteration 458, loss = 625933728237.99255371\n",
      "Iteration 459, loss = 620759084715.57141113\n",
      "Iteration 460, loss = 614977847897.66186523\n",
      "Iteration 461, loss = 609384047558.21154785\n",
      "Iteration 462, loss = 604767221308.11682129\n",
      "Iteration 463, loss = 598861271164.34008789\n",
      "Iteration 464, loss = 593151930046.29321289\n",
      "Iteration 465, loss = 588544945666.88256836\n",
      "Iteration 466, loss = 582908986751.91699219\n",
      "Iteration 467, loss = 578091817864.60778809\n",
      "Iteration 468, loss = 572710749964.58935547\n",
      "Iteration 469, loss = 567779248311.19311523\n",
      "Iteration 470, loss = 562998753976.32214355\n",
      "Iteration 471, loss = 557551616591.35168457\n",
      "Iteration 472, loss = 553517187485.46752930\n",
      "Iteration 473, loss = 548139354633.90698242\n",
      "Iteration 474, loss = 543127438554.06115723\n",
      "Iteration 475, loss = 538692250277.71832275\n",
      "Iteration 476, loss = 533541859420.66082764\n",
      "Iteration 477, loss = 529312584218.36383057\n",
      "Iteration 478, loss = 524843546413.07543945\n",
      "Iteration 479, loss = 519828008576.85449219\n",
      "Iteration 480, loss = 515846859024.50366211\n",
      "Iteration 481, loss = 511751639769.98480225\n",
      "Iteration 482, loss = 506759895315.31909180\n",
      "Iteration 483, loss = 502409361138.71575928\n",
      "Iteration 484, loss = 498117316812.32812500\n",
      "Iteration 485, loss = 493587053477.77258301\n",
      "Iteration 486, loss = 489858957651.21301270\n",
      "Iteration 487, loss = 485716939852.69976807\n",
      "Iteration 488, loss = 481156512224.17968750\n",
      "Iteration 489, loss = 477467712067.40502930\n",
      "Iteration 490, loss = 473073532996.33715820\n",
      "Iteration 491, loss = 469283544202.15856934\n",
      "Iteration 492, loss = 465635229484.27282715\n",
      "Iteration 493, loss = 461399543297.42004395\n",
      "Iteration 494, loss = 457475166450.28997803\n",
      "Iteration 495, loss = 453695835442.44519043\n",
      "Iteration 496, loss = 449766123946.40893555\n",
      "Iteration 497, loss = 446439732679.96392822\n",
      "Iteration 498, loss = 442280826938.37811279\n",
      "Iteration 499, loss = 438510175516.47998047\n",
      "Iteration 500, loss = 434790565953.73394775\n",
      "Iteration 501, loss = 431437356030.38000488\n",
      "Iteration 502, loss = 427754714772.78710938\n",
      "Iteration 503, loss = 424521308493.79064941\n",
      "Iteration 504, loss = 421913230568.78955078\n",
      "Iteration 505, loss = 417204594902.76580811\n",
      "Iteration 506, loss = 413370416140.32745361\n",
      "Iteration 507, loss = 409803056903.12854004\n",
      "Iteration 508, loss = 406661853140.37670898\n",
      "Iteration 509, loss = 403687971175.81744385\n",
      "Iteration 510, loss = 400472376070.67095947\n",
      "Iteration 511, loss = 396732715133.85150146\n",
      "Iteration 512, loss = 393553048669.59539795\n",
      "Iteration 513, loss = 390812775252.90783691\n",
      "Iteration 514, loss = 388241604324.21051025\n",
      "Iteration 515, loss = 386226186179.23168945\n",
      "Iteration 516, loss = 382263421175.57531738\n",
      "Iteration 517, loss = 380155594606.77770996\n",
      "Iteration 518, loss = 378936907887.47460938\n",
      "Iteration 519, loss = 380872942251.39355469\n",
      "Iteration 520, loss = 385428945911.30810547\n",
      "Iteration 521, loss = 392216920954.26062012\n",
      "Iteration 522, loss = 385047549158.41223145\n",
      "Iteration 523, loss = 384468700426.87231445\n",
      "Iteration 524, loss = 397747459494.08465576\n",
      "Iteration 525, loss = 431462777253.98480225\n",
      "Iteration 526, loss = 464051420018.07098389\n",
      "Iteration 527, loss = 483449929696.39648438\n",
      "Iteration 528, loss = 476401445623.41082764\n",
      "Iteration 529, loss = 491187627522.22021484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.9853199639609076"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = MLPRegressor(hidden_layer_sizes=(800, 500, 100), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9658812395500416.00000000\n",
      "Iteration 2, loss = 4880336480572720.00000000\n",
      "Iteration 3, loss = 4332360837154315.00000000\n",
      "Iteration 4, loss = 4194391133010503.00000000\n",
      "Iteration 5, loss = 4168126972030673.50000000\n",
      "Iteration 6, loss = 4134115957056925.00000000\n",
      "Iteration 7, loss = 4171014934373055.00000000\n",
      "Iteration 8, loss = 4133475728046373.00000000\n",
      "Iteration 9, loss = 4162364366452409.00000000\n",
      "Iteration 10, loss = 4101856638924171.50000000\n",
      "Iteration 11, loss = 4104163450234917.50000000\n",
      "Iteration 12, loss = 4117484279157487.00000000\n",
      "Iteration 13, loss = 4105489633723290.50000000\n",
      "Iteration 14, loss = 4116760156048518.00000000\n",
      "Iteration 15, loss = 4118411532866270.50000000\n",
      "Iteration 16, loss = 4136889469632305.00000000\n",
      "Iteration 17, loss = 4121678957485150.00000000\n",
      "Iteration 18, loss = 4110214517932114.50000000\n",
      "Iteration 19, loss = 4114349507400780.00000000\n",
      "Iteration 20, loss = 4104246014542306.50000000\n",
      "Iteration 21, loss = 4107820770373103.50000000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(1000, 100), max_iter=1000, random_state=21,\n",
       "             verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(1000, 100), max_iter=1000, random_state=21,\n",
       "             verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(1000, 100), max_iter=1000, random_state=21,\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from evaluation_features.npy\n",
    "with open('evaluation_features.npy', 'rb') as f:\n",
    "    evaluation_features = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from evaluation_features.npy\n",
    "with open('evaluation_features_short.npy', 'rb') as f:\n",
    "    evaluation_features = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = pd.read_csv('data/test.csv', usecols=[0]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4398, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4398,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict revenue for test data\n",
    "preds = regr.predict(evaluation_features)\n",
    "# save predictions to a csv file\n",
    "np.savetxt('result_mlp3.csv', np.concatenate((test_ids, preds.reshape(-1, 1)), axis=1), delimiter=',', header='id,revenue', comments='', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = regr.predict(scaler.transform(evaluation_features))\n",
    "\n",
    "np.savetxt('result_mlp5.csv', np.concatenate((test_ids, preds.reshape(-1, 1)), axis=1), delimiter=',', header='id,revenue', comments='', fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict revenue for test data\n",
    "preds = mlp_regr.predict(evaluation_features)\n",
    "# save predictions to a csv file\n",
    "np.savetxt('result_mlp-f.csv', np.concatenate((test_ids, preds.reshape(-1, 1)), axis=1), delimiter=',', header='id,revenue', comments='', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1623: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10649169009674728.00000000\n",
      "Iteration 2, loss = 7423416665176381.00000000\n",
      "Iteration 3, loss = 4387045960520514.50000000\n",
      "Iteration 4, loss = 4478326826968907.00000000\n",
      "Iteration 5, loss = 4245883173492859.00000000\n",
      "Iteration 6, loss = 4256421994430762.00000000\n",
      "Iteration 7, loss = 4224290369340590.00000000\n",
      "Iteration 8, loss = 4220636750145438.00000000\n",
      "Iteration 9, loss = 4220286952294331.50000000\n",
      "Iteration 10, loss = 4219868623491017.50000000\n",
      "Iteration 11, loss = 4222761196760835.50000000\n",
      "Iteration 12, loss = 4226513886461231.00000000\n",
      "Iteration 13, loss = 4234880087621019.50000000\n",
      "Iteration 14, loss = 4209723827266092.50000000\n",
      "Iteration 15, loss = 4217978707976180.00000000\n",
      "Iteration 16, loss = 4215376911955309.00000000\n",
      "Iteration 17, loss = 4249808424897363.50000000\n",
      "Iteration 18, loss = 4251845922937924.50000000\n",
      "Iteration 19, loss = 4234977163333819.50000000\n",
      "Iteration 20, loss = 4332773937400825.00000000\n",
      "Iteration 21, loss = 4287544090011541.50000000\n",
      "Iteration 22, loss = 4236184431768347.50000000\n",
      "Iteration 23, loss = 4221311517582032.00000000\n",
      "Iteration 24, loss = 4246232925625554.00000000\n",
      "Iteration 25, loss = 4283001165480641.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.596870894442328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "regr = MLPRegressor(hidden_layer_sizes=(500, 200), max_iter=1000, alpha=0.0001,\n",
    "                    solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data and transform the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the test data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean and standard deviation for each column of the training data\n",
    "means = X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index where the mean is > 10000\n",
    "idx = np.where(means > 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21932807.30041667])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transformation to idx column\n",
    "X_train_log = np.log(X_train[:, idx] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back to X_train\n",
    "X_train[:, idx] = X_train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for test data\n",
    "X_test_log = np.log(X_test[:, idx] + 1)\n",
    "X_test[:, idx] = X_test_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([768]),)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize only some columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the training data and transform the data\n",
    "X_train_scaled = scaler.fit_transform(X_train[:, 768:769])\n",
    "\n",
    "# transform the test data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test[:, 768:769])\n",
    "\n",
    "# replace\n",
    "X_train[:, 768:769] = X_train_scaled\n",
    "X_test[:, 768:769] = X_test_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler cannot improve performance and makes the training taking a longer time to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mean Squared Logarithmic Error cannot be used when targets contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m regr\u001b[39m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[1;32m      8\u001b[0m preds \u001b[39m=\u001b[39m regr\u001b[39m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m---> 10\u001b[0m rmsle(y_test, preds)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mrmsle\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmsle\u001b[39m(y_true, y_pred):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msqrt(mean_squared_log_error(y_true, y_pred))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:525\u001b[0m, in \u001b[0;36mmean_squared_log_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    522\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    524\u001b[0m \u001b[39mif\u001b[39;00m (y_true \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m (y_pred \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many():\n\u001b[0;32m--> 525\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    526\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMean Squared Logarithmic Error cannot be used when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtargets contain negative values.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m mean_squared_error(\n\u001b[1;32m    531\u001b[0m     np\u001b[39m.\u001b[39mlog1p(y_true),\n\u001b[1;32m    532\u001b[0m     np\u001b[39m.\u001b[39mlog1p(y_pred),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m     squared\u001b[39m=\u001b[39msquared,\n\u001b[1;32m    536\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
     ]
    }
   ],
   "source": [
    "# use linear regression for revenue prediction\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = regr.predict(X_test_scaled)\n",
    "\n",
    "rmsle(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(regr, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
